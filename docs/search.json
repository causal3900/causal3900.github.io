[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"Cornell STSCI / INFO / ILRST 3900. Causal Inference. Fall 2025.Welcome! Together, learn reason assess plausibility causal claims combining data assumptions.Taught Christina Lee Yu, Y. Samuel Wang, Filippo Fiocchi, Shira Mingelgrin. Read us !","code":""},{"path":"index.html","id":"learning-objectives","chapter":"Welcome","heading":"Learning objectives","text":"result participating course, students able todefine counterfactuals outcomes hypothetical interventionsidentify counterfactuals causal assumptions presented graphsestimate counterfactual outcomes pairing assumptions statistical evidence","code":""},{"path":"index.html","id":"is-this-course-for-me","chapter":"Welcome","heading":"Is this course for me?","text":"course designed upper-division undergraduate students. assume familiarity introductory statistics course level STSCI 2110, PAM 2100, PSYCH 2500, SOC 3010, ECON 3110, similar courses. also assume students familiar statistical computing language R.Cornell student? welcome follow along site.","code":""},{"path":"index.html","id":"readings","chapter":"Welcome","heading":"Readings","text":"Especially beginning, course draws heavily onHernán, M.., J.M. Robins. 2020. Causal Inference: ? Boca Raton: Chapman & Hall / CRC.grateful authors excellent text.","code":""},{"path":"index.html","id":"organization-of-the-site","chapter":"Welcome","heading":"Organization of the site","text":"course module left panel span several lectures. Within module, right panel help navigate. build site course semester, uploading lecture slides go. tells bit teaching team.","code":""},{"path":"index.html","id":"previous-iterations-of-the-course","chapter":"Welcome","heading":"Previous iterations of the course","text":"Much material course draw directly previous iterations course also developed Ian Lundberg Mayleen Cortez-Rodriguez.access course website Fall 2023 click .","code":""},{"path":"index.html","id":"land-acknowledgment","chapter":"Welcome","heading":"Land acknowledgment","text":"recognize university land acknowledgment, well additional emphasis Cornell American Indian Indigenous Studies Program.Cornell University located traditional homelands Gayogo̱hó:nǫɁ (Cayuga Nation). Gayogo̱hó:nǫɁ members Haudenosaunee Confederacy, alliance six sovereign Nations historic contemporary presence land. Confederacy precedes establishment Cornell University, New York state, United States America. acknowledge painful history Gayogo̱hó:nǫɁ dispossession, honor ongoing connection Gayogo̱hó:nǫɁ people, past present, lands waters.land acknowledgment reviewed approved traditional Gayogo̱hó:nǫɁ leadership.addition Gayogo̱hó:nǫɁ land acknowledgment separate , AIISP faculty like emphasize: Cornell’s founding enabled course national genocide sale almost one million acres stolen Indian land Morrill Act 1862. date university neither officially acknowledged complicity theft offered form restitution hundreds Native communities impacted. additional information, see Cornell University Indigenous Dispossession website.","code":""},{"path":"defining-counterfactuals.html","id":"defining-counterfactuals","chapter":"1 Defining counterfactuals","heading":"1 Defining counterfactuals","text":"","code":""},{"path":"defining-counterfactuals.html","id":"observing-versus-intervening","chapter":"1 Defining counterfactuals","heading":"1.1 Observing versus intervening","text":"Aug 26. Slides\nclass, install R Rstudio computer (see slide 17 today’s lecture).Statistical inference observing: observe sample population, can infer population?\nCausal inference intervening: intervene change exposure, average outcome result?Today discuss observing, intervening, difference important.","code":""},{"path":"defining-counterfactuals.html","id":"lab-statistics-review","chapter":"1 Defining counterfactuals","heading":"1.2 Lab: Statistics review","text":"Aug 27. Discussion, download .Rmd fileIn lab, start reviewing basic statistical (random variables, expectation, conditional expectation, etc) programming concepts.","code":""},{"path":"defining-counterfactuals.html","id":"defining-causal-effects","chapter":"1 Defining counterfactuals","heading":"1.3 Defining causal effects","text":"Aug 28. Slides.\nclass, read Chapter 1 Hernán Robins 2020 begin Problem Set 1.Today define average causal effects potential outcomes framework.end class, able todefine potential outcomesexplain Fundamental Problem Causal Inference1","code":""},{"path":"exchangeability-and-experiments.html","id":"exchangeability-and-experiments","chapter":"2 Exchangeability and experiments","heading":"2 Exchangeability and experiments","text":"","code":""},{"path":"exchangeability-and-experiments.html","id":"randomized-experiments","chapter":"2 Exchangeability and experiments","heading":"2.1 Randomized experiments","text":"Sep 2. Slides.\nclass, read Hernán Robins 2020 Chapter 2 end 2.1.Much course address observational studies non-randomized treatments. set stage, today first discuss randomized experiments powerful possible.","code":""},{"path":"exchangeability-and-experiments.html","id":"lab-analyze-a-randomized-experiment","chapter":"2 Exchangeability and experiments","heading":"2.2 Lab: Analyze a randomized experiment","text":"Sep 3. DiscussionThis lab use R analyze data randomized experiment households randomized receive mailers encouraging vote, researchers examined effects voter turnout (Gerber, Green, & Larimer 2008). Download R Markdown file .","code":""},{"path":"exchangeability-and-experiments.html","id":"exchangeability-and-conditional-randomization","chapter":"2 Exchangeability and experiments","heading":"2.3 Exchangeability and conditional randomization","text":"Sep 4. Slides.\nclass, read Hernán Robins 2020 Chapter 2.2 & 2.3.talk experiments good: setting key identification assumption (exchangeability) holds design. discuss exchangeability important: allows us link causal quantities observable data. discuss exchangeability simple randomized experiments experiments conditionally randomized treatment assignment probabilities functions pre-existing characteristics.","code":""},{"path":"exchangeability-and-experiments.html","id":"standardization-and-effect-measures","chapter":"2 Exchangeability and experiments","heading":"2.4 Standardization and effect measures","text":"Sep 9. Slides\nclass, read Hernán Robins 2020 Chapter 2.3 4.1-4.3.Although can use ACE describe average entire population, treatment effect may vary across sub-populations. called treatment effect heterogeneity important consideration making policy decisions.Stratification allows us estimate average causal effect within subpopulation, strata, also known conditional average treatment effect. Standardization important statistical procedure allows us estimate population average treatment effect taking weighted average subpopulations.conditionally randomized experiments, standardization essential yield unbiased estimates population average causal effect. strategy also essential observational studies discuss soon.","code":""},{"path":"exchangeability-and-experiments.html","id":"lab-treatment-effect-heterogeneity-and-exploring-add-health-data","chapter":"2 Exchangeability and experiments","heading":"2.5 Lab: Treatment Effect Heterogeneity and Exploring ADD Health Data","text":"Sep 10. Discussion discussion slidesThis lab use randomized experiment last week (Gerber, Green, & Larimer 2008) explore treatment effect across sub-population, determine whether treatment effect heterogeneity. slides layout tasks course project explore Add Health Codebook Explorer.","code":""},{"path":"exchangeability-and-experiments.html","id":"inverse-probability-weighting","chapter":"2 Exchangeability and experiments","heading":"2.6 Inverse probability weighting","text":"Sep 11. Slides\nclass, read Hernán Robins 2020 Chapters 3.1 3.2.class introduce inverse probability weighting approach estimate average causal effects conditional exchangeability holds. also discuss alternative ways measure causal effect. Finally, discuss conditional exchangeability might also hold observational studies.","code":""},{"path":"consistency-and-positivity.html","id":"consistency-and-positivity","chapter":"3 Consistency and positivity","heading":"3 Consistency and positivity","text":"","code":""},{"path":"consistency-and-positivity.html","id":"exchangeability-in-observational-studies","chapter":"3 Consistency and positivity","heading":"3.1 Exchangeability in Observational Studies","text":"Sep 16. Slides class, read Hernán Robins 2020 Chapter 3.4-3.5. Optionally, read Hernán 2016.makes causal inference observational data challenging? making treatment precise important? topics ’ll discuss lecture!","code":""},{"path":"consistency-and-positivity.html","id":"lab-causal-inference-with-interference","chapter":"3 Consistency and positivity","heading":"3.2 Lab: Causal inference with interference","text":"Sep 17 Discussion discussion slidesWhen defining causal effects, often discuss outcome \\(Y^\\) person realize exposed treatment value \\(\\). definitions become harder exists interference: outcome unit \\(\\) depends treatment assigned unit \\(j\\). discussion focus understanding interference need update potential outcomes notation interference present.","code":""},{"path":"directed-acyclic-graphs.html","id":"directed-acyclic-graphs","chapter":"4 Directed Acyclic Graphs","heading":"4 Directed Acyclic Graphs","text":"","code":""},{"path":"directed-acyclic-graphs.html","id":"marginal-independence","chapter":"4 Directed Acyclic Graphs","heading":"4.1 Marginal independence","text":"Sep 18. Slides. class, read Hernán Robins 2020 Chapter 6.1 6.2. historical reference, optionally see Greenland, Pearl, Robins 1999.class introduce key ideas DAGs.Directed Acyclic Graph. series nodes representing variables, connected directed edges representing direct causal effects. node edge least two nodes must drawn graph.Path. path sequence edges connecting two nodesCollider along path. node \\(B\\) directed edges collide: \\(\\rightarrow B \\leftarrow C\\). collider blocks path.DAGs help us know variables \\(\\) \\(B\\) statistically related\\(\\) \\(B\\) marginally dependent exists unblocked path connecting \\(\\) \\(B\\) marginally independent paths connecting blocked","code":""},{"path":"directed-acyclic-graphs.html","id":"conditional-independence","chapter":"4 Directed Acyclic Graphs","heading":"4.2 Conditional independence","text":"Sep 23. Slides class, read Hernán Robins 2020 Chapter 6.3 6.4, especially Fine Point 6.1 page abbreviation.Often, want condition set variables \\(\\vec{L}\\) conditional exchangeability holds.path blocked node path blocked. every node path open, entire path openA non-collider blocked conditioned , otherwise openA collider open descendants conditioned . Otherwise blocked","code":""},{"path":"directed-acyclic-graphs.html","id":"lab-dags-review","chapter":"4 Directed Acyclic Graphs","heading":"4.3 Lab: DAGs Review","text":"Sep 24. Discussion discussion slidesIn lab, ’re reviewing DAG basics identifying paths determining whether path open closed.","code":""},{"path":"directed-acyclic-graphs.html","id":"sufficient-adjustment-sets","chapter":"4 Directed Acyclic Graphs","heading":"4.4 Sufficient adjustment sets","text":"Sep 25. Slides. class, read Hernán Robins 2020 7.1–7.4.marginal exchangeability hold, may able condition set variables \\(\\vec{L}\\) conditional exchangeability holds. can accomplish blocking non-causal paths \\(\\) \\(Y\\). set called sufficient adjustment set. find sufficient adjustment set, use backdoor criterion:set \\(L\\) blocks backdoor pathsThe set \\(L\\) contain descendants \\(\\)","code":""},{"path":"statistical-modeling.html","id":"statistical-modeling","chapter":"5 Statistical modeling","heading":"5 Statistical modeling","text":"","code":""},{"path":"statistical-modeling.html","id":"why-model","chapter":"5 Statistical modeling","heading":"5.1 Why model?","text":"Sep 30. Slides. class, read Hernán Robins 2020 Chapter 11.point, used statistical models. Instead, havetaken means within subgroupsthen aggregated subgroupsToday discuss strategy breaks many confounding variables, thus many subgroups.","code":""},{"path":"statistical-modeling.html","id":"lab-project-discussion","chapter":"5 Statistical modeling","heading":"5.2 Lab: Project Discussion","text":"Oct 1. Slides. go Part 1 course project. complete part 1, download .Rmd, compile PDF submit PDF Canvas.","code":""},{"path":"statistical-modeling.html","id":"parametric-modeling","chapter":"5 Statistical modeling","heading":"5.3 Parametric Modeling","text":"Oct 2. Slides 2024. Reading: class, read Hernán Robins 2020 Chapter 12.1–12.5.Today introduce estimate causal effects directly modeling outcome based covariates. addition, discuss model probability treatment, also known propensity score.","code":""},{"path":"statistical-modeling.html","id":"lab-parametric-g-formula","chapter":"5 Statistical modeling","heading":"5.4 Lab: Parametric g-formula","text":"Oct 9. Slides 2024.Download corresponding R Markdown file .discussion, make sure download data ’ll using. See Ed Discussion post detail.class, read Hernán Robins 2020 Chapter 13 15.1.Solutions lab exercise .","code":""},{"path":"statistical-modeling.html","id":"matching","chapter":"5 Statistical modeling","heading":"5.5 Matching","text":"Oct 10. Slides 2024. class, read Hernán Robins 2020 Chapter 15.2.Today introduce idea matching allows us estimate average treatment treated.","code":""},{"path":"statistical-modeling.html","id":"lab-matching-in-r","chapter":"5 Statistical modeling","heading":"5.6 Lab: Matching in R","text":"Oct 16. Slides 2024.lab, ’ll go distance metrics matching multiple covariates. ’ll also go examples using R matching estimate causal effects.Download R Markdown. file today’s lab; view knit file . Submit work Canvas leave discussion! SolutionsIf finish exact matching exercise early, work R Markdown Notebook examples (download .Rmd file ). also video walking examples canvas.","code":""},{"path":"statistical-modeling.html","id":"discussion-of-matching","chapter":"5 Statistical modeling","heading":"5.7 Discussion of matching","text":"Oct 17 Slides 2024.’ll wrap discussion matching introducing propensity score matching coarsened exact matching. ’ll also discuss combining regression matching methods estimate causal effects.","code":""},{"path":"statistical-modeling.html","id":"worked-example-of-statistical-modeling","chapter":"5 Statistical modeling","heading":"5.8 Worked example of statistical modeling","text":"section presents math code worked example statistical\nmodeling, includingoutcome modelingmatchingWe use methods answer causal question:degree completing 4-year college degree age 25\nincrease probability college-educated spouse \nresidential partner age 35?theory motivates question follows. College causes\npeople personally higher earnings. also affects \nprobability someone lives high-earning partner. college\ndegree thus affects household incomes effect \nindividual earnings also effect individuals pool\nhouseholds.","code":""},{"path":"statistical-modeling.html","id":"data-access","chapter":"5 Statistical modeling","heading":"5.8.1 Data access","text":"prepared data study question. need \ndownload data directly data distributor National\nLongitudinal Survey Youth 1997\ncohort. .First, download two supporting files us:nlsy97.NLSY97\ntagset file containing variable namesprepare_nlsy97.R\nR script prepare dataput files directory workNext, download data NLSY97.register surveylog NLS Investigatorchoose NLSY97 studyupload tagset downloaded usdownload data. , change file name default\nnlsy97unzip file. Find nlsy97.dat unzipped folderdrag file folder workIn R console, run line code belowAfter following steps, data working directory! \ncan load data quickly future typingWhy can’t just send data? Two reasons!NLSY97 created procedure register users encourage\nethical use data researchBy registering, help Bureau Labor Statistics know \nmany people using data, helpful demonstrating\nwide use data useful securing funding \nfuture surveys!","code":"\ninstall.packages(\"tidyverse\") # if you do not have it yet\ninstall.packages(\"Amelia\")    # if you do not have it yet\nsource(\"prepare_nlsy97.R\")\nlibrary(tidyverse)\nd <- readRDS(\"d.RDS\")"},{"path":"statistical-modeling.html","id":"worked-example-outcome-modeling","chapter":"5 Statistical modeling","heading":"5.8.2 Worked example: Outcome modeling","text":"Outcome modeling based following identification result, \ntranslates causal quantity statistical estimand \ninvolve counterfactual outcomes.\\[\\begin{aligned}\n&E(Y^) \\\\\n&\\text{law iterated expectation,}\\\\\n&= E(E(Y^\\mid \\vec{L})) \\\\\n&\\text{exchangeability,}\\\\\n&= E(E(Y^\\mid \\vec{L}, = )) \\\\\n&\\text{consistency,}\\\\\n&= E(E(Y\\mid \\vec{L}, = ))\n\\end{aligned}\\]use sample mean estimator outer expectation, \ndiscuss several estimators inner conditional\nexpectation. \\[\\begin{aligned}\n\\hat{E}(Y^) &= \\frac{1}{n}\\sum_{=1}^n \\hat{E}(Y\\mid \\vec{L} = \\vec\\ell_i, = )\n\\end{aligned}\\]Now intuition: estimator tells tofor unit \\(\\) sample, estimate expected outcome among\npeople look like unit (\\(\\vec{L} = \\vec\\ell_i\\)) got\ntreatment value interest \\(= \\).take average estimate unitsA nonparametric strategy step (1) literally estimate \nexpected outcome taking sample average among units \nidentical unit \\(\\) along confounders \\(\\vec{L}\\). \nmany confounding variables units, might zero\ncases! parametric strategy assume model outcome,\n\\[E(Y\\mid \\vec{L} = \\vec\\ell, = ) = \\alpha + \\beta + \\vec\\ell'\\vec\\gamma\\]\nparameters \\(\\{\\alpha,\\beta,\\vec\\gamma\\}\\) estimated \nOrdinary Least Squares regression.Note: model like! example, add\ninteractions use logistic regression instead.model, want predict expected outcome \ntreatment value \\(\\) every unit unit’s observed confounder\nvalues.\\[\\hat{E}(Y\\mid \\vec{L} = \\vec\\ell_i, = ) = \\hat\\alpha + \\hat\\beta + \\vec\\ell'_i\\hat{\\vec\\gamma}\\]\ncode, wouldmodify every unit’s treatment value \\(\\)\nintuition: actually intervened treatment \nworld, value treatment change values \n\\(\\vec{L}\\) remain unchanged\nintuition: actually intervened treatment \nworld, value treatment change values \n\\(\\vec{L}\\) remain unchangedpredict outcome every unitaverage sampleIn code , estimated three causal quantities\\(E(Y^1)\\), probability respondent \ncollege-educated spouse partner intervened assign \ncollege degree \\(E(Y^0)\\), probability respondent \ncollege-educated spouse partner intervened assign \ncollege degree \\(E(Y^1-Y^0)\\), average causal effect college degree \nspouse partner college degree","code":"\noutcome_model <- lm(y ~ a + sex + race + \n                      mom_educ + dad_educ + \n                      log_parent_income +\n                      log_parent_wealth +\n                      test_percentile,\n                    data = d)\n# Make data where all are treated\nd_if_treated <- d %>%\n  mutate(a = \"college\")\n# Make data where all are untreated\nd_if_untreated <- d %>%\n  mutate(a = \"no_college\")\npredicted_outcome <- d %>%\n  mutate(yhat1 = predict(outcome_model,\n                         newdata = d_if_treated),\n         yhat0 = predict(outcome_model,\n                         newdata = d_if_untreated))\npredicted_outcome %>%\n  summarize(average_yhat1 = mean(yhat1),\n            average_yhat0 = mean(yhat0),\n            average_effect = mean(yhat1 - yhat0))## # A tibble: 1 × 3\n##   average_yhat1 average_yhat0 average_effect\n##           <dbl>         <dbl>          <dbl>\n## 1         0.427         0.164          0.263"},{"path":"statistical-modeling.html","id":"worked-example-matching","chapter":"5 Statistical modeling","heading":"5.8.3 Worked example: Matching","text":"also estimate matching. Matching can interpreted \noutcome modeling strategy conditional mean outcome\n\\(E(Y\\mid\\vec{} = , \\vec{L} = \\vec\\ell_i)\\) estimated mean\noutcome among set units whose confounder values similar \nunit \\(\\) received treatment value \\(\\) interest.One way defining ``similar’’ propensity score matching: find\nunits whose probability treatment given confounders close \nprobability unit \\(\\). example, code ,estimates probability college completion using logistic\nregressionfor person finished college, matches non-college\ngraduate whose probability completing college similarThe variable matched$weights one element person \ndataset. indicates many times person appears matched\nsample. are1,533 college graduates weight 11,533 matched non-graduates weight 14,705 non-matched non-graudates weight 0Within matched data, non-graduates graduates similar\nalong confounding variables. estimatethe probability college educated spouse among college\ngraduates mean among peoplethe probability persisted \nfinished college, mean among matched counterpartsAn even better estimator might use linear regression adjust \ndifferences within matched pairs exist matches \nidentical., predict potential outcomes matched fit report \nexpected outcome among college graduates factual treatment\nunderThese results suggest completing college increases probability\ncollege-educated spouse partner 27 percentage\npoints.","code":"\nlibrary(MatchIt)\nmatched <- matchit(a == \"college\" ~ sex + race + mom_educ + \n                     dad_educ + log_parent_income + \n                     log_parent_wealth + test_percentile,\n                   method = \"nearest\", \n                   distance = \"glm\",\n                   estimand = \"ATT\",\n                   data = d)\ntable(d$a,matched$weights)##             \n##                 0    1\n##   college       0 1533\n##   no_college 4705 1533\nd %>%\n  mutate(weight = matched$weights) %>%\n  group_by(a) %>%\n  summarize(p_spouse_college = weighted.mean(y, w = weight))## # A tibble: 2 × 2\n##   a          p_spouse_college\n##   <chr>                 <dbl>\n## 1 college               0.528\n## 2 no_college            0.232\nmatched_fit <- lm(y ~ a*(sex + race + mom_educ + \n                           dad_educ + log_parent_income + \n                           log_parent_wealth + test_percentile), \n                  data = d, \n                  weights = matched$weights)\n# Create data frames for prediction\ncollege_grads_factual <- d %>%\n  filter(a == \"college\")\ncollege_grads_counterfactual <- college_grads_factual %>%\n  mutate(a = \"no_college\")\n\n# Predict outcomes from the model\ncollege_grads_factual %>%\n  mutate(yhat_college = predict(matched_fit, \n                                newdata = college_grads_factual),\n         yhat_no_college = predict(matched_fit,\n                                   newdata = college_grads_counterfactual)) %>%\n  # Report estimated average outcomes\n  select(starts_with(\"yhat\")) %>%\n  summarize_all(.funs = mean) %>%\n  # Estimate the causal effect\n  mutate(effect = yhat_college - yhat_no_college)## # A tibble: 1 × 3\n##   yhat_college yhat_no_college effect\n##          <dbl>           <dbl>  <dbl>\n## 1        0.528           0.259  0.269"},{"path":"instrumental-variables.html","id":"instrumental-variables","chapter":"6 Instrumental variables","heading":"6 Instrumental variables","text":"","code":""},{"path":"instrumental-variables.html","id":"experimental-settings","chapter":"6 Instrumental variables","heading":"6.1 Experimental settings","text":"Oct 22. Slides 2024.instrumental variable (IV) identification strategy applies treatment effect \\(\\) \\(Y\\) confounded unobserved variables (\\(U\\)), instrument \\(Z\\) creates random unconfounded variation \\(\\).clean setting IV randomized experiments non-compliance: experimenter randomizes assigned treatment (\\(Z\\)) actual treatment (\\(\\)) may unequal \\(Z\\) units follow assignment. first class discuss setting.","code":""},{"path":"instrumental-variables.html","id":"lab-instrumental-variable-analysis-in-code","chapter":"6 Instrumental variables","heading":"6.2 Lab: Instrumental Variable analysis in Code","text":"lab implement instrumental variables estimators R.October 23 Slides 2024. Download \nR Markdown file .\nSolutions lab exercise .","code":""},{"path":"instrumental-variables.html","id":"observational-settings","chapter":"6 Instrumental variables","heading":"6.3 Observational settings","text":"Oct 24 Slides 2024. class, read Hernán Robins 2020 Chapter 16.Thursday, move IV analysis observational settings. focus casual assumptions required IV. assumptions often hold design experiments non-compliance. observational settings, can doubtful.","code":""},{"path":"regression-discontinuity.html","id":"regression-discontinuity","chapter":"7 Regression discontinuity","heading":"7 Regression discontinuity","text":"","code":""},{"path":"regression-discontinuity.html","id":"introduction","chapter":"7 Regression discontinuity","heading":"7.1 Introduction","text":"Oct 29. Slides 2024. class, read Huntington Klein 2021 Chapter 20, sections 20.1 20.3.4. Slides annotations lecture: Annotated Slides.settings, treatment assigned based solely value continuous variable. situations, can identify local ATE without many additional assumptions. Today introduce works using regression discontinuity designs.","code":""},{"path":"regression-discontinuity.html","id":"lab-regression-discontinuity---bandwidth-and-examples","chapter":"7 Regression discontinuity","heading":"7.2 Lab: Regression Discontinuity - Bandwidth and Examples","text":"Oct 30 Slides 2024.. Download today’s .Rmd document . lab, read Huntington Klein 2021 Chapter 20, section 20.2.1.’ll discuss bandwidth selection triangular weighting. ’ll also apply regression discontinuity concrete example give chance try .","code":""},{"path":"regression-discontinuity.html","id":"discussion","chapter":"7 Regression discontinuity","heading":"7.3 Discussion","text":"Oct 31. Slides 2024. class, read Huntington Klein 2021 Chapter 20, sections 20.2.2. 20.2.4. R Code Example: Fuzzy RDDWe’ll continue discussion regression discontinuity designs discussing fuzzy RDD validation checks RDD.","code":""},{"path":"difference-in-differences.html","id":"difference-in-differences","chapter":"8 Difference in differences","heading":"8 Difference in differences","text":"","code":""},{"path":"difference-in-differences.html","id":"introduction-1","chapter":"8 Difference in differences","heading":"8.1 Introduction","text":"Nov 5. Slides 2024. reading required, reference see Card & Krueger 1994Today study effect policy change New Jersey, drawing evidence neighboring state Pennsylvania.Difference difference identification strategy used one units become treated time point others . believe assumption parallel trends, can use change time never-treated units estimate change time experienced units become treated, counterfactual world become treated.","code":""},{"path":"difference-in-differences.html","id":"lab","chapter":"8 Difference in differences","heading":"8.2 Lab","text":"Nov 6 Slides 2024. Download \nR Markdown file . can find knitted version .lab, implement difference difference estimator specific setting. example comes Malesky, Nguyen, & Tran 2014 closely follow re-analysis data Egami & Yamauchi 2023.","code":""},{"path":"difference-in-differences.html","id":"extensions-of-did","chapter":"8 Difference in differences","heading":"8.3 Extensions of DID","text":"Nov 7. Slides 2024. reading required, reference see Egami & Yamauchi 2023\nSlides annotations lecture: Annotated Slides.know parallel trends assumption holds? hold? class discusses recent extensions framework.","code":""},{"path":"synthetic-control.html","id":"synthetic-control","chapter":"9 Synthetic control","heading":"9 Synthetic control","text":"","code":""},{"path":"synthetic-control.html","id":"introduction-2","chapter":"9 Synthetic control","heading":"9.1 Introduction","text":"Nov 12 Slides 2024.Today introduce idea synthetic control. class able :Explain intuition behind synthetic controlUnderstand synthetic control relates causal inference methods","code":""},{"path":"synthetic-control.html","id":"lab-1","chapter":"9 Synthetic control","heading":"9.2 Lab","text":"Nov 13 SlidesWe review main idea behind synthetic control well compare contrast synthetic control matching difference--differences. also dig deeper select weights synthetic control review worked example assess performance method.","code":""},{"path":"synthetic-control.html","id":"discussion-1","chapter":"9 Synthetic control","heading":"9.3 Discussion","text":"Nov 14 Slides 2024.. Read Chapter 10 Causal Inference Mixtape Cunningham 2021After class able :Understand conduct hypothesis test estimates\nsynthtic control analysisReason interference affect estimates synthetic control analysis","code":""},{"path":"current-research.html","id":"current-research","chapter":"10 Current research","heading":"10 Current research","text":"","code":""},{"path":"current-research.html","id":"research-discussion-mayleen","chapter":"10 Current research","heading":"10.1 Research discussion: Mayleen","text":"Dec 3 Slides 2024.Today discussing interference, challenges brings causal inference, proposed solutions.","code":""},{"path":"course-recap.html","id":"course-recap","chapter":"11 Course recap","heading":"11 Course recap","text":"Dec 5 Slides 2024.review learned semester!","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"discussion-1.-prob-stats-review","chapter":"Discussion 1. Prob & Stats Review","heading":"Discussion 1. Prob & Stats Review","text":"","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"stsciinfoilrst-3900-causal-inference","chapter":"Discussion 1. Prob & Stats Review","heading":"STSCI/INFO/ILRST 3900: Causal Inference","text":"","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"august-27-2025","chapter":"Discussion 1. Prob & Stats Review","heading":"August 27, 2025","text":"execute simulations locally, download .Rmd ","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"announcements","chapter":"Discussion 1. Prob & Stats Review","heading":"Announcements","text":"Office Hours throughout week (see Syllabus website)\nFilippo: Thursday 4-5pm 321A Computing & Information Science Building\nShira: Monday 5-6 pm 329A Computing Information Science Building\nFilippo: Thursday 4-5pm 321A Computing & Information Science BuildingShira: Monday 5-6 pm 329A Computing Information Science Building","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"probability-and-statistics-review","chapter":"Discussion 1. Prob & Stats Review","heading":"Probability and Statistics Review","text":"ExpectationVarianceConditional ExpectationIndependenceBernoulli Random VariablesLaw Total ExpectationConfidence IntervalsRegression (OLS, logistic)","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"expectation","chapter":"Discussion 1. Prob & Stats Review","heading":"1. Expectation","text":"","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"expected-value-population-mean-average","chapter":"Discussion 1. Prob & Stats Review","heading":"(Expected Value, Population Mean, Average)","text":"Notation: \\(E(X), \\mu\\)expected value finite random variable\n\\[\\mu=E(X) := \\sum_{=1}^N x_i P(x_i) \\quad \\text{} \\quad P(x_i):=\\text{prob}(X=x_i)\\]expected value countable random variable, .e. (long run) average\n\\[E(X):= \\sum_{=1}^\\infty x_i P(x_i)\\]\\(n\\) independent identically distributed (..d.) random variables \\(X_1,\\ldots,X_N\\)\nsample mean \n\\[\\bar X = \\frac{1}{N}\\sum_{=1}^N X_i\\]Law Large Numbers (LLN): sample mean converges expected value (population mean) \\(N \\\\infty\\)Example: \\(X_i\\) random draws \\(\\sim \\mathcal{N}(2,5)\\) (Normal r.v. mean 2, variance 5)","code":"\ntrue_mean <- 2\ntrue_var <- 5\n\nx <- seq(-8, 12 , length=1000)\ny <- dnorm(x, mean=true_mean, sd=sqrt(true_var))\n\nggplot() + geom_line(aes(x,y)) + \n    geom_vline(xintercept = true_mean, color = \"red\") +\n    theme_bw() +\n    labs(y=\"Density\")\nsample_seq <- 1:3000\nmeans <- numeric(length(sample_seq))\nvars <- numeric(length(sample_seq))\n\n\nfor(i in 1:length(sample_seq)){\n  n <- sample_seq[i]\n  data <- rnorm(n = n, mean = true_mean, sd = sqrt(true_var))\n  \n  sample_mean <- mean(data)\n  sample_var <- sum((data - sample_mean)^2)/length(data)\n  \n  means[i] <- sample_mean\n  vars[i] <- sample_var\n}\n\n\nmeans <- tibble(\"N\" = sample_seq, \"Sample Mean\" = means)\nvars <- tibble(\"N\" = sample_seq, \"Sample Variance\" = vars)\n\ncolors <- c(\"Sample Mean\" = \"lightblue\", \"Population Mean\" = \"red\")\n\nggplot(means, aes(y = `Sample Mean`, x = N)) +\n  geom_line(color = \"lightblue\") +\n  geom_abline(slope = 0, intercept = true_mean, color = \"red\") +\n  theme_bw()"},{"path":"discussion-1.-prob-stats-review.html","id":"variance","chapter":"Discussion 1. Prob & Stats Review","heading":"2. Variance","text":"","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"describes-the-spread-of-the-data","chapter":"Discussion 1. Prob & Stats Review","heading":"Describes the spread of the data","text":"Notation: \\(V(X), Var(X),\\sigma^2\\)Variance average squared differences meanFor random variable \\(X\\) expected value \\(\\mu:=E(X)\\), variance \n\\[\\sigma^2 = Var(X) := E\\Big[\\big(X-\\mu\\big)^2\\Big] = E\\big[X^2\\big] - \\mu^2\\]\nexplicitly\n\\[Var(X) = \\sum_{=1}^n P(x_i)\\cdot (x_i-\\mu)^2 \\quad \\text{} \\quad P(x_i):=\\text{prob}(X=x_i)\\]","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"sample-empirical-variance","chapter":"Discussion 1. Prob & Stats Review","heading":"3. Sample (Empirical) Variance","text":"","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"for-a-finite-dataset-or-finite-sample","chapter":"Discussion 1. Prob & Stats Review","heading":"For a finite dataset or finite sample","text":"practice, can compute variance finite dataset \n\\[\\sigma^2 = \\Big(\\frac{1}{N}\\sum_{=1}^N x_i^2\\Big)-\\bar{X}^2 \\quad \\text{} \\quad \\bar{X} := \\frac{1}{N}\\sum_{=1}^N x_i\\]don’t need formula memorized, just aware itLikely ’ll never explicitly compute way, just use R functionExample: \\(X_i\\) random draws \\(\\sim \\mathcal{N}(2,5)\\) (Normal r.v. mean 2, variance 5)","code":"\nggplot(vars, aes(y = `Sample Variance`, x = N)) +\n  geom_line(color = \"lightblue\") +\n  geom_abline(slope = 0, intercept = true_var, color = \"red\") +\n  theme_bw()"},{"path":"discussion-1.-prob-stats-review.html","id":"conditional-expectation","chapter":"Discussion 1. Prob & Stats Review","heading":"4. Conditional Expectation","text":"Notation: \\(E(X|Y)\\)expected value given set “conditions”Read “expectation \\(X\\) given (conditioned ) \\(Y\\)”\n\\[E(X|Y) = \\sum_{=1}^n x_i \\cdot P(X=x_i | Y) \\quad \\text{} \\quad P(X=x_i|Y) = \\frac{P(X=x_i \\text{ } Y)}{P(Y)}\\]Example: Roll fair dice\nLet \\(=1\\) roll even number, \\(0\\) otherwise\nLet \\(B=1\\) roll prime number, \\(0\\) otherwise\n\n\\[E[] = \\sum_{=1}^6 a_i\\cdot P(a_i) = \\frac{0+1+0+1+0+1}{6} = \\frac{1}{2}\\]\nconditional expectation \\(\\) given \\(B=1\\) (.e. rolled 2, 3, 5)\n\\[E[| B=1]= \\sum_{=1}^3 a_i\\cdot P(a_i|B=1) = \\frac{1 + 0 + 0}{3}= \\frac{1}{3}\\]\nLet \\(=1\\) roll even number, \\(0\\) otherwiseLet \\(B=1\\) roll prime number, \\(0\\) otherwiseThen\n\\[E[] = \\sum_{=1}^6 a_i\\cdot P(a_i) = \\frac{0+1+0+1+0+1}{6} = \\frac{1}{2}\\]\nconditional expectation \\(\\) given \\(B=1\\) (.e. rolled 2, 3, 5)\n\\[E[| B=1]= \\sum_{=1}^3 a_i\\cdot P(a_i|B=1) = \\frac{1 + 0 + 0}{3}= \\frac{1}{3}\\]Visualization R \\(E(X)=25\\), \\(E[X| \\text{group 1}] = 20\\), \\(E[X| \\text{group 2}] = 30\\)","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"independence","chapter":"Discussion 1. Prob & Stats Review","heading":"5. Independence","text":"Notation: \\(\\perp, \\ X \\perp Y\\)Two random variables independent outcome one give information outcome otherEvents \\(\\) \\(B\\) independent \\(P(\\text{ } B) =P(\\cap B) = P()P(B)\\)Recall: \\(P(\\cap B) = P(| B)P(B)\\)\\(\\perp B\\) , \\(P(|B)=P() \\text{ } P(B|)=P(B)\\)Example:\nSuppose roll two fair dice. Let \\(\\) value first dice let \\(B\\) value second dice.\nsay \\(=3\\), give info value \\(B\\) ?\ncan show events \\(=3\\) \\(B=3\\) independent:\n\\[\\begin{align*}\n  P(\\{=3\\} \\cap \\{B=3\\}) &= P(\\{=3\\} | \\{B=3\\})\\cdot P(\\{B=3\\}) \\\\\n  &= \\frac{1}{6} \\cdot \\frac{1}{6} \\\\\n  &= P(\\{=3\\}) \\cdot P(\\{B=3\\})\n  \\end{align*}\\]\nshow \\(\\perp B\\), show holds values \\(\\) \\(B\\)\nSuppose roll two fair dice. Let \\(\\) value first dice let \\(B\\) value second dice.say \\(=3\\), give info value \\(B\\) ?can show events \\(=3\\) \\(B=3\\) independent:\n\\[\\begin{align*}\n  P(\\{=3\\} \\cap \\{B=3\\}) &= P(\\{=3\\} | \\{B=3\\})\\cdot P(\\{B=3\\}) \\\\\n  &= \\frac{1}{6} \\cdot \\frac{1}{6} \\\\\n  &= P(\\{=3\\}) \\cdot P(\\{B=3\\})\n  \\end{align*}\\]show \\(\\perp B\\), show holds values \\(\\) \\(B\\)","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"bernoulli-random-variables","chapter":"Discussion 1. Prob & Stats Review","heading":"6. Bernoulli Random Variables","text":"","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"a-binarydichotomous-random-variable","chapter":"Discussion 1. Prob & Stats Review","heading":"A binary/dichotomous random variable","text":"Notation: \\(B(p), \\text{Bernoulli}(p), \\mathcal{B}(p)\\)Takes value \\(1\\) probability (w.p.) \\(p\\), value \\(0\\) w.p. \\(q:=1-p\\)Let \\(X \\sim B(p)\\):\n“Let \\(X\\) Bernoulli random variable mean \\(p\\)”\n\\(E(X) = p \\text{ } Var(X) = p(1-p) = pq\\)\n“Let \\(X\\) Bernoulli random variable mean \\(p\\)”\\(E(X) = p \\text{ } Var(X) = p(1-p) = pq\\)Cool fact: \\(E(X) = P(X=1) = p\\)","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"law-of-total-expectation","chapter":"Discussion 1. Prob & Stats Review","heading":"7. Law of Total Expectation","text":"","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"i.e.-law-of-iterated-expectations-tower-rule","chapter":"Discussion 1. Prob & Stats Review","heading":"(i.e. law of iterated expectations, tower rule)","text":"Useful property (“trick) used class \\[E(X) = E\\big(E(X|Y)\\big) \\]Don’t worry much technical details, just add toolbox :)","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"confidence-intervals","chapter":"Discussion 1. Prob & Stats Review","heading":"8. Confidence Intervals","text":"set values contains real parameter probability \\(1-\\alpha\\)Define \\(CI=[L,U]\\) \\(P(L \\leq \\mu \\leq U)= 1-\\alpha\\)Usually \\(1-\\alpha\\) \\(95\\%\\) \\(99\\%\\)Example: \\(X_i\\) random draws \\(\\sim \\mathcal{N}(2,5)\\)Estimating expectation random variable using sample mean:\n\\[\\hat E(X)=\\hat\\mu= \\bar X =\\frac{1}{N}\\sum_{=1}^N X_i\\]\\(\\bar X\\) estimate \\(\\mu\\) uncertainty\\(P(\\mu \\leq \\bar X -c)=P(\\mu \\geq \\bar X +c)=\\frac{\\alpha}{2}\\)\\(P\\left(\\frac{\\bar X-\\mu}{\\sigma/\\sqrt{N}}\\leq \\frac{\\mu-c-\\mu}{\\sigma/\\sqrt{N}}\\right) \\Rightarrow -c=Z_\\frac{\\alpha}{2}\\frac{\\sigma}{\\sqrt{N}}\\)\\(Z_\\frac{\\alpha}{2}\\) critical value Normal distribution (example R: \\(\\texttt{qnorm(0.025)})\\)\\(CI= \\bar X \\pm Z_\\frac{\\alpha}{2} \\frac{\\sigma}{\\sqrt{N}}\\)","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"regression","chapter":"Discussion 1. Prob & Stats Review","heading":"9. Regression","text":"Estimates relationships \\(X\\) \\(Y\\) \\(Y\\)- dependent variable, outcome/response\\(X\\)- independent variable, regressor/explanatoryMain types regression: Linear Logistic","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"linear-regression","chapter":"Discussion 1. Prob & Stats Review","heading":"9.1. Linear Regression","text":"Assume data generated: \\(Y_i=\\alpha+\\beta X_i+\\varepsilon_i\\) \\(=1,\\ldots,N\\)\\(\\alpha, \\beta\\) coefficients \\(\\alpha\\) intercept \\(\\beta\\) slope\n\nUsing ordinary least squares (OLS) estimate \\(\\hat Y_i=\\hat\\alpha+\\hat\\beta X_i\\)Minimizes sum squared errors: \\((\\hat \\alpha,\\hat \\beta)=\\mathrm{argmin}_{,b} \\sum_{=1}^N\\big(Y_i-(+bX_i)\\big)^2\\)\\(\\frac{\\partial}{\\partial } SSE = \\sum_{=1}^N -2(Y_i--bX_i) \\qquad \\Rightarrow \\qquad \\hat \\alpha=\\bar Y-\\hat \\beta \\bar X\\)\\(\\frac{\\partial}{\\partial b} SSE = \\sum_{=1}^N -2(Y_i-(\\bar Y-b\\bar X) -bX_i)X_i\\)\n\\(\\qquad \\qquad = \\sum_{=1}^N -2\\big[(Y_i-\\bar Y)X_i-b(X_i-\\bar X)X_i \\big]\\)\n\\(\\qquad\\qquad\\qquad \\Rightarrow \\hat \\beta=\\frac{\\sum_{=1}^N (Y_i-\\bar Y)(X_i-\\bar X)}{\\sum_{=1}^N (X_i-\\bar X)^2}\\)","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"logistic-regression","chapter":"Discussion 1. Prob & Stats Review","heading":"9.2. Logistic Regression","text":"\\(Y_i\\)- outcome variable binary \\(=1,\\ldots,N\\)Use link function estimate \\(P(Y_i=1):=p_i\\) satisfies \\(\\mathbb{R} \\(0,1)\\)\ncommon- logistic function: \\(\\sigma(t)=\\frac{1}{1+e^{-t}}\\)\n\n\ncommon- logistic function: \\(\\sigma(t)=\\frac{1}{1+e^{-t}}\\)\n\nlinear model estimate \\(\\hat Y_i=\\hat\\alpha+\\hat\\beta X_i\\)logistic model estimate \\(\\hat p_i= \\frac{1}{1+e^{-(\\hat\\alpha+\\hat\\beta X_i)}}\\)\\(\\alpha+\\beta X_i= \\ln\\left(\\frac{ p_i}{1- p_i}\\right)\\)Odds ratio: \\(\\frac{ p_i}{1- p_i}=\\frac{P(Y_i=1)}{P(Y_1=0)}\\)example: \\(\\frac{P(\\text{Passing exam})}{P(\\text{passing})}=\\frac{3/4}{1/4}\\) odds ratio \\(3:1\\)estimate \\(\\hat \\alpha, \\hat \\beta\\) use maximum likelihood estimates (MLE)Likelihood function: \\(L(,b;y)= \\prod_{=1}^N P(Y_i=y_i)= \\prod_{=1}^Np_i^{y_i}(1-p_i)^{(1-y_i)}\\)Log likelihood: \\(l(,b;y)= \\sum_{=1}^N y_i\\ln(p_i)+(1-y_i)\\ln(1-p_i)=\\sum_{=1}^N \\ln(1-p_i)+y_i \\ln\\left(\\frac{p_i}{1-p_i}\\right)\\)find MLE solve \\(\\frac{\\partial}{\\partial (,b)}l(,b;y)=0\\)close form solution iterative method : gradient descent Newton–Raphson\n\n","code":""},{"path":"discussion-1.-prob-stats-review.html","id":"rrstudio-intro","chapter":"Discussion 1. Prob & Stats Review","heading":"R/RStudio Intro","text":"R open-source programming languageUsed statistical computing creating plotsDownload install R\n\nRStudio open-source IDE (integrated development environment)Download install RStudio (scroll earlier versions)\n\ninstall.packages(“rmarkdown”)install.packages(“knitr”)tinytex::install_tinytex()Download .Rmd open RStudioCompile PDF (HW submission PDF file)\n\nR Markdown tutorial  open RStudio\n\nSubscripts superscripts: get \\(Y_{}^{}\\) inline use $Y_{}^{}$","code":""},{"path":"discussion-2.-analyzing-an-experiment-in-r.html","id":"discussion-2.-analyzing-an-experiment-in-r","chapter":"Discussion 2. Analyzing an Experiment in R","heading":"Discussion 2. Analyzing an Experiment in R","text":"","code":""},{"path":"discussion-2.-analyzing-an-experiment-in-r.html","id":"stsciinfoilrst-3900-causal-inference-1","chapter":"Discussion 2. Analyzing an Experiment in R","heading":"STSCI/INFO/ILRST 3900: Causal Inference","text":"","code":""},{"path":"discussion-2.-analyzing-an-experiment-in-r.html","id":"september-3-2025","chapter":"Discussion 2. Analyzing an Experiment in R","heading":"September 3, 2025","text":"","code":""},{"path":"discussion-2.-analyzing-an-experiment-in-r.html","id":"announcements-1","chapter":"Discussion 2. Analyzing an Experiment in R","heading":"Announcements","text":"HW 1 due Tuesday (September 9) 5pm\nSubmit PDF RMarkdown via Canvas\nSubmit PDF RMarkdown via CanvasOffice Hours throughout week (see Syllabus website)\nFilippo: Thursday 4-5pm 321A CIS Building\nShira: Monday 5-6 pm 329A CIS Building\nSam: Tuesday 4-5pm, CIS Building\nFilippo: Thursday 4-5pm 321A CIS BuildingShira: Monday 5-6 pm 329A CIS BuildingSam: Tuesday 4-5pm, CIS Building","code":""},{"path":"discussion-2.-analyzing-an-experiment-in-r.html","id":"get-out-and-vote-experiment","chapter":"Discussion 2. Analyzing an Experiment in R","heading":"Get out and Vote Experiment","text":"people vote?One long-standing theory: People vote due social norms (civic duty)Empirical evidence theory extremely thinResearch Question: extent social norms cause voter turnout?Article: “Social Pressure Voter Turnout: Evidence Large-scale Field Experiment.”American Political Science ReviewAuthors: Alan S. Gerber, Donald P. Green, Christopher W. LarimerWe’ll analyzing experiment today!","code":""},{"path":"discussion-2.-analyzing-an-experiment-in-r.html","id":"experimental-design","chapter":"Discussion 2. Analyzing an Experiment in R","heading":"Experimental Design","text":"Approximately 80k Michigan households randomly assigned 1 4 mailings encouraging vote\nSimply reminded voting civic duty\nTold researchers studying turnout based public records\nReceived record voting turnout within household\nReceived record voting turnout within household neighbors’ households.\nSimply reminded voting civic dutyTold researchers studying turnout based public recordsReceived record voting turnout within householdReceived record voting turnout within household neighbors’ households.Third fourth treatment arms told turnout revealed well","code":""},{"path":"discussion-2.-analyzing-an-experiment-in-r.html","id":"goal-for-today","chapter":"Discussion 2. Analyzing an Experiment in R","heading":"Goal for Today","text":"","code":""},{"path":"discussion-2.-analyzing-an-experiment-in-r.html","id":"resources-for-markdown","chapter":"Discussion 2. Analyzing an Experiment in R","heading":"Resources for Markdown","text":"Hadley Wickham’s R Data Science “Chapter 27”“RMarkdown cheat sheet” RStudio“Data Wrangling Analyses Tidyverse” Bookdown“RMarkdown Scientists” Nicholas TierneyIf can’t figure something, try Googling first!Also feel free ask classmate ask :)homework sets, don’t forget Ed Discussion!","code":""},{"path":"discussion-2.-analyzing-an-experiment-in-r.html","id":"step-1-download-the-.rmd-file-here","chapter":"Discussion 2. Analyzing an Experiment in R","heading":"Step 1: Download the .Rmd file here","text":"Start running code Section “Necessary packages”get error, may need install package","code":""},{"path":"discussion-2.-analyzing-an-experiment-in-r.html","id":"step-2-import-and-clean-the-data","chapter":"Discussion 2. Analyzing an Experiment in R","heading":"Step 2: Import and Clean the Data","text":"Quick peek dataset using function glimpseNotice information year birth yob explicitly ageNotice treatments labeled numbers 0 4Calculate ages everyone dataset year 2006Use mutatet() construct age variableYou can arithmetic operations information datasetFor example: mutate(col_3 = col_1 + col_2)Replace numeric labels treatment (0-4) word labels:\n0: “Control”\n1: “Hawthorne” (‘researchers viewing records via public data’ treatment arm)\n2: “Civic Duty” (‘voting civic duty’ treatment arm)\n3: “Neighbors” (‘voting turnout revealed neighbors’ treatment arm)\n4: “Self” (‘voting turnout revealed household’ treatment arm)\n0: “Control”1: “Hawthorne” (‘researchers viewing records via public data’ treatment arm)2: “Civic Duty” (‘voting civic duty’ treatment arm)3: “Neighbors” (‘voting turnout revealed neighbors’ treatment arm)4: “Self” (‘voting turnout revealed household’ treatment arm), want use function case_when described hereThe general syntax case_when(condition ~ output-value)condition treatement == 0 output value \"Control\"run glimpse(gotv), see something like ","code":"\ngotv <- read_dta(\"https://causal3900.github.io/assets/data/social_pressure.dta\")\nglimpse(gotv)## Rows: 344,084\n## Columns: 16\n## $ sex           <dbl+lbl> 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,…\n## $ yob           <dbl> 1941, 1947, 1951, 1950, 1982, 1981, …\n## $ g2000         <dbl+lbl> 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,…\n## $ g2002         <dbl+lbl> 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,…\n## $ g2004         <dbl+lbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n## $ p2000         <dbl+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n## $ p2002         <dbl+lbl> 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,…\n## $ p2004         <dbl+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n## $ treatment     <dbl+lbl> 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0,…\n## $ cluster       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n## $ voted         <dbl+lbl> 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,…\n## $ hh_id         <dbl> 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 5, 6, …\n## $ hh_size       <dbl> 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 1, 2, …\n## $ numberofnames <dbl> 21, 21, 21, 21, 21, 21, 21, 21, 21, …\n## $ p2004_mean    <dbl> 0.09523810, 0.09523810, 0.04761905, …\n## $ g2004_mean    <dbl> 0.8571429, 0.8571429, 0.8571429, 0.8…\ngotv <- gotv |>\n  mutate(age = )\ngotv <- gotv |>\n  mutate(treatment = case_when()) "},{"path":"discussion-2.-analyzing-an-experiment-in-r.html","id":"step-3-table-1","chapter":"Discussion 2. Analyzing an Experiment in R","heading":"Step 3: Table 1","text":"data balanced covariates?want check treatment groups balanced covariatesFor treatment arm/group, calculate mean designated covariates:\nHousehold size, Nov 2002, Nov 2000, Aug 2004, Aug 2002, Aug 2000, Female, Age (years)\nHousehold size, Nov 2002, Nov 2000, Aug 2004, Aug 2002, Aug 2000, Female, Age (years)Use group_by() calculate separate means treatment armUse summarise() computes mean covariate covariatesYour table look like (covariates similar across groups)\n\n","code":"\ncovariates <- c(\"sex\", \"age\", \"g2000\", \"g2002\", \"p2000\", \"p2002\", \"p2004\", \"hh_size\")\n\ngotv_balance <- gotv |>\n  group_by(...) |>\n  summarise(...)\n\nprint(gotv_balance)"},{"path":"discussion-2.-analyzing-an-experiment-in-r.html","id":"step-4-table-2","chapter":"Discussion 2. Analyzing an Experiment in R","heading":"Step 4: Table 2","text":"results experiment?treatment group, calculate percent voted total number individuals groupUse group_by() calculate separate means treatment armUse summarise() following:\nCreate column Percentage_Voting- percent voted group\nCreate column num_of_individuals- total number people group\nCreate column Percentage_Voting- percent voted groupCreate column num_of_individuals- total number people groupYour table look like \n\n","code":"\ngotv_results <- gotv |>\n  group_by(...) |>\n  summarise(...)\n\nprint(gotv_results)"},{"path":"discussion-3.-treatment-effect-heterogneity-in-an-experiment.html","id":"discussion-3.-treatment-effect-heterogneity-in-an-experiment","chapter":"Discussion 3. Treatment effect heterogneity in an Experiment","heading":"Discussion 3. Treatment effect heterogneity in an Experiment","text":"","code":""},{"path":"discussion-3.-treatment-effect-heterogneity-in-an-experiment.html","id":"stsciinfoilrst-3900-causal-inference-2","chapter":"Discussion 3. Treatment effect heterogneity in an Experiment","heading":"STSCI/INFO/ILRST 3900: Causal Inference","text":"","code":""},{"path":"discussion-3.-treatment-effect-heterogneity-in-an-experiment.html","id":"september-10-2025","chapter":"Discussion 3. Treatment effect heterogneity in an Experiment","heading":"September 10, 2025","text":"can download slides. week’s discussion.","code":""},{"path":"discussion-3.-treatment-effect-heterogneity-in-an-experiment.html","id":"get-out-and-vote-experiment-1","chapter":"Discussion 3. Treatment effect heterogneity in an Experiment","heading":"Get out and Vote Experiment","text":"Last week, explored experiment digs mechanisms\nunderlying people vote. exercise based :Gerber, Alan S., Donald P. Green, Christopher W. Larimer. “Social Pressure Voter Turnout: Evidence Large-scale Field Experiment.” American Political Science Review 102.1 (2008): 33-48.long-standing theory many people\nvote driven social norms (e.g. understanding voting\ncivic duty). theory, dominant theoretical\nexplanation, little empirical backing long time. experiment\nexamines theory asking question:\nextent social norms cause voter turnout?","code":""},{"path":"discussion-3.-treatment-effect-heterogneity-in-an-experiment.html","id":"experimental-design-1","chapter":"Discussion 3. Treatment effect heterogneity in an Experiment","heading":"Experimental Design","text":"order answer question, approximately 80,000 Michigan households\nrandomly assigned treatment control groups, treatment\ngroup randomly assigned one four possible treatment arms. \ntreatment arms varied intensity social pressure conveyed,\ndefined follows:first treatment arm mailed letter simply reminded \nvoting civic duty.second treatment arm mailed letter telling researchers\nstudying voting turnout based public records.third treatment arm mailed letter stating voting turnout\nrevealed members household.fourth treatment arm mailed letter stating voting turnout\nrevealed household neighbors.","code":""},{"path":"discussion-3.-treatment-effect-heterogneity-in-an-experiment.html","id":"analyze-experiment","chapter":"Discussion 3. Treatment effect heterogneity in an Experiment","heading":"Analyze Experiment","text":"","code":""},{"path":"discussion-3.-treatment-effect-heterogneity-in-an-experiment.html","id":"necessary-packages","chapter":"Discussion 3. Treatment effect heterogneity in an Experiment","heading":"Necessary packages","text":"","code":"\nlibrary(dplyr)\nlibrary(haven)\nlibrary(kableExtra)"},{"path":"discussion-3.-treatment-effect-heterogneity-in-an-experiment.html","id":"import-data","chapter":"Discussion 3. Treatment effect heterogneity in an Experiment","heading":"Import data","text":"","code":"\ngotv <- read_dta(\"https://causal3900.github.io/assets/data/social_pressure.dta\")"},{"path":"discussion-3.-treatment-effect-heterogneity-in-an-experiment.html","id":"clean-data","chapter":"Discussion 3. Treatment effect heterogneity in an Experiment","heading":"Clean data","text":"First, construct age variable describing old (number years) person year 2006. yob variable says year person born . > , use mutate function.\n, convert treatment variable ’s numeric representation corresponding labels are0: “Control”1: “Hawthorne” (‘researchers viewing records via public data’ treatment arm)2: “Civic Duty” (‘voting civic duty’ treatment arm)3: “Neighbors” (‘voting turnout revealed neighbors’ treatment arm)4: “Self” (‘voting turnout revealed household’ treatment arm), use case_when function.","code":"\ngotv <- gotv |>\n  mutate(treatment = case_when(\n    treatment == 0 ~ \"Control\",\n    treatment == 1 ~ \"Hawthorne\",\n    treatment == 2 ~ \"Civic Duty\",\n    treatment == 3 ~ \"Neighbors\",\n    treatment == 4 ~ \"Self\")) \n\n\ngotv <- gotv |>\n  mutate(age = 2006 - yob)"},{"path":"discussion-3.-treatment-effect-heterogneity-in-an-experiment.html","id":"average-causal-effect","chapter":"Discussion 3. Treatment effect heterogneity in an Experiment","heading":"Average Causal Effect","text":"Finally, treatment group, calculate percentage individuals got voted, well total number individuals group! solutions use function n counts number observations current group .","code":"\ngotv_results <- gotv |>\n  group_by(treatment) |>\n  summarise(Per_Voting = mean(voted), num_of_individuals = n())\n\nprint(gotv_results)## # A tibble: 5 × 3\n##   treatment  Per_Voting num_of_individuals\n##   <chr>           <dbl>              <int>\n## 1 Civic Duty      0.315              38218\n## 2 Control         0.297             191243\n## 3 Hawthorne       0.322              38204\n## 4 Neighbors       0.378              38201\n## 5 Self            0.345              38218"},{"path":"discussion-3.-treatment-effect-heterogneity-in-an-experiment.html","id":"conditional-average-causal-effect","chapter":"Discussion 3. Treatment effect heterogneity in an Experiment","heading":"Conditional Average Causal Effect","text":"Now, look treatment effect across sub-population, can determine treatment effect heterogeneityFirst, assign age groups household size groups","code":"\ngotv <- gotv |>\n  mutate(ageGroup = cut(age, breaks = c(18, 30, 45, 60, 120))) |>\n  mutate(hhGroup = cut(hh_size, breaks = c(0,1, 2, 10)))"},{"path":"discussion-3.-treatment-effect-heterogneity-in-an-experiment.html","id":"examine-voting-by-age-group","chapter":"Discussion 3. Treatment effect heterogneity in an Experiment","heading":"Examine voting by age group","text":"","code":"\ngotv_results_age <- gotv |>\n  group_by(ageGroup, treatment) |>\n  summarise(\n    Per_Voting = mean(voted),\n    Count = n(),\n    .groups = \"drop\"\n  ) |>\n  group_by(treatment) |>\n  mutate( Per_in_AgeGroup = Count / sum(Count))\n\nprint(gotv_results_age, n = Inf)## # A tibble: 20 × 5\n## # Groups:   treatment [5]\n##    ageGroup treatment  Per_Voting Count Per_in_AgeGroup\n##    <fct>    <chr>           <dbl> <int>           <dbl>\n##  1 (18,30]  Civic Duty      0.166  4255           0.111\n##  2 (18,30]  Control         0.156 20650           0.108\n##  3 (18,30]  Hawthorne       0.158  4087           0.107\n##  4 (18,30]  Neighbors       0.193  4189           0.110\n##  5 (18,30]  Self            0.175  4139           0.108\n##  6 (30,45]  Civic Duty      0.293  9921           0.260\n##  7 (30,45]  Control         0.268 49917           0.261\n##  8 (30,45]  Hawthorne       0.297 10159           0.266\n##  9 (30,45]  Neighbors       0.356 10026           0.262\n## 10 (30,45]  Self            0.317 10043           0.263\n## 11 (45,60]  Civic Duty      0.320 16086           0.421\n## 12 (45,60]  Control         0.310 80330           0.420\n## 13 (45,60]  Hawthorne       0.338 15926           0.417\n## 14 (45,60]  Neighbors       0.391 15735           0.412\n## 15 (45,60]  Self            0.357 15968           0.418\n## 16 (60,120] Civic Duty      0.410  7956           0.208\n## 17 (60,120] Control         0.378 40346           0.211\n## 18 (60,120] Hawthorne       0.407  8032           0.210\n## 19 (60,120] Neighbors       0.474  8251           0.216\n## 20 (60,120] Self            0.444  8068           0.211"},{"path":"discussion-3.-treatment-effect-heterogneity-in-an-experiment.html","id":"examine-voting-by-hh-size-group","chapter":"Discussion 3. Treatment effect heterogneity in an Experiment","heading":"Examine voting by hh size group","text":"","code":"\ngotv_results_hh <- gotv |>\n  group_by(hhGroup, treatment) |>\n  summarise(\n    Per_Voting = mean(voted),\n    Count = n(),\n    .groups = \"drop\"\n  ) |>\n  group_by(treatment) |>\n  mutate( Per_in_hhGroup = Count / sum(Count) ) \n\nprint(gotv_results_hh, n = Inf)## # A tibble: 15 × 5\n## # Groups:   treatment [5]\n##    hhGroup treatment  Per_Voting  Count Per_in_hhGroup\n##    <fct>   <chr>           <dbl>  <int>          <dbl>\n##  1 (0,1]   Civic Duty      0.354   5398          0.141\n##  2 (0,1]   Control         0.331  26481          0.138\n##  3 (0,1]   Hawthorne       0.370   5281          0.138\n##  4 (0,1]   Neighbors       0.423   5364          0.140\n##  5 (0,1]   Self            0.400   5310          0.139\n##  6 (1,2]   Civic Duty      0.327  23536          0.616\n##  7 (1,2]   Control         0.303 119022          0.622\n##  8 (1,2]   Hawthorne       0.326  23998          0.628\n##  9 (1,2]   Neighbors       0.391  23738          0.621\n## 10 (1,2]   Self            0.352  23792          0.623\n## 11 (2,10]  Civic Duty      0.261   9284          0.243\n## 12 (2,10]  Control         0.261  45740          0.239\n## 13 (2,10]  Hawthorne       0.285   8925          0.234\n## 14 (2,10]  Neighbors       0.318   9099          0.238\n## 15 (2,10]  Self            0.296   9116          0.239"},{"path":"discussion-3.-treatment-effect-heterogneity-in-an-experiment.html","id":"questions","chapter":"Discussion 3. Treatment effect heterogneity in an Experiment","heading":"Questions:","text":"seem heterogeneity treatment effects across age /house hold size?improve voting rates assigning different treatments different individuals?expect treatment effect civic duty considered population evenly split across 4 age groups?answer questions might useful slightly rearrange table:","code":"\ngotv_results_age <- gotv |>\n  group_by(ageGroup, treatment) |>\n  summarise(\n    Per_Voting = mean(voted),\n    Count = n(),\n    .groups = \"drop\"\n  ) |>\n  group_by(treatment) |>\n  mutate(\n    Per_in_AgeGroup = Count / sum(Count)\n  ) |>\n  group_by(ageGroup) |>\n  mutate(\n    Control_Voting = Per_Voting[treatment == \"Control\"],\n    Difference_from_Control = Per_Voting - Control_Voting\n  ) |>\n  ungroup()\n\n\ngotv_results_age |>\n  arrange(treatment,ageGroup) |>\n  kbl() |>\n  kable_styling(font_size = 12, full_width = FALSE) |>\n  scroll_box(width = \"100%\", height = \"500px\")"},{"path":"discussion-3.-treatment-effect-heterogneity-in-an-experiment.html","id":"answers","chapter":"Discussion 3. Treatment effect heterogneity in an Experiment","heading":"Answers:","text":"seem heterogeneity treatment effects across age /house hold size?say treatment effect heterogeneity treatment effect varies across sub-population. check ’s treatment effect heterogeneity across age groups, look \\(E[Y^{=j}|L=l]-E[Y^{=0}|L=l]\\) age group \\(l\\), treatment \\(j\\).\nexample, “Civic Duty” treatment effect individuals ages 18-30 \n\\[\\begin{align*}\nE\\big[Y^{=\"Civic Duty\"}|L=(18-30]\\big]-&E\\big[Y^{=\"Control\"}|L=(18-30]\\big]]\\\\ &= 0.166-0.156\\\\&=0.001\n\\end{align*}\\]values can found following table, gotv_results_ageGroupWhat expect treatment effect civic duty considered population evenly split across 4 age groups?First, let’s consider average treatment effect Civic Duty, given :\n\\[E[Y|=\\text{Civic Duty}]-E[Y|=\\text{Control}]\\]\nStandardization allows us estimate ACE combining\nestimates sub-population\n\\[\\sum_l P(L=l)E[Y|=\\text{Civic Duty},L=l]-\\sum_l P(L=l)E[Y|=\\text{Control},L=l]\\]\n\\[\\sum_l P(L=l) \\Big(E[Y|=\\text{Civic Duty},L=l]-E[Y|=\\text{Control},L=l]\\Big)\\]\nage group ACE looks like:\n\\[\\begin{align*} ACE =& 0.111 \\times (0.166-0.156)  \\\\  &+0.260 \\times (0.293-0.268)\\\\&+0.421\\times (0.320-0.310)\\\\&+ 0.208\\times (0.410-0.378)\n\\end{align*}\\]estimate treatment effect civic duty population evenly split across 4 age group, replace share age group \\(0.25\\).","code":"\ngotv_results_ageGroup <- gotv |>\n  group_by(ageGroup, treatment) |>\n  summarise(\n    Per_Voting = mean(voted),\n    Count = n())\ngotv_results_age |>\n   filter(treatment==\"Civic Duty\") |>\n   summarise(sum(Per_in_AgeGroup*Difference_from_Control))## # A tibble: 1 × 1\n##   `sum(Per_in_AgeGroup * Difference_from_Control)`\n##                                              <dbl>\n## 1                                           0.0185\ngotv_results_age |>\n   filter(treatment==\"Civic Duty\") |>\n   summarise(sum(.25*Difference_from_Control))## # A tibble: 1 × 1\n##   `sum(0.25 * Difference_from_Control)`\n##                                   <dbl>\n## 1                                0.0193"},{"path":"discussion-4.-interference-and-stat-review.html","id":"discussion-4.-interference-and-stat-review","chapter":"Discussion 4. Interference and Stat Review","heading":"Discussion 4. Interference and Stat Review","text":"","code":""},{"path":"discussion-4.-interference-and-stat-review.html","id":"stsciinfoilrst-3900-causal-inference-3","chapter":"Discussion 4. Interference and Stat Review","heading":"STSCI/INFO/ILRST 3900: Causal Inference","text":"","code":""},{"path":"discussion-4.-interference-and-stat-review.html","id":"september-17-2025","chapter":"Discussion 4. Interference and Stat Review","heading":"September 17, 2025","text":"","code":""},{"path":"discussion-4.-interference-and-stat-review.html","id":"interference","chapter":"Discussion 4. Interference and Stat Review","heading":"Interference","text":"can download slides week’s discussion.","code":""},{"path":"discussion-4.-interference-and-stat-review.html","id":"stat-review-reminder","chapter":"Discussion 4. Interference and Stat Review","heading":"Stat review reminder","text":"\\(\\pi^= P(Y^=1)\\)) treatment condition \\(\\)Let \\(\\hat\\pi^\\) estimate unknown probability\\(\\hat\\pi^= \\frac{1}{n_a}\\sum_{:A_i=} Y_i^\\), proportion people treatment condition outcome 1To make confidence interval \\(\\hat\\pi^\\) need standard error \\(\\hat\\pi^\\)","code":""},{"path":"discussion-4.-interference-and-stat-review.html","id":"standard-error","chapter":"Discussion 4. Interference and Stat Review","heading":"Standard error","text":"Let \\(Y^\\) Bernoulli random variableThe variance \\(Y^\\) \\(V(Y^) = \\pi^(1-\\pi^)\\)estimated average: \\(\\hat\\pi^\\)many times many hypothetical samples, get different estimateThe estimates sampling variance \\(V(\\hat\\pi^) = \\frac{\\pi^(1-\\pi^)}{n_a}\\)standard error square root sampling variance:\n\\[SE(\\hat\\pi^) = \\sqrt\\frac{\\pi^(1-\\pi^)}{n_a}\\]can estimate SE plugging estimate \\(\\hat\\pi^\\)R: translated standard error formula code","code":"\nse_binary <- function(p, n) {\n  se <- sqrt( p * (1 - p) / n )\n  return(se)\n}"},{"path":"discussion-4.-interference-and-stat-review.html","id":"sampling-distribution","chapter":"Discussion 4. Interference and Stat Review","heading":"Sampling distribution","text":"\\(\\hat\\pi^\\) sample meanBy Central Limit Theorem: \\(n \\\\infty\\), across hypothetical repeated samples distribution \\(\\hat\\pi^\\) estimates becomes NormalAcross repeated samples, middle 95% estimates fall within known range:\n\\[\\pi^\\pm \\Phi^{-1}(.975) \\times SE(\\hat\\pi^)\\]\\(\\Phi^{-1}()\\) quantile standard Normal distribution\\(\\Phi^{-1}(.975) \\approx 1.96\\), number 1.96 might familiar youPlug estimates \\(\\hat\\pi^\\) \\(\\widehat{SE}(\\hat\\pi^)\\) get 95% confidence intervalThe CI centered estimate \\(\\hat\\pi^\\)repeatedly made CI using hypothetical samples population, CI contain unknown true parameter \\(\\pi^\\) 95% timeIn R: translated confidence interval formula code","code":"\nci_lower <- function(estimate, standard_error) {\n  estimate - qnorm(.975) * standard_error\n}\nci_upper <- function(estimate, standard_error) {\n  estimate + qnorm(.975) * standard_error\n}"},{"path":"discussion-5.-directed-acyclic-graphs.html","id":"discussion-5.-directed-acyclic-graphs","chapter":"Discussion 5. Directed Acyclic Graphs","heading":"Discussion 5. Directed Acyclic Graphs","text":"","code":""},{"path":"discussion-5.-directed-acyclic-graphs.html","id":"stsciinfoilrst-3900-causal-inference-4","chapter":"Discussion 5. Directed Acyclic Graphs","heading":"STSCI/INFO/ILRST 3900: Causal Inference","text":"","code":""},{"path":"discussion-5.-directed-acyclic-graphs.html","id":"september-24-2025","chapter":"Discussion 5. Directed Acyclic Graphs","heading":"September 24, 2025","text":"can download slides week’s discussion.following DAGs look whether \\(X_1\\) \\(X_3\\) independent- \\(X_1 \\perp\\!\\!\\!\\!\\perp X_3\\) whether changes condition \\(X_2\\): \\(X_1 \\perp\\!\\!\\!\\!\\perp X_3 \\mid X_2\\)practice, conditioning \\(Y\\) \\(X\\), involves linear regression \\(Y=\\beta X + \\varepsilon\\). ask “part \\(Y\\) explained \\(X\\)?”. residuals \\(Y-\\hat \\beta X\\) represent part \\(Y\\) left removing part explained \\(X\\).","code":""},{"path":"discussion-5.-directed-acyclic-graphs.html","id":"dag-1","chapter":"Discussion 5. Directed Acyclic Graphs","heading":"DAG 1:","text":"\\[X_1 \\rightarrow X_2 \\rightarrow X_3\\]","code":"\nx1 <- rnorm(n = 100, mean = 5, sd = 2)\nx2 <- 2 * x1 + rnorm(n = 100, mean = 0, sd = 1)\nx3 <- -1 * x2 + rnorm(n = 100, mean = 0, sd = 1)\n\nplot(x1,x3, pch=16, main = paste(\"Cor=\",round(cor(x1,x3),3)))\nx1_given_x2 <- lm(x1 ~ x2)$residuals\nx3_given_x2 <- lm(x3 ~ x2)$residuals\nplot(x1_given_x2,x3_given_x2, pch=16, main = paste(\"Conditioning on X2, Cor=\",round(cor(x1_given_x2,x3_given_x2),3)))"},{"path":"discussion-5.-directed-acyclic-graphs.html","id":"dag-2","chapter":"Discussion 5. Directed Acyclic Graphs","heading":"DAG 2:","text":"\\[X_1 \\leftarrow X_2 \\rightarrow X_3\\]","code":"\nx2 <- rnorm(n = 100, mean = 5, sd = 2)\nx1 <- 2 * x2 + rnorm(n = 100, mean = 0, sd = 1)\nx3 <- -1 * x2 + rnorm(n = 100, mean = 0, sd = 1)\n\nplot(x1,x3, pch=16, main = paste(\"Cor=\",round(cor(x1,x3),3)))\nx1_given_x2 <- lm(x1 ~ x2)$residuals\nx3_given_x2 <- lm(x3 ~ x2)$residuals\nplot(x1_given_x2,x3_given_x2, pch=16, main = paste(\"Conditioning on X2, Cor=\",round(cor(x1_given_x2,x3_given_x2),3)))"},{"path":"discussion-5.-directed-acyclic-graphs.html","id":"dag-3","chapter":"Discussion 5. Directed Acyclic Graphs","heading":"DAG 3:","text":"\\[X_1 \\rightarrow X_2 \\leftarrow X_3\\]","code":"\nx1 <- rnorm(n = 100, mean = 5, sd = 2)\nx3 <- rnorm(n = 100, mean = 3, sd = 2)\nx2 <- 2 * x1 -3 * x3 + rnorm(n = 100, mean = 0, sd = 1)\n\nplot(x1,x3, pch=16, main = paste(\"Cor=\",round(cor(x1,x3),3)))\nx1_given_x2 <- lm(x1 ~ x2)$residuals\nx3_given_x2 <- lm(x3 ~ x2)$residuals\nplot(x1_given_x2,x3_given_x2, pch=16, main = paste(\"Conditioning on X2, Cor=\",round(cor(x1_given_x2,x3_given_x2),3)))"},{"path":"discussion-6.-course-project--part-1.html","id":"discussion-6.-course-project--part-1","chapter":"Discussion 6. Course Project- Part 1","heading":"Discussion 6. Course Project- Part 1","text":"","code":""},{"path":"discussion-6.-course-project--part-1.html","id":"stsciinfoilrst-3900-causal-inference-5","chapter":"Discussion 6. Course Project- Part 1","heading":"STSCI/INFO/ILRST 3900: Causal Inference","text":"","code":""},{"path":"discussion-6.-course-project--part-1.html","id":"october-1-2025","chapter":"Discussion 6. Course Project- Part 1","heading":"October 1, 2025","text":"","code":""},{"path":"discussion-6.-course-project--part-1.html","id":"announcements-2","chapter":"Discussion 6. Course Project- Part 1","heading":"Announcements","text":"HW 3 due Wednesday (October 8)\nSubmit PDF RMarkdown via Canvas\nSubmit PDF RMarkdown via CanvasOffice Hours throughout week (see Syllabus website)\nFilippo: Thursday 4-5pm 321A CIS Building\nShira: Monday 5-6 pm 329A CIS Building\nSam: Tuesday 4-5pm, 350 CIS Building\nFilippo: Thursday 4-5pm 321A CIS BuildingShira: Monday 5-6 pm 329A CIS BuildingSam: Tuesday 4-5pm, 350 CIS Building","code":""},{"path":"discussion-6.-course-project--part-1.html","id":"course-project","chapter":"Discussion 6. Course Project- Part 1","heading":"Course Project","text":"can download slides. week’s discussion.Take look Part 1 details.\ncomplete part 1, download .Rmd. Compile PDF submit PDF Canvas.","code":""},{"path":"due-dates.html","id":"due-dates","chapter":"Due dates","heading":"Due dates","text":"’ll post due dates throughout semester.","code":""},{"path":"due-dates.html","id":"homeworks-and-quizzes","chapter":"Due dates","heading":"Homeworks and Quizzes","text":"Problem Set 1: Due Sep 9, Peer review due Sep 16Problem Set 2: Due Sep 19, Peer review due Sep 26, Quiz Sep 30Problem Set 3: Due Oct 8, Peer review due Oct 16Problem Set 4 (subject change): Due Oct 23, Peer review due Oct 30Problem Set 5 (subject change): Due Nov 6, Peer review due Nov 13Problem Set 6 (subject change): Due Nov 20, Peer review due Dec 2","code":""},{"path":"problem-set-1.-definitions.html","id":"problem-set-1.-definitions","chapter":"Problem Set 1. Definitions","heading":"Problem Set 1. Definitions","text":"Relevant material covered Aug 28. Problem set due Sept 9 5pm.Welcome first problem set! homework practice conceptual notation ideas descriptive causal inference.complete problem set, Download .Rmd complete homework. Omit name can anonymous peer feedback. Compile PDF submit PDF Canvas.","code":""},{"path":"problem-set-1.-definitions.html","id":"practice-with-potential-outcomes","chapter":"Problem Set 1. Definitions","heading":"1. Practice with potential outcomes","text":"Popeye says eating spinach caused grow strong muscles. claims eaten spinach, grown strong muscles.","code":""},{"path":"problem-set-1.-definitions.html","id":"points","chapter":"Problem Set 1. Definitions","heading":"1.1 (7 points)","text":"Popeye’s claim, treatment? can either describe treatment variable explicitly write two options treatment variable can take.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-1","chapter":"Problem Set 1. Definitions","heading":"1.2 (7 points)","text":"Using mathematical notation discussed class, define two potential outcomes Popeye referringAnswer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-2","chapter":"Problem Set 1. Definitions","heading":"1.3 (7 points)","text":"sentence two, say Fundamental Problem Causal Inference applies Popeye’s claim.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-3","chapter":"Problem Set 1. Definitions","heading":"1.4 (7 points)","text":"Using conditional expectations probabilities, write following math: probability strong muscles higher among individuals eat spinach among individuals eat spinach.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-4","chapter":"Problem Set 1. Definitions","heading":"1.5 (7 points)","text":"Give one reason average causal effect eating spinach (quantity 1.2, averaged individuals) might different average descriptive difference (1.4) strong muscles.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"taking-3900","chapter":"Problem Set 1. Definitions","heading":"2. Taking 3900","text":"wondering taking STSCI 3900 help get higher paying job. claim , tell us whether claim causal descriptive.","code":""},{"path":"problem-set-1.-definitions.html","id":"points-5","chapter":"Problem Set 1. Definitions","heading":"2.1 (5 points)","text":"Last year, survey Cornell students graduated past two years. students take 3900 reported average salary 85,000 dollars students take 3900 reported average salary 78,000 dollars.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-6","chapter":"Problem Set 1. Definitions","heading":"2.2 (5 points)","text":"student take course took job graduation paid 80,000. student taken course, taken job paid 85,000.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-7","chapter":"Problem Set 1. Definitions","heading":"2.3 (5 points)","text":"Among students -campus job took course, average wage 18.50/hr semester took course average wage semester taking course 21.25.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-8","chapter":"Problem Set 1. Definitions","heading":"2.4 (5 points)","text":"Students took course applied twice many jobs graduation compared students take course.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"session-info","chapter":"Problem Set 1. Definitions","heading":"Session info","text":"chunk record information R session, useful debugging issues homework assignments contain code.","code":"\nsessionInfo()## R version 4.5.1 (2025-06-13)\n## Platform: aarch64-apple-darwin20\n## Running under: macOS Tahoe 26.0.1\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n## LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n## \n## locale:\n## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n## \n## time zone: America/New_York\n## tzcode source: internal\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets \n## [6] methods   base     \n## \n## loaded via a namespace (and not attached):\n##  [1] digest_0.6.37     R6_2.6.1          bookdown_0.43    \n##  [4] fastmap_1.2.0     xfun_0.52         glue_1.8.0       \n##  [7] cachem_1.1.0      knitr_1.50        memoise_2.0.1    \n## [10] htmltools_0.5.8.1 rmarkdown_2.29    lifecycle_1.0.4  \n## [13] xml2_1.3.8        cli_3.6.5         downlit_0.4.4    \n## [16] vctrs_0.6.5       sass_0.4.10       withr_3.0.2      \n## [19] jquerylib_0.1.4   compiler_4.5.1    rstudioapi_0.17.1\n## [22] tools_4.5.1       pillar_1.11.0     evaluate_1.0.4   \n## [25] bslib_0.9.0       yaml_2.3.10       fs_1.6.6         \n## [28] jsonlite_2.0.0    rlang_1.1.6"},{"path":"problem-set-2.-experiments.html","id":"problem-set-2.-experiments","chapter":"Problem Set 2. Experiments","heading":"Problem Set 2. Experiments","text":"Relevant material covered Sep 11. Problem set due Sep 19.complete problem set, Download .Rmd complete homework. Omit name can anonymous peer feedback. Compile PDF submit PDF Canvas.problem set based :Bertrand, M & Mullainathan, S. 2004. “Emily Greg Employable Lakisha Jamal? Field Experiment Labor Market Discrimination.” American Economic Review 94(4):991–1013.’s heads-hard problem setfor , reading social science paper hardfor , mathematical statistics hardfor , R coding hardFor almost one three easy.want support succeed! Text format help .","code":""},{"path":"problem-set-2.-experiments.html","id":"conceptual-questions-about-the-study-design","chapter":"Problem Set 2. Experiments","heading":"1. Conceptual questions about the study design","text":"Read first 10 pages paper (end section 2).","code":""},{"path":"problem-set-2.-experiments.html","id":"points-fundamental-problem","chapter":"Problem Set 2. Experiments","heading":"1.1. (5 points) Fundamental Problem","text":"One submitted resume name “Emily Baker.” yielded callback. resume name “Lakisha Washington,” hypothetical world yielded callback. Explain Fundamental Problem Causal Inference applies case (1–2 sentences). Write potential outcomes using notation described class.Answer. answer ","code":""},{"path":"problem-set-2.-experiments.html","id":"points-exchangeability","chapter":"Problem Set 2. Experiments","heading":"1.2. (5 points) Exchangeability","text":"sentence, state exchangeability means study. concreteness, question may suppose names study “Emily Baker” “Lakisha Washington.”Answer. answer ","code":""},{"path":"problem-set-2.-experiments.html","id":"points-something-you-liked","chapter":"Problem Set 2. Experiments","heading":"1.3. (5 points) Something you liked","text":"State something concrete appreciate study design, randomization.Answer. answer ","code":""},{"path":"problem-set-2.-experiments.html","id":"analyzing-the-experimental-data","chapter":"Problem Set 2. Experiments","heading":"2. Analyzing the experimental data","text":"Load packages code use.complete rest assignment, need download study’s data OpenICPSR: https://www.openicpsr.org/openicpsr/project/116023/version/V1/view. require creating account agreeing terms using data ethically. Put data folder computer .Rmd located. Read data R using read_dta.error, might need set working directory first. tells R look data files. words, data file needs folder homework file, RStudio needs told folder look . top RStudio, click Session -> Set Working Directory -> Source File Location.now see d Global Environment top right RStudio.use four variables:2.1–2.4, think race treatment. 2.5–2.6, think firstname treatment.can print relevant variables tableIf new R, just happened:created new object d_selectedused assignment operator <- put something objectwe started data object dwe used pipe operator %>% hand d new actionthe action select() selected variables interestWe often analyze data starting data object handing series actions connected pipe %>%","code":"\nlibrary(tidyverse)\nlibrary(haven)\nd <- read_dta(\"assets/data/lakisha_aer.dta\")\nd_selected <- d %>%\n  select(call, firstname, race, sex) %>% print()## # A tibble: 4,870 × 4\n##     call firstname race  sex  \n##    <dbl> <chr>     <chr> <chr>\n##  1     0 Allison   w     f    \n##  2     0 Kristen   w     f    \n##  3     0 Lakisha   b     f    \n##  4     0 Latonya   b     f    \n##  5     0 Carrie    w     f    \n##  6     0 Jay       w     m    \n##  7     0 Jill      w     f    \n##  8     0 Kenya     b     f    \n##  9     0 Latonya   b     f    \n## 10     0 Tyrone    b     m    \n## # ℹ 4,860 more rows"},{"path":"problem-set-2.-experiments.html","id":"points-point-estimates-of-expected-potential-outcomes","chapter":"Problem Set 2. Experiments","heading":"2.1. (5 points) Point estimates of expected potential outcomes","text":"top Table 1 reports callback rates: 9.65% white names 6.45% Black names. Reproduce numbers. , take code add group_by() action d_selected summarize.’s reference introduces group_by summarize.Answer. Modify code ","code":"\nd_summarized <- d_selected %>%\n  summarize(callback_rate = mean(call),\n            number_cases = n()) %>%\n  print()## # A tibble: 1 × 2\n##   callback_rate number_cases\n##           <dbl>        <int>\n## 1        0.0805         4870"},{"path":"problem-set-2.-experiments.html","id":"points-inference-for-expected-potential-outcomes","chapter":"Problem Set 2. Experiments","heading":"2.2. (5 points) Inference for expected potential outcomes","text":"Use mutate() (see reference page) create new columns containing standard error estimate well lower upper limits 95% confidence intervals.Suppose \\(\\pi^\\) probability callback treatment condition \\(\\). Let \\(\\hat\\pi^\\) estimate unknown probability, meaning \\(\\hat\\pi^\\) proportion people treatment condition receive callback experiment (computed 2.1). making confidence interval \\(\\hat\\pi^\\), estimate \\(\\hat\\pi^\\) standard error standard error \\(\\hat\\pi^\\)make easier, quick math review R functions can use.Standard error math. Let \\(Y^\\) Bernoulli random variable, taking value 1 random resume name \\(\\) yields callback 0 otherwise. Let \\(\\pi^= P(Y^=1)\\) probability callback. statistics, know variance \\(V(Y^) = \\pi^(1-\\pi^)\\). estimated average: \\(\\hat\\pi^= \\frac{1}{n_a}\\sum_{:A_i=} Y_i^\\). many times many hypothetical samples, always get estimate. fact, estimate sampling variance \\(V(\\hat\\pi^) = \\frac{\\pi^(1-\\pi^)}{n_a}\\). know \\(\\hat\\pi^\\) mean \\(n_a\\) independent identically distributed random variables \\(Y^\\). standard error square root sampling variance: \\(SE(\\hat\\pi^) = \\sqrt\\frac{\\pi^(1-\\pi^)}{n_a}\\). can estimate standard error plugging estimate \\(\\hat\\pi^\\) true unknown \\(\\pi^\\) wherever appears.Standard error code. translated standard error formula code . function accepts estimated probability p sample size n returns estimated standard error. can use se_binary() function code within mutate() just like mean() used within summarize() start problem set.Sampling distribution math. \\(\\hat\\pi^\\) sample mean, know something sampling distribution: limit sample size grows infinity, across hypothetical repeated samples distribution \\(\\hat\\pi^\\) estimates becomes Normal. Central Limit Theorem! Across repeated samples, middle 95% estimates fall within known range: \\(\\pi^\\pm \\Phi^{-1}(.975) \\times SE(\\hat\\pi^)\\), \\(\\Phi^{-1}()\\) inverse cumulative distribution function standard Normal distribution. might previously learned \\(\\Phi^{-1}(.975) \\approx 1.96\\), might familiar number 1.96.Sampling distribution graph.Confidence interval math. get 95% confidence interval plugging estimates \\(\\hat\\pi^\\) \\(\\widehat{SE}(\\hat\\pi^)\\) limits . interval centered estimate \\(\\hat\\pi^\\) nice property: repeatedly made confidence interval procedure using hypothetical samples population, interval contain unknown true parameter \\(\\pi^\\) 95% time.Confidence interval code. translated confidence interval formula code . functions accept estimate standard error return lower upper bounds (respectively) 95% confidence interval assumes Normal sampling distribution. can use functions code within mutate() just like mean() used within summarize() start problem set.","code":"\nse_binary <- function(p, n) {\n  se <- sqrt( p * (1 - p) / n )\n  return(se)\n}\nci_lower <- function(estimate, standard_error) {\n  estimate - qnorm(.975) * standard_error\n}\nci_upper <- function(estimate, standard_error) {\n  estimate + qnorm(.975) * standard_error\n}"},{"path":"problem-set-2.-experiments.html","id":"points-interpret-your-confidence-interval","chapter":"Problem Set 2. Experiments","heading":"2.3. (5 points) Interpret your confidence interval","text":"words, interpret confidence intervals. sure discuss property hypothetical repeated samples, sure frame answer using numbers variables actual experiment analyzing.Answer. answer ","code":""},{"path":"problem-set-2.-experiments.html","id":"points-visualize-expected-potential-outcomes","chapter":"Problem Set 2. Experiments","heading":"2.4. (5 points) Visualize expected potential outcomes","text":"Using ggplot(), visualize estimated callback rate race. Use geom_point() point estimates geom_errorbar() confidence intervals, race x axis estimates y axis. Label axes using full words.never used ggplot, see Ch 3 R Data Science Hadley Wickham.Answer. answer ","code":""},{"path":"problem-set-2.-experiments.html","id":"points-estimate-and-visualize-by-firstname","chapter":"Problem Set 2. Experiments","heading":"2.5. (5 points) Estimate and visualize by firstname","text":"distinct first names yield distinct effects? Repeat coding steps 2.2–2.4, now create estimates grouped race, sex, firstname. Visualize point estimates confidence intervals.One way visualize placing first names \\(x\\)-axis using facet_wrap() layer facet race sex.strategy visualize fine, long shows estimates firstname indicates race sex signaled firstname\n2.4, making two estimates: one white one Black. 2.5, aggregating within fine-grained set groups defined race, sex, firstname. need start fresh raw person-level data order answer 2.5Answer. answer ","code":"\nyour_ggplot +\n  facet_wrap(~ race + sex,\n             scales = \"free_x\", \n             nrow = 1)"},{"path":"problem-set-2.-experiments.html","id":"points-interpret","chapter":"Problem Set 2. Experiments","heading":"2.6. (5 points) Interpret","text":"Within race sex, first names effect. Suppose true differences (due sampling variability). tell importance researcher decisions names use treatments? many possible right answers, asking think might mean research design names different effects.Answer. answer ","code":""},{"path":"problem-set-2.-experiments.html","id":"points-treatment-effect-heterogeneity","chapter":"Problem Set 2. Experiments","heading":"2.7. (5 points) Treatment effect heterogeneity","text":"simplify next questions, define treatment race coded first name instead first name . researchers conducted study Boston Chicago. many ways two cities differ might expect causal effect race might heterogeneous across cities. Similar , calculate callback rate race city placing group_by() action d summarize.Answer. Modify code ","code":"\ncallback_city <- d %>%\n  summarize(callback_rate = mean(call),\n            number_cases = n()) %>%\n  print(n = Inf)## # A tibble: 1 × 2\n##   callback_rate number_cases\n##           <dbl>        <int>\n## 1        0.0805         4870"},{"path":"problem-set-2.-experiments.html","id":"points-transportability","chapter":"Problem Set 2. Experiments","heading":"2.8. (5 points) Transportability","text":"One reason causal effect may differ across cities proportion industries varies across cities causal effect may differ across industries. code first creates industry variable looks proportion industries city. Specifically, using filter(city == \"b\") filter(city == \"c\") can use rows correspond Boston Chicago respectively.Suppose researchers first conducted pilot study Boston . code calculates estimated conditional ACE industry using data Boston. Assuming conditional causal effect industry Boston Chicago, estimate average causal effect race callback rates Chicago ? receive full points, include code calculated answer.}Answer. answer ","code":"\n# Currently the industries for each firm is encoded across 7 different columns;\n# i.e., a 1 in manuf and 0 in the other columns indicates a firm is in manufacturing.\n# So, we first create an additional variable in d which explicitly states the industry \nindustry_list <- c(\"manuf\", \"transcom\", \"bankreal\", \"trade\", \"busservice\", \"otherservice\", \"other\")\nd <- d %>% mutate(industry = industry_list[max.col(cbind(manuf, transcom, bankreal, trade, busservice, othservice, missind))])\n\n## Proportions for Boston\n# By using filter(city == \"b) we are only including rows\n# for which city == b; i.e. when the city is boston\nd %>% filter(city == \"b\") %>% \n  group_by(industry) %>% summarise(freq = n()) %>% # first get the number of times each industry appears and put that in the freq variable\n  mutate(percent = freq / sum(freq)) %>% # using the frequency calculate the proportions \n  select(-freq) %>% # don't print the freq column\n  print(n = Inf)## # A tibble: 7 × 2\n##   industry     percent\n##   <chr>          <dbl>\n## 1 bankreal      0.0480\n## 2 busservice    0.259 \n## 3 manuf         0.0896\n## 4 other         0.193 \n## 5 otherservice  0.117 \n## 6 trade         0.249 \n## 7 transcom      0.0443\n## Proportions for Chicago\n# By using filter(city == \"b) we are only including rows\n# for which city == b; i.e. when the city is boston\nd %>% filter(city == \"c\") %>% group_by(industry) %>%\n  summarise(freq = n()) %>%\n  mutate(percent = freq / sum(freq)) %>% select(-freq) %>%\n  print(n = Inf)## # A tibble: 7 × 2\n##   industry     percent\n##   <chr>          <dbl>\n## 1 bankreal      0.115 \n## 2 busservice    0.275 \n## 3 manuf         0.0777\n## 4 other         0.143 \n## 5 otherservice  0.185 \n## 6 trade         0.186 \n## 7 transcom      0.0192\n# Table with callback rates by industry and race for Boston\nd %>% filter(city == \"b\") %>%\n  group_by(industry, race) %>%\n  summarize(callback_rate = mean(call))  %>%\n  print()## `summarise()` has grouped output by 'industry'. You can\n## override using the `.groups` argument.## # A tibble: 14 × 3\n## # Groups:   industry [7]\n##    industry     race  callback_rate\n##    <chr>        <chr>         <dbl>\n##  1 bankreal     b            0.0385\n##  2 bankreal     w            0.154 \n##  3 busservice   b            0.0821\n##  4 busservice   w            0.136 \n##  5 manuf        b            0.0103\n##  6 manuf        w            0.0928\n##  7 other        b            0.0861\n##  8 other        w            0.105 \n##  9 otherservice b            0.165 \n## 10 otherservice w            0.173 \n## 11 trade        b            0.0519\n## 12 trade        w            0.0778\n## 13 transcom     b            0.104 \n## 14 transcom     w            0.125\n# Table with same information as above, but rearranged so that callback rates by each race is it's own column\nd %>% filter(city == \"b\") %>%\n  group_by(industry, race) %>%\n  summarize(callback_rate = mean(call)) %>% \n  pivot_wider(names_from = race, values_from = callback_rate) %>% # puts the each race into its own column\n  mutate(diff = w - b) %>% # calculates the difference\n  print()## `summarise()` has grouped output by 'industry'. You can\n## override using the `.groups` argument.## # A tibble: 7 × 4\n## # Groups:   industry [7]\n##   industry          b      w    diff\n##   <chr>         <dbl>  <dbl>   <dbl>\n## 1 bankreal     0.0385 0.154  0.115  \n## 2 busservice   0.0821 0.136  0.0536 \n## 3 manuf        0.0103 0.0928 0.0825 \n## 4 other        0.0861 0.105  0.0191 \n## 5 otherservice 0.165  0.173  0.00787\n## 6 trade        0.0519 0.0778 0.0259 \n## 7 transcom     0.104  0.125  0.0208"},{"path":"problem-set-3.-dags..html","id":"problem-set-3.-dags.","chapter":"Problem Set 3. DAGs.","heading":"Problem Set 3. DAGs.","text":"Relevant material covered Sep 25. Problem set due Oct 8.complete problem set, Download .Rmd complete homework. Omit name can anonymous peer feedback. Compile PDF submit PDF Canvas.","code":""},{"path":"problem-set-3.-dags..html","id":"true-or-false","chapter":"Problem Set 3. DAGs.","heading":"1. True or False","text":"1.1–1.5, answer True False: \\(Z\\) nodes (.e., either \\(Z\\) set \\(\\{Z_1, Z_2, \\ldots \\}\\)) form sufficient adjustment set identify causal effect \\(\\) \\(Y\\). Explain answer one sentence. False, state backdoor path unblocked conditional \\(Z\\).","code":""},{"path":"problem-set-3.-dags..html","id":"points-9","chapter":"Problem Set 3. DAGs.","heading":"1.1 (2 points)","text":"Answer. answer ","code":""},{"path":"problem-set-3.-dags..html","id":"points-10","chapter":"Problem Set 3. DAGs.","heading":"1.2 (2 points)","text":"Answer. answer ","code":""},{"path":"problem-set-3.-dags..html","id":"points-11","chapter":"Problem Set 3. DAGs.","heading":"1.3 (2 points)","text":"Answer. answer ","code":""},{"path":"problem-set-3.-dags..html","id":"points-12","chapter":"Problem Set 3. DAGs.","heading":"1.4 (2 points)","text":"Answer. answer ","code":""},{"path":"problem-set-3.-dags..html","id":"points-13","chapter":"Problem Set 3. DAGs.","heading":"1.5 (2 points)","text":"Answer. answer ","code":""},{"path":"problem-set-3.-dags..html","id":"which-dag-is-correct","chapter":"Problem Set 3. DAGs.","heading":"2. Which DAG is ‘’Correct’’?","text":"Suppose classmate interested estimating causal effect New York resident admission Cornell undergraduate. classmate states causal DAG believe describes causal system. included variables :Residency: treatment either NY non-NYAdmission: outcome either Yes NoFamily SES: socioeconomic level applicant’s familySAT scores: applicant’s SAT scoreLegacy: Whether applicant family members Cornell alumni","code":""},{"path":"problem-set-3.-dags..html","id":"pts","chapter":"Problem Set 3. DAGs.","heading":"2.1 (5 pts)","text":"Write notation potential outcomes (generic individual \\(\\)) using notation class.Answer.","code":""},{"path":"problem-set-3.-dags..html","id":"pts-1","chapter":"Problem Set 3. DAGs.","heading":"2.1 (8 pts)","text":"causal diagram list paths Residency Admission state whether causal paths . paths contains just treatment outcome, indicate whether node path collider non-collider. 4 paths total.Answer.Path 1:Path 2:Path 3:Path 4:","code":""},{"path":"problem-set-3.-dags..html","id":"pts-2","chapter":"Problem Set 3. DAGs.","heading":"2.2 (8 pts)","text":"path , determine path open blocked conditioning ‘’Family SES.’’ Explain path.Answer.Path 1:Path 2:Path 3:Path 4:","code":""},{"path":"problem-set-3.-dags..html","id":"pts-3","chapter":"Problem Set 3. DAGs.","heading":"2.3 (5 pts)","text":"Suppose classmate uses standardization estimate causal effect conditioning ‘’Family SES.’’ Assuming graph correct, conditional exchangeability hold? ?Answer","code":""},{"path":"problem-set-3.-dags..html","id":"pts-4","chapter":"Problem Set 3. DAGs.","heading":"2.4 (8 pts)","text":"Since individual admissions data publicly available, recall classmate gathered data first collecting contact information high school students visited Cornell. admissions decisions released, classmate sent individuals email link survey. Thus, add following variables graphVisit: Whether student visits CornellRespond: Whether student responded surveyStudents live NY likely visit since closer. Students visit likely get link survey can respond. Also, students get admitted likely respond (said another way, students get admitted less likely respond). Thus, think accurate causal graph.Since data students responded, analysis classmate conducted implicitly conditions ‘’Respond.’’ case, causal estimate classmate estimated still reliable? ?Answer. answer ","code":""},{"path":"who-we-are.html","id":"who-we-are","chapter":"Who we are","heading":"Who we are","text":"","code":""},{"path":"who-we-are.html","id":"faculty","chapter":"Who we are","heading":"Faculty","text":"research interests include algorithm design analysis, high dimensional statistics, inference networks, sequential decision making uncertainty, online learning. interests causal inference primarily revolve around settings involve complex dependencies networks dynamics time.enjoy thinking problems goal discover interpretable structure underlies data generating process. includes problems areas causal discovery, graphical models, mixed membership models.","code":""},{"path":"who-we-are.html","id":"teaching-assistants","chapter":"Who we are","heading":"Teaching assistants","text":"currently working problem related Causality, particularly Causal Graph discovery Functional data. interested learn interpretable structures data hidden confounders. free time enjoy playing basketball, running hanging friends.currently working graphical models noisy measurements. interested causal discovery applications social science biology. free time like puzzles, playing pool, baking.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
