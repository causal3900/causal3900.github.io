[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"Cornell STSCI / INFO / ILRST 3900. Causal Inference. Fall 2023.Welcome! Together, learn make causal claims combining data arguments.Taught Ian Lundberg, Y. Samuel Wang, Mayleen Cortez-Rodriguez, Daniel Molitor. Read us !","code":""},{"path":"index.html","id":"learning-objectives","chapter":"Welcome","heading":"Learning objectives","text":"result participating course, students able todefine counterfactuals outcomes hypothetical interventionsidentify counterfactuals causal assumptions presented graphsestimate counterfactual outcomes pairing assumptions statistical evidence","code":""},{"path":"index.html","id":"is-this-course-for-me","chapter":"Welcome","heading":"Is this course for me?","text":"course designed upper-division undergraduate students. assume familiarity introductory statistics course level STSCI 2110, PAM 2100, PSYCH 2500, SOC 3010, ECON 3110, similar courses.Cornell student? welcome follow along site.","code":""},{"path":"index.html","id":"readings","chapter":"Welcome","heading":"Readings","text":"Especially beginning, course draws heavily onHernán, M.., J.M. Robins. 2020. Causal Inference: ? Boca Raton: Chapman & Hall / CRC.grateful authors excellent text.","code":""},{"path":"index.html","id":"organization-of-the-site","chapter":"Welcome","heading":"Organization of the site","text":"course module left panel span several lectures. Within module, right panel help navigate. build site course semester, uploading lecture slides go. tells bit teaching team.","code":""},{"path":"index.html","id":"land-acknowledgment","chapter":"Welcome","heading":"Land acknowledgment","text":"recognize university land acknowledgment, well additional emphasis Cornell American Indian Indigenous Studies Program.Cornell University located traditional homelands Gayogo̱hó:nǫɁ (Cayuga Nation). Gayogo̱hó:nǫɁ members Haudenosaunee Confederacy, alliance six sovereign Nations historic contemporary presence land. Confederacy precedes establishment Cornell University, New York state, United States America. acknowledge painful history Gayogo̱hó:nǫɁ dispossession, honor ongoing connection Gayogo̱hó:nǫɁ people, past present, lands waters.land acknowledgment reviewed approved traditional Gayogo̱hó:nǫɁ leadership.addition Gayogo̱hó:nǫɁ land acknowledgment separate , AIISP faculty like emphasize: Cornell’s founding enabled course national genocide sale almost one million acres stolen Indian land Morrill Act 1862. date university neither officially acknowledged complicity theft offered form restitution hundreds Native communities impacted. additional information, see Cornell University Indigenous Dispossession website.","code":""},{"path":"defining-counterfactuals.html","id":"defining-counterfactuals","chapter":"1 Defining counterfactuals","heading":"1 Defining counterfactuals","text":"","code":""},{"path":"defining-counterfactuals.html","id":"observing-versus-intervening","chapter":"1 Defining counterfactuals","heading":"1.1 Observing versus intervening","text":"Aug 22. SlidesStatistical inference observing: observe sample population, can infer population? Causal inference intervening: take sample population intervene change exposure, average outcome result?Today discuss observing, intervening, difference important.","code":""},{"path":"defining-counterfactuals.html","id":"lab-designing-a-study","chapter":"1 Defining counterfactuals","heading":"1.2 Lab: Designing a study","text":"Aug 23In lab, start getting know one another. , discuss hypothetical scenario.researcher (may disagree) says :\n> Coming office hours frequently causes student success classroom.groups 3 students, discuss following.Imagine start semester. design randomized experiment assess claim?\nImagine can assign students treatment condition comply\nConsider details: enroll, define frequently, assess success, etc.\nImagine can assign students treatment condition complyConsider details: enroll, define frequently, assess success, etc.Imagine end semester. randomized study run. want conduct observational study using administrative records (get IRB approval ). design observational study assess claim?expectation clear answers now. course semester learn formalize types questions solutions.","code":""},{"path":"defining-counterfactuals.html","id":"defining-causal-effects","chapter":"1 Defining counterfactuals","heading":"1.3 Defining causal effects","text":"Aug 24. Slides. class, read Chapter 1 Hernán Robins 2020 begin Homework 1.Today define average causal effects potential outcomes framework.end class, able todefine potential outcomesexplain Fundamental Problem Causal Inference1recall statistical concepts: random variables, expectation, conditional expectation","code":""},{"path":"exchangeability-and-experiments.html","id":"exchangeability-and-experiments","chapter":"2 Exchangeability and experiments","heading":"2 Exchangeability and experiments","text":"","code":""},{"path":"exchangeability-and-experiments.html","id":"randomized-experiments","chapter":"2 Exchangeability and experiments","heading":"2.1 Randomized experiments","text":"Aug 29. Slides. class, read Hernán Robins 2020 Chapter 2 end 2.1.Much course address observational studies non-randomized treatments. set stage, today first discuss randomized experiments powerful possible.","code":""},{"path":"exchangeability-and-experiments.html","id":"lab-statistics-review-with-math-and-simulations","chapter":"2 Exchangeability and experiments","heading":"2.2 Lab: Statistics review with math and simulations","text":"Aug 30. Slides.course use several ideas previous coursework statistics, including random variables, expected values, independence. lab review concepts math using simulations R.","code":""},{"path":"exchangeability-and-experiments.html","id":"exchangeability-and-conditional-randomization","chapter":"2 Exchangeability and experiments","heading":"2.3 Exchangeability and conditional randomization","text":"Aug 31. Slides. class, read Hernán Robins 2020 Chapter 2.2.talk experiments good: help us ask precise causal questions, setting key assumption (exchangeability) holds design. discuss exchangeability simple randomized experiments experiments conditionally randomized treatment assignment probabilities functions pre-existing characteristics.","code":""},{"path":"exchangeability-and-experiments.html","id":"standardization-and-effect-measures","chapter":"2 Exchangeability and experiments","heading":"2.4 Standardization and effect measures","text":"Sep 5. Slides. class, read Hernán Robins 2020 Chapter 1.3 2.3.Standardization important statistical procedure two steps:estimate causal effect population subgroupaverage population distribution subgroupsIn conditionally randomized experiments, standardization essential yield unbiased estimates population average causal effect. strategy also essential observational studies discuss soon.end class, able todescribe different ways quantitatively measure causal effectestimate average causal effect using data conditionally randomized experiment","code":""},{"path":"exchangeability-and-experiments.html","id":"lab-analyze-a-randomized-experiment","chapter":"2 Exchangeability and experiments","heading":"2.5 Lab: Analyze a randomized experiment","text":"Sep 6. Link Materials.Link SolutionsThis lab use R analyze data randomized experiment households randomized receive mailers encouraging vote, researchers examined effects voter turnout (Gerber, Green, & Larimer 2008).","code":""},{"path":"exchangeability-and-experiments.html","id":"inverse-probability-weighting","chapter":"2 Exchangeability and experiments","heading":"2.6 Inverse probability weighting","text":"Sep 7. Slides. class, read Hernán Robins 2020 Chapter 2.4, 3.1, 3.2.class introduce inverse probability weighting approach estimate average causal effects conditional exchangeability holds.","code":""},{"path":"consistency-and-positivity.html","id":"consistency-and-positivity","chapter":"3 Consistency and positivity","heading":"3 Consistency and positivity","text":"","code":""},{"path":"consistency-and-positivity.html","id":"asking-good-causal-questions","chapter":"3 Consistency and positivity","heading":"3.1 Asking good causal questions","text":"Sep 12. Slides. class, read Hernán Robins 2020 Chapter 3. Optionally, read Hernán 2016.Good causal questions structured credibility strong two key assumptions: positivity consistency.Positivity. Every population subgroup receives every treatment value non-zero probabilityConsistency. Potential outcomes \\(Y^\\) well-defined linked observable dataAfter class, ready discussion lab related common violation consistency assumption one unit’s treatment affects another unit’s outcome.","code":""},{"path":"consistency-and-positivity.html","id":"lab-interference","chapter":"3 Consistency and positivity","heading":"3.2 Lab: Interference","text":"Sep 13 Slides.defining causal effects, often discuss outcome \\(Y^\\) person realize exposed treatment value \\(\\). definitions become harder exists interference: outcome unit \\(\\) depends treatment assigned unit \\(j\\). discussion focus understanding interference need update potential outcomes notation interference present.","code":""},{"path":"directed-acyclic-graphs.html","id":"directed-acyclic-graphs","chapter":"4 Directed Acyclic Graphs","heading":"4 Directed Acyclic Graphs","text":"","code":""},{"path":"directed-acyclic-graphs.html","id":"marginal-independence","chapter":"4 Directed Acyclic Graphs","heading":"4.1 Marginal independence","text":"Sep 14. Slides. class, read Hernán Robins 2020 Chapter 6.1 6.2. historical reference, optionally see Greenland, Pearl, Robins 1999.class introduce key ideas DAGs.Directed Acyclic Graph. series nodes representing variables, connected directed edges representing direct causal effects. node edge least two nodes must drawn graph.Path. path sequence edges connecting two nodesCollider along path. node \\(B\\) directed edges collide: \\(\\rightarrow B \\leftarrow C\\). collider blocks path.DAGs help us know variables \\(\\) \\(B\\) statistically related\\(\\) \\(B\\) marginally dependent exists unblocked path connecting \\(\\) \\(B\\) marginally independent paths connecting blocked","code":""},{"path":"directed-acyclic-graphs.html","id":"conditional-independence","chapter":"4 Directed Acyclic Graphs","heading":"4.2 Conditional independence","text":"Sep 19. Slides. class, read Hernán Robins 2020 Chapter 6.3 6.4, especially Fine Point 6.1 page abbreviation.Often, want condition set variables \\(\\vec{L}\\) conditional exchangeability holds.path blocked node path blocked. every node path open, entire path openA non-collider blocked conditioned , otherwise openA collider open descendants conditioned . Otherwise blocked","code":""},{"path":"directed-acyclic-graphs.html","id":"lab-dags-review-and-causal-discovery","chapter":"4 Directed Acyclic Graphs","heading":"4.3 Lab: DAGs Review and Causal discovery","text":"Sep 20. Slides. class, read Hernán Robins 2020 Fine Point 6.3.lab, ’re reviewing DAG basics identifying paths determining whether path open closed. ’ll also talk bit causal discovery practice creating DAGs data.","code":""},{"path":"directed-acyclic-graphs.html","id":"sufficient-adjustment-sets","chapter":"4 Directed Acyclic Graphs","heading":"4.4 Sufficient adjustment sets","text":"Sep 21. Slides. class, read Hernán Robins 2020 7.1–7.4.marginal exchangeability hold, may able condition set variables \\(\\vec{L}\\) conditional exchangeability holds. can accomplish blocking non-causal paths \\(\\) \\(Y\\). set called sufficient adjustment set. find sufficient adjustment set, use backdoor criterion:set \\(L\\) blocks backdoor pathsThe set \\(L\\) contain descendants \\(\\)","code":""},{"path":"statistical-modeling.html","id":"statistical-modeling","chapter":"5 Statistical modeling","heading":"5 Statistical modeling","text":"","code":""},{"path":"statistical-modeling.html","id":"why-model","chapter":"5 Statistical modeling","heading":"5.1 Why model?","text":"Sep 26. Slides. class, read Hernán Robins 2020 Chapter 11.point, used statistical models. Instead, havetaken means within subgroupsthen aggregated subgroupsToday discuss strategy breaks many confounding variables, thus many subgroups.","code":""},{"path":"statistical-modeling.html","id":"lab-parametric-g-formula","chapter":"5 Statistical modeling","heading":"5.2 Lab: Parametric g-formula","text":"Sep 27. Download corresponding R Markdown file .discussion, make sure download data ’ll using. See Ed Discussion post detail.class, read Hernán Robins 2020 Chapter 13 15.1.Solutions lab exercise slides","code":""},{"path":"statistical-modeling.html","id":"inverse-probability-of-treatment-weighting","chapter":"5 Statistical modeling","heading":"5.3 Inverse probability of treatment weighting","text":"Sep 28. Slides. Reading: class, read Hernán Robins 2020 Chapter 12.1–12.5.Today introduce estimate causal effects modeling probability treatment, also known propensity score.","code":""},{"path":"statistical-modeling.html","id":"matching","chapter":"5 Statistical modeling","heading":"5.4 Matching","text":"Oct 3. Slides. class, read Hernán Robins 2020 Chapter 15.2.Today introduce idea matching allows us estimate average treatment treated.","code":""},{"path":"statistical-modeling.html","id":"lab-matching-in-r","chapter":"5 Statistical modeling","heading":"5.5 Lab: Matching in R","text":"Oct 4. Slides. R Markdown.lab, ’ll go distance metrics matching multiple covariates. ’ll also go examples using R matching estimate causal effects.","code":""},{"path":"statistical-modeling.html","id":"discussion-of-matching","chapter":"5 Statistical modeling","heading":"5.6 Discussion of matching","text":"Oct 5 Slides. R Markdown.’ll wrap discussion matching introducing propensity score matching coarsened exact matching. ’ll also discuss combining regression matching methods estimate causal effects.","code":""},{"path":"statistical-modeling.html","id":"lab-final-project-hw4-qa","chapter":"5 Statistical modeling","heading":"5.7 Lab: Final Project + HW4 Q&A","text":"Oct 10 SlidesWe’ll talk final project!","code":""},{"path":"statistical-modeling.html","id":"worked-example-of-statistical-modeling","chapter":"5 Statistical modeling","heading":"5.8 Worked example of statistical modeling","text":"section presents math code worked example statistical\nmodeling, includingoutcome modelinginverse probability treatment weightingmatchingWe use methods answer causal question:degree completing 4-year college degree age 25\nincrease probability college-educated spouse \nresidential partner age 35?theory motivates question follows. College causes\npeople personally higher earnings. also affects \nprobability someone lives high-earning partner. college\ndegree thus affects household incomes effect \nindividual earnings also effect individuals pool\nhouseholds.","code":""},{"path":"statistical-modeling.html","id":"data-access","chapter":"5 Statistical modeling","heading":"5.8.1 Data access","text":"prepared data study question. need \ndownload data directly data distributor National\nLongitudinal Survey Youth 1997\ncohort. .First, download two supporting files us:nlsy97.NLSY97\ntagset file containing variable namesprepare_nlsy97.R\nR script prepare dataput files directory workNext, download data NLSY97.register surveylog NLS Investigatorchoose NLSY97 studyupload tagset downloaded usdownload data. , change file name default\nnlsy97unzip file. Find nlsy97.dat unzipped folderdrag file folder workIn R console, run line code belowAfter following steps, data working directory! \ncan load data quickly future typingWhy can’t just send data? Two reasons!NLSY97 created procedure register users encourage\nethical use data researchBy registering, help Bureau Labor Statistics know \nmany people using data, helpful demonstrating\nwide use data useful securing funding \nfuture surveys!","code":"\ninstall.packages(\"tidyverse\") # if you do not have it yet\ninstall.packages(\"Amelia\")    # if you do not have it yet\nsource(\"prepare_nlsy97.R\")\nlibrary(tidyverse)\nd <- readRDS(\"d.RDS\")"},{"path":"statistical-modeling.html","id":"worked-example-outcome-modeling","chapter":"5 Statistical modeling","heading":"5.8.2 Worked example: Outcome modeling","text":"Outcome modeling based following identification result, \ntranslates causal quantity statistical estimand \ninvolve counterfactual outcomes.\\[\\begin{aligned}\n&E(Y^) \\\\\n&\\text{law iterated expectation,}\\\\\n&= E(E(Y^\\mid \\vec{L})) \\\\\n&\\text{exchangeability,}\\\\\n&= E(E(Y^\\mid \\vec{L}, = )) \\\\\n&\\text{consistency,}\\\\\n&= E(E(Y\\mid \\vec{L}, = ))\n\\end{aligned}\\]use sample mean estimator outer expectation, \ndiscuss several estimators inner conditional\nexpectation. \\[\\begin{aligned}\n\\hat{E}(Y^) &= \\frac{1}{n}\\sum_{=1}^n \\hat{E}(Y\\mid \\vec{L} = \\vec\\ell_i, = )\n\\end{aligned}\\]Now intuition: estimator tells tofor unit \\(\\) sample, estimate expected outcome among\npeople look like unit (\\(\\vec{L} = \\vec\\ell_i\\)) got\ntreatment value interest \\(= \\).take average estimate unitsA nonparametric strategy step (1) literally estimate \nexpected outcome taking sample average among units \nidentical unit \\(\\) along confounders \\(\\vec{L}\\). \nmany confounding variables units, might zero\ncases! parametric strategy assume model outcome,\n\\[E(Y\\mid \\vec{L} = \\vec\\ell, = ) = \\alpha + \\beta + \\vec\\ell'\\vec\\gamma\\]\nparameters \\(\\{\\alpha,\\beta,\\vec\\gamma\\}\\) estimated \nOrdinary Least Squares regression.Note: model like! example, add\ninteractions use logistic regression instead.model, want predict expected outcome \ntreatment value \\(\\) every unit unit’s observed confounder\nvalues.\\[\\hat{E}(Y\\mid \\vec{L} = \\vec\\ell_i, = ) = \\hat\\alpha + \\hat\\beta + \\vec\\ell'_i\\hat{\\vec\\gamma}\\]\ncode, wouldmodify every unit’s treatment value \\(\\)\nintuition: actually intervened treatment \nworld, value treatment change values \n\\(\\vec{L}\\) remain unchanged\nintuition: actually intervened treatment \nworld, value treatment change values \n\\(\\vec{L}\\) remain unchangedpredict outcome every unitaverage sampleIn code , estimated three causal quantities\\(E(Y^1)\\), probability respondent \ncollege-educated spouse partner intervened assign \ncollege degree \\(E(Y^0)\\), probability respondent \ncollege-educated spouse partner intervened assign \ncollege degree \\(E(Y^1-Y^0)\\), average causal effect college degree \nspouse partner college degree","code":"\noutcome_model <- lm(y ~ a + sex + race + \n                      mom_educ + dad_educ + \n                      log_parent_income +\n                      log_parent_wealth +\n                      test_percentile,\n                    data = d)\n# Make data where all are treated\nd_if_treated <- d %>%\n  mutate(a = \"college\")\n# Make data where all are untreated\nd_if_untreated <- d %>%\n  mutate(a = \"no_college\")\npredicted_outcome <- d %>%\n  mutate(yhat1 = predict(outcome_model,\n                         newdata = d_if_treated),\n         yhat0 = predict(outcome_model,\n                         newdata = d_if_untreated))\npredicted_outcome %>%\n  summarize(average_yhat1 = mean(yhat1),\n            average_yhat0 = mean(yhat0),\n            average_effect = mean(yhat1 - yhat0))## # A tibble: 1 × 3\n##   average_yhat1 average_yhat0 average_effect\n##           <dbl>         <dbl>          <dbl>\n## 1         0.427         0.164          0.263"},{"path":"statistical-modeling.html","id":"worked-example-treatment-modeling","chapter":"5 Statistical modeling","heading":"5.8.3 Worked example: Treatment modeling","text":"Using different identification result, can also proceed \nparametric model treatment instead outcome. \nstrategy, population mean outcome \\(E(Y^)\\) treatment \\(\\)\nequals weighted average units observed treatment,\nweighted weight equalswhen \\(= \\), inverse probability treatmentwhen \\(\\neq \\), zeroBelow identification proof inverse probability treatment\nweighting.math complicated, intuition simpler: \nweighting, create pseudo-population treatment \\(\\) \nindependent confounders \\(\\vec{L}\\). , need know\npropensity score: probability observed treatment value\ngiven confounders.identification result points toward inverse probability \ntreatment weighting estimator known Horvitz-Thompson estimator,\n\\[\\hat{E}(Y^) = \\frac{1}{n}\\sum_{=1}^n \\frac{Y_i\\mathbb{}(A_i=)}{\\hat{P}(= \\mid\\vec{L} = \\vec\\ell_i)}\\]\nrelated estimator often used Hajek estimator, \nnormalizes weights sum 1.\n\\[\\hat{E}(Y^) = \\frac{1}{\\sum_{=1}^n\\frac{\\mathbb{}(A_i=)}{\\hat{P}(= \\mid \\vec{L} = \\vec\\ell_i)}}\\sum_{=1}^n \\frac{Y_i\\mathbb{}(A_i=)}{\\hat{P}(= \\mid\\vec{L} = \\vec\\ell_i)}\\]code, wouldestimate model probability treatment given confounders,\nexample logistic regressionfor every unit, predict probability \\(= \\text{College}\\)estimate probability treatment observedfor went college, equals p_collegefor , equals 1 - p_collegeThis quantity often called propensity score. encapsulates\ninformation contained confounders probability \ntreatment.estimate mean outcomes among treatment, weighted \ninverse propensity score. actually two ways \n.Horvitz-Thompson estimator relies fact true\npropensity scores sum number observationsthe Hajek estimator uses weighted mean, thus normalizing weights\nsum 1While asymptotically valid, finite-sample reasons \nprefer second estimator (Hajek).","code":"\ntreatment_model <- glm(I(a == \"college\") ~ \n                         sex + race + \n                         mom_educ + dad_educ + \n                         log_parent_income +\n                         log_parent_wealth +\n                         test_percentile,\n                       data = d,\n                       family = binomial)\npredicted_p_college <- d %>%\n  mutate(p_college = predict(treatment_model,\n                             # the line below tells R\n                             # to predict a probability\n                             # rather than log odds\n                             type = \"response\"))\npredicted_p_scores <- predicted_p_college %>%\n  mutate(propensity_score = case_when(\n    a == \"college\" ~ p_college,\n    a == \"no_college\" ~ 1 - p_college\n  ))\npredicted_p_scores %>%\n  summarize(y1 = mean(y * I(a == \"college\") / propensity_score),\n            y0 = mean(y * I(a == \"no_college\") / propensity_score))## # A tibble: 1 × 2\n##      y1    y0\n##   <dbl> <dbl>\n## 1 0.374 0.164\npredicted_p_scores %>%\n  group_by(a) %>%\n  summarize(estimate = weighted.mean(y, w = 1 / propensity_score))## # A tibble: 2 × 2\n##   a          estimate\n##   <chr>         <dbl>\n## 1 college       0.401\n## 2 no_college    0.163"},{"path":"statistical-modeling.html","id":"worked-example-matching","chapter":"5 Statistical modeling","heading":"5.8.4 Worked example: Matching","text":"also estimate matching. Matching can interpreted \noutcome modeling strategy conditional mean outcome\n\\(E(Y\\mid\\vec{} = , \\vec{L} = \\vec\\ell_i)\\) estimated mean\noutcome among set units whose confounder values similar \nunit \\(\\) received treatment value \\(\\) interest.One way defining ``similar’’ propensity score matching: find\nunits whose probability treatment given confounders close \nprobability unit \\(\\). example, code ,estimates probability college completion using logistic\nregressionfor person finished college, matches non-college\ngraduate whose probability completing college similarThe variable matched$weights one element person \ndataset. indicates many times person appears matched\nsample. are1,533 college graduates weight 11,533 matched non-graduates weight 14,705 non-matched non-graudates weight 0Within matched data, non-graduates graduates similar\nalong confounding variables. estimatethe probability college educated spouse among college\ngraduates mean among peoplethe probability persisted \nfinished college, mean among matched counterpartsAn even better estimator might use linear regression adjust \ndifferences within matched pairs exist matches \nidentical., predict potential outcoems matched fit report \nexpected outcome among college graduates factual treatment\nunderThese results suggest completing college increases probability\ncollege-educated spouse partner 27 percentage\npoints.","code":"\nlibrary(MatchIt)\nmatched <- matchit(a == \"college\" ~ sex + race + mom_educ + \n                     dad_educ + log_parent_income + \n                     log_parent_wealth + test_percentile,\n                   method = \"nearest\", \n                   distance = \"glm\",\n                   estimand = \"ATT\",\n                   data = d)\ntable(d$a,matched$weights)##             \n##                 0    1\n##   college       0 1533\n##   no_college 4705 1533\nd %>%\n  mutate(weight = matched$weights) %>%\n  group_by(a) %>%\n  summarize(p_spouse_college = weighted.mean(y, w = weight))## # A tibble: 2 × 2\n##   a          p_spouse_college\n##   <chr>                 <dbl>\n## 1 college               0.528\n## 2 no_college            0.232\nmatched_fit <- lm(y ~ a*(sex + race + mom_educ + \n                           dad_educ + log_parent_income + \n                           log_parent_wealth + test_percentile), \n                  data = d, \n                  weights = matched$weights)\n# Create data frames for prediction\ncollege_grads_factual <- d %>%\n  filter(a == \"college\")\ncollege_grads_counterfactual <- college_grads_factual %>%\n  mutate(a = \"no_college\")\n\n# Predict outcomes from the model\ncollege_grads_factual %>%\n  mutate(yhat_college = predict(matched_fit, \n                                newdata = college_grads_factual),\n         yhat_no_college = predict(matched_fit,\n                                   newdata = college_grads_counterfactual)) %>%\n  # Report estimated average outcomes\n  select(starts_with(\"yhat\")) %>%\n  summarize_all(.funs = mean) %>%\n  # Estimate the causal effect\n  mutate(effect = yhat_college - yhat_no_college)## # A tibble: 1 × 3\n##   yhat_college yhat_no_college effect\n##          <dbl>           <dbl>  <dbl>\n## 1        0.528           0.259  0.269"},{"path":"front-door.html","id":"front-door","chapter":"6 Front door","heading":"6 Front door","text":"Oct 12. Slides. class, read Hernán Robins 2020 Technical Point 7.4. Optionally, see Glynn Kashin 2018This lecture engage new methods causal identification beyond backdoor adjustment. learning goals generalengage new causal identification approachtranslate method codecritique identification assumptionsFront door methods causal identification one case use show building blocks already know prepared learn new approaches causal identification.","code":""},{"path":"front-door.html","id":"identification","chapter":"6 Front door","heading":"Identification","text":"focus simplest case front door identification, depicted DAG variables \\(\\), \\(M\\), \\(Y\\) binary.setting, slides show following identification result.\\[P(Y^)=\\sum_m P(M = m\\mid = ) \\sum_{'}P(= ')P(Y\\mid M = m, = ')\\]","code":""},{"path":"front-door.html","id":"code-example","chapter":"6 Front door","heading":"Code example","text":"lecture slides translate method code one simulated example. providing code make easy copy follow along.Examine descriptive relationship \\(\\) \\(Y\\).Estimate probability \\(M\\) given \\(\\). causal assumptions, corresponds expected value \\(M\\) assignment value \\(\\) since \\(M\\rightarrow \\) unconfounded.Within front-door identification formula, need marginal probability treatment value.also need outcome distribution given \\(M\\) \\(\\).Given , can use backdoor adjustment identify outcome intervention \\(M\\) backdoor adjustment \\(\\).Bringing together, front-door identification.","code":"\nlibrary(tidyverse)\nsim_data <- function(n = 100) {\n  data.frame(U = runif(n)) %>%\n    # Generate a binary treatment\n    mutate(A = rbinom(n(), \n                      prob = U, \n                      size = 1)) %>%\n    # Generate a binary mediator\n    mutate(M = rbinom(n(), \n                      prob = .1 + .8*A, \n                      size = 1)) %>%\n    # Generate a binary outcome\n    mutate(Y = rbinom(n(), \n                      prob = plogis(U + .5*M), \n                      size = 1))\n}\ndata <- sim_data(n = 10e3)\ndata %>%\n  group_by(A) %>%\n  summarize(Y = mean(Y))## # A tibble: 2 × 2\n##       A     Y\n##   <int> <dbl>\n## 1     0 0.601\n## 2     1 0.751\np_M_given_A <- data %>%\n  # Count size of each group\n  group_by(A, M) %>%\n  count() %>%\n  # Convert to probability within A\n  group_by(A) %>%\n  mutate(p_M_under_A = n / sum(n)) %>%\n  select(A,M,p_M_under_A) %>%\n  print()## # A tibble: 4 × 3\n## # Groups:   A [2]\n##       A     M p_M_under_A\n##   <int> <int>       <dbl>\n## 1     0     0      0.891 \n## 2     0     1      0.109 \n## 3     1     0      0.0908\n## 4     1     1      0.909\n# Probability of each A\np_A <- data %>%\n  # Count size of each group\n  group_by(A) %>%\n  count() %>%\n  # Convert to probability\n  ungroup() %>%\n  mutate(p_A = n / sum(n)) %>%\n  select(A,p_A) %>%\n  print()## # A tibble: 2 × 2\n##       A   p_A\n##   <int> <dbl>\n## 1     0 0.494\n## 2     1 0.506\n# Probability of Y = 1 given M and A\np_Y_given_M_A <- data %>%\n  group_by(A,M) %>%\n  summarize(P_Y_given_A_M = mean(Y),\n            .groups = \"drop\") %>%\n  print()## # A tibble: 4 × 3\n##       A     M P_Y_given_A_M\n##   <int> <int>         <dbl>\n## 1     0     0         0.589\n## 2     0     1         0.693\n## 3     1     0         0.65 \n## 4     1     1         0.761\n# Probability of Y = 1 under intervention on M\np_Y_under_M <- p_Y_given_M_A %>%\n  left_join(p_A, by = \"A\") %>%\n  group_by(M) %>%\n  summarize(p_Y_under_M = sum(P_Y_given_A_M  * p_A)) %>%\n  print()## # A tibble: 2 × 2\n##       M p_Y_under_M\n##   <int>       <dbl>\n## 1     0       0.620\n## 2     1       0.727\n# Probability of Y = 1 under intervention on A\np_Y_under_A <- p_M_given_A %>%\n  left_join(p_Y_under_M,\n            by = \"M\") %>%\n  group_by(A) %>%\n  summarize(estimate = sum(p_M_under_A * p_Y_under_M)) %>%\n  print()## # A tibble: 2 × 2\n##       A estimate\n##   <int>    <dbl>\n## 1     0    0.632\n## 2     1    0.717"},{"path":"instrumental-variables.html","id":"instrumental-variables","chapter":"7 Instrumental variables","heading":"7 Instrumental variables","text":"","code":""},{"path":"instrumental-variables.html","id":"experimental-settings","chapter":"7 Instrumental variables","heading":"7.1 Experimental settings","text":"Oct 17. Slides.instrumental variable (IV) identification strategy applies treatment effect \\(\\) \\(Y\\) confounded unobserved variables (\\(U\\)), instrument \\(Z\\) creates random unconfounded variation \\(\\).clean setting IV randomized experiments non-compliance: experimenter randomizes assigned treatment (\\(Z\\)) actual treatment (\\(\\)) may unequal \\(Z\\) units follow assignment. first class discuss setting.","code":""},{"path":"instrumental-variables.html","id":"lab-instrumental-variable-analysis-in-code","chapter":"7 Instrumental variables","heading":"7.2 Lab: Instrumental Variable analysis in Code","text":"lab implement instrumental variables estimators R.October 18 Slides. Download \nR Markdown file . Update:\nSolutions coding exercise .","code":""},{"path":"instrumental-variables.html","id":"observational-settings","chapter":"7 Instrumental variables","heading":"7.3 Observational settings","text":"Oct 19 Slides. class, read Hernán Robins 2020 Chapter 16.Thursday, move IV analysis observational settings. focus casual assumptions required IV. assumptions often hold design experiments non-compliance. observational settings, can doubtful.","code":""},{"path":"regression-discontinuity.html","id":"regression-discontinuity","chapter":"8 Regression discontinuity","heading":"8 Regression discontinuity","text":"","code":""},{"path":"regression-discontinuity.html","id":"introduction","chapter":"8 Regression discontinuity","heading":"8.1 Introduction","text":"Oct 24. Slides. class, read Huntington Klein 2021 Chapter 20, sections 20.1 20.3.4.settings, treatment assigned based solely value continuous variable. situations, can identify local ATE without many additional assumptions. Today introduce works using regression discontinuity designs.","code":""},{"path":"regression-discontinuity.html","id":"lab-regression-discontinuity---bandwidth-and-examples","chapter":"8 Regression discontinuity","heading":"8.2 Lab: Regression Discontinuity - Bandwidth and Examples","text":"Oct 25 lab, read Huntington Klein 2021 Chapter 20, section 20.2.1.\nSlides Download today’s .Rmd document .’ll discuss bandwidth selection triangular weighting. ’ll also apply regression discontinuity concrete example give chance try .","code":""},{"path":"regression-discontinuity.html","id":"discussion","chapter":"8 Regression discontinuity","heading":"8.3 Discussion","text":"Oct 26. Slides. class, read Huntington Klein 2021 Chapter 20, sections 20.2.2. 20.3.1.’ll continue discussion regression discontinuity designs discussing fuzzy RDD validation checks RDD.","code":""},{"path":"difference-in-difference.html","id":"difference-in-difference","chapter":"9 Difference in difference","heading":"9 Difference in difference","text":"","code":""},{"path":"difference-in-difference.html","id":"introduction-1","chapter":"9 Difference in difference","heading":"9.1 Introduction","text":"Oct 31. Slides. reading required, reference see Card & Krueger 1994Today study effect policy change New Jersey, drawing evidence neighboring state Pennsylvania.Difference difference identification strategy used one units become treated time point others . believe assumption parallel trends, can use change time never-treated units estimate change time experienced units become treated, counterfactual world become treated.","code":""},{"path":"difference-in-difference.html","id":"lab","chapter":"9 Difference in difference","heading":"9.2 Lab","text":"Nov 1 Slides. Download \nR Markdown file .lab, implement difference difference estimator specific setting. example comes Malesky, Nguyen, & Tran 2014 closely follow re-analysis data Egami & Yamauchi 2023.","code":""},{"path":"difference-in-difference.html","id":"extensions-of-did","chapter":"9 Difference in difference","heading":"9.3 Extensions of DID","text":"Nov 2. Slides. reading required, reference see Egami & Yamauchi 2023How know parallel trends assumption holds? hold? class discusses recent extensions framework.","code":""},{"path":"synthetic-control.html","id":"synthetic-control","chapter":"10 Synthetic control","heading":"10 Synthetic control","text":"","code":""},{"path":"synthetic-control.html","id":"introduction-2","chapter":"10 Synthetic control","heading":"10.1 Introduction","text":"Nov 7 Slides.","code":""},{"path":"synthetic-control.html","id":"lab-1","chapter":"10 Synthetic control","heading":"10.2 Lab","text":"Nov 8 SlidesWe review main idea behind synthetic control well compare contrast synthetic control matching difference--differences. also dig deeper select weights synthetic control review worked example assess performance method.","code":""},{"path":"synthetic-control.html","id":"discussion-1","chapter":"10 Synthetic control","heading":"10.3 Discussion","text":"Nov 9 Slides. (Updated explanation interference).\nclass, read Chapter 10 Causal Inference Mixtape Cunningham 2021","code":""},{"path":"data-driven-methods.html","id":"data-driven-methods","chapter":"11 Data-driven methods","heading":"11 Data-driven methods","text":"","code":""},{"path":"data-driven-methods.html","id":"introduction-3","chapter":"11 Data-driven methods","heading":"11.1 Introduction","text":"Nov 14 Slides.given intervention, subgroups people respond others. Ideas machine learning can help us target human attention toward subgroups.Concrete example. responds nudget go walk? Imagine first conduct survey asks people much love fall, (\\(X = 1\\) least) (\\(X = 10\\) ). randomize control condition (\\(= \\texttt{untreated}\\)) treatment condition (\\(= \\texttt{treated}\\)) encourages go walk outside. outcome \\(Y\\) active minutes day, recorded activity tracker.Simulated data. real data, can difficult evaluate causal estimators truth unknown. Today use data simulated known process order study properties estimators. code prepare R environment function simulate_sample() generate data 50 observations.example code simulate data:Causal estimands. example, like estimate \\[\\tau_x = E(\\underbrace{Y^1 - Y^0}_{\\substack{\\text{effect }\\\\\\text{nudge walk}\\\\\\text{active}\\\\\\text{minutes}}}\\mid \\underbrace{X = x}_{\\substack{\\text{among }\\\\\\text{love }\\\\\\text{fall = }x}})\\]\nvalue \\(x = 1,\\dots,10\\). estimands average causal effect nudge walk (\\(\\)) active minutes (\\(Y\\)) within subgroups defined value scale love fall (\\(X\\)).Identification. simulate data, \\(\\) assigned random. backdoor paths \\(\\) \\(Y\\).Estimator. estimator function takes dataset returns estimates. nonparametric estimator setting.can apply estimator follows.Task. Using sample simulated computer, estimate average causal effect \\(\\) \\(Y\\) within subgroups defined \\(X\\). Report two numbers us.value \\(X\\) estimated effect \\(\\) positive?effect estimate?discuss distribution estimates get class.ready early, think might evaluate performance approach many repeated simulations.","code":"\nlibrary(tidyverse)\nsource(\"https://raw.githubusercontent.com/causal3900/causal3900.github.io/main/assets/data/simulate_sample.R\")\nsimulated <- simulate_sample()##   X         A         Y\n## 1 1 untreated  18.79989\n## 2 1   treated  62.71983\n## 3 1 untreated 142.74158\n## 4 1   treated  34.86843\n## 5 1 untreated 138.84262\n## 6 2   treated  10.54790\nestimator <- function(data) {\n  data %>%\n    # Group by treatment A and confounder X\n    group_by(A, X) %>%\n    # Summarize by the average outcome within groups\n    summarize(Y = mean(Y),\n              .groups = \"drop\") %>%\n    # Reshape the data\n    pivot_wider(names_from = \"A\",\n                values_from = \"Y\",\n                names_prefix = \"y_\") %>%\n    # Estimate the effect within groups\n    mutate(effect = y_treated - y_untreated)\n}\nestimate <- estimator(simulated)"},{"path":"data-driven-methods.html","id":"machine-learning-approaches","chapter":"11 Data-driven methods","heading":"11.2 Machine learning approaches","text":"Nov 16 Slides.Today generalize ideas Tuesday. discuss sample splitting makes easier tochoose among many estimandschoose among many estimatorsdevelop new data science approaches","code":""},{"path":"current-research.html","id":"current-research","chapter":"12 Current research","heading":"12 Current research","text":"","code":""},{"path":"current-research.html","id":"research-discussion-sam","chapter":"12 Current research","heading":"12.1 Research discussion: Sam","text":"Nov 21 SlidesToday discussing causal discovery: might learn DAGs data","code":""},{"path":"current-research.html","id":"research-discussion-ian","chapter":"12 Current research","heading":"12.2 Research discussion: Ian","text":"Nov 28 Slides can optionally read project page: ilundberg.github.io/pstratreg.Sometimes treatment causes outcome undefined. problem well-studied randomized controlled trials biostatistics, people die end trial. talk applications idea study inequality require adjustment measured confounding. joint work Soonhong Cho (PhD Candidate, UCLA Political Science).","code":""},{"path":"course-recap.html","id":"course-recap","chapter":"13 Course recap","heading":"13 Course recap","text":"Nov 30We review learned semester.","code":""},{"path":"discussion-02.-stats-review.html","id":"discussion-02.-stats-review","chapter":"Discussion 02. Stats review","heading":"Discussion 02. Stats review","text":"SlidesTo execute simulations locally, download .Rmd ","code":"\nlibrary(ggplot2)\nlibrary(ggmosaic)\nlibrary(tibble)"},{"path":"discussion-02.-stats-review.html","id":"sample-expectations-converge-to-population","chapter":"Discussion 02. Stats review","heading":"13.1 Sample expectations converge to population","text":"can generate simulations show sample mean variance converge \npopulation values.","code":"\ntrue_mean <- 2\ntrue_var <- 5\n\nsample_mean_seq <- 1:3000\nsample_means <- vapply(\n  sample_mean_seq,\n  \\(x) mean(rnorm(n = x, mean = true_mean, sd = sqrt(true_var))),\n  numeric(1)\n)\nsample_variances <- vapply(\n  sample_mean_seq,\n  \\(x) {\n    data <- rnorm(n = x, mean = true_mean, sd = sqrt(true_var))\n    sample_mean <- mean(data)\n    sum((data - sample_mean)^2)/length(data)\n  },\n  numeric(1)\n)\n\nmeans <- tibble(\"N\" = sample_mean_seq, \"Sample Mean\" = sample_means)\nvars <- tibble(\"N\" = sample_mean_seq, \"Sample Variance\" = sample_variances)\n\ncolors <- c(\"Sample Mean\" = \"lightblue\", \"Population Mean\" = \"red\")\n\nggplot(means, aes(y = `Sample Mean`, x = N)) +\n  geom_line(color = \"lightblue\") +\n  geom_abline(slope = 0, intercept = true_mean, color = \"red\") +\n  theme_bw()\nggplot(vars, aes(y = `Sample Variance`, x = N)) +\n  geom_line(color = \"lightblue\") +\n  geom_abline(slope = 0, intercept = true_var, color = \"red\") +\n  theme_bw()"},{"path":"discussion-02.-stats-review.html","id":"simulate-conditional-expectations","chapter":"Discussion 02. Stats review","heading":"13.2 Simulate conditional expectations","text":"Simulate conditional expectations within groups differ sample\nmean.","code":"\ngroup1_means <- rnorm(100, mean = 20, sd = 5)\ngroup2_means <- rnorm(100, mean = 30, sd = 5)\ngroup_means <- data.frame(\n  \"Group\" = c(rep(\"Group 1\", 100), rep(\"Group 2\", 100)),\n  \"Values\" = c(group1_means, group2_means),\n  \"x\" = rnorm(200, 5, sd = 3)\n)\nggplot(group_means, aes(x = x, y = Values, color = Group)) +\n  geom_point() +\n  geom_abline(\n    slope = 0,\n    intercept = mean(group_means$Values),\n    show.legend = TRUE,\n    color = \"gray30\"\n  ) +\n  geom_abline(\n    slope = 0,\n    intercept = mean(group_means[group_means$Group == \"Group 1\", ]$Values),\n    show.legend = TRUE,\n    color = \"#F8766D\"\n  ) +\n  geom_abline(\n    slope = 0,\n    intercept = mean(group_means[group_means$Group == \"Group 2\", ]$Values),\n    show.legend = TRUE,\n    color = \"#00BFC4\"\n  ) +\n  theme_bw()"},{"path":"discussion-02.-stats-review.html","id":"show-independence-of-variables---example-of-two-dice-rolling","chapter":"Discussion 02. Stats review","heading":"13.3 Show independence of variables - example of two dice rolling","text":"","code":"\ndice_1 <- sample(1:6, 100000, replace = TRUE)\ndice_2 <- sample(1:6, 100000, replace = TRUE)\ndice <- tibble(\n  \"dice\" = c(rep(\"Die 1\", 100000), rep(\"Die 2\", 100000)),\n  \"value\" = c(dice_1, dice_2)\n)\n\nggplot(data = dice) +\n  geom_mosaic(aes(x = product(dice, value), fill = dice)) +   \n  labs(y=\"\", x=\"Value Rolled\", title = \"Independence of dice roll\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"none\")"},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"discussion-03.-analyzing-an-experiment-in-r","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"Discussion 03. Analyzing an Experiment in R","text":"Slides","code":""},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"download-.rmd-document","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.4 Download .Rmd Document","text":"Download today’s .Rmd document .","code":""},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"get-out-and-vote-experiment","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.5 Get out and Vote Experiment","text":"lab, explore experiment digs mechanisms\nunderlying people vote. exercise based :Gerber, Alan S., Donald P. Green, Christopher W. Larimer. “Social Pressure Voter Turnout: Evidence Large-scale Field Experiment.” American Political Science Review 102.1 (2008): 33-48.long-standing theory many people\nvote driven social norms (e.g. understanding voting\ncivic duty). theory, dominant theoretical\nexplanation, little empirical backing long time. experiment\nexamines theory asking question:\nextent social norms cause voter turnout?","code":""},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"experimental-design","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.5.1 Experimental Design","text":"order answer question, approximately 80,000 Michigan households\nrandomly assigned treatment control groups, treatment\ngroup randomly assigned one four possible treatment arms. \ntreatment arms varied intensity social pressure conveyed,\ndefined follows:first treatment arm mailed letter simply reminded \nvoting civic duty.second treatment arm mailed letter telling researchers\nstudying voting turnout based public records.third treatment arm mailed letter stating voting turnout\nrevealed members household.fourth treatment arm mailed letter stating voting turnout\nrevealed household neighbors.","code":""},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"analyze-experiment","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.6 Analyze Experiment","text":"Download RMarkdown file .","code":""},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"necessary-packages","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.6.1 Necessary packages","text":"Note: errors probably either don’t dplyr haven\ninstalled.","code":"\nlibrary(dplyr)\nlibrary(haven)"},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"import-data","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.6.2 Import data","text":"Alternatively (really want), download data load directly computer. Make sure save data directory RMarkdown file .\ncan can import data :gotv <- read_dta(\"social_pressure.dta\")Run following code get quick peek dataset using function glimpse. returns info number rows/columns, column names, type data column. Notice information year birth yob explicitly age. Also notice treatments labeled numbers 0 4.","code":"\ngotv <- read_dta(\"https://causal3900.github.io/assets/data/social_pressure.dta\")\nglimpse(gotv)"},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"clean-data","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.6.3 Clean data","text":"First, let’s construct age variable describing old (number years)\nperson year 2006. yob variable says year person\nborn . , use mutate function, can read .Given person’s year birth, calculate age year 2006? Note can arithmetic operations information dataset. example, two columns col_1 col_2 wanted create third column called col_3 sum two columns, write:mutate(col_3 = col_1 + col_2)code started . Fill appropriate expression age = add column gotv labeled age contains old person 2006.Now, convert treatment variable ’s numeric representation \ncorresponding labels are0: “Control”1: “Hawthorne” (‘researchers viewing records via public data’ treatment arm)2: “Civic Duty” (‘voting civic duty’ treatment arm)3: “Neighbors” (‘voting turnout revealed neighbors’ treatment arm)4: “Self” (‘voting turnout revealed household’ treatment arm), want use function case_when described .\ngeneral syntax case_when(condition ~ output-value)example, condition treatement == 0 output value \"Control\". search every value treatment column equals 0 replace string \"Control\".started code . Decide argument(s) pass inside parantheses case_when().Now, use glimpse see added age variable treatments word instead number labels.","code":"\ngotv <- gotv |>\n  mutate(age = )\ngotv <- gotv |>\n  mutate(treatment = case_when()) \nglimpse(gotv)"},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"balance-table","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.6.4 Balance table","text":"Next, ’re going confirm control treatment groups look pretty\nmuch across set covariates, .e. two groups balanced covariates. Specifically means ’re going calculate mean value set covariates across treatment/control\narms, expect pretty much equal randomization worked. related idea exchangeability.exercise, going reproduce table similar Table 1 paper. want table shows mean value following covariates five treatment arms: Household size, Nov 2002, Nov 2000, Aug 2004, Aug 2002, Aug 2000, Female, Age (years). create table 5 rows, one treatment arm, 8 columns, one covariate interest.started code . need :Pass argument group_by() calculate seperate means treatment arm.\nLook documentation group_by function.\nLook documentation group_by function.Pass argument summarise() computes mean covariate covariates seperate treatment arm.\nLook documentation summarise function.\nmay find function across useful well. can use function inside summarise()!\nLook documentation summarise function.may find function across useful well. can use function inside summarise()!Note numbers match exactly Table 1. want notice values column similar across rows.","code":"\ncovariates <- c(\"sex\", \"age\", \"g2000\", \"g2002\", \"p2000\", \"p2002\", \"p2004\", \"hh_size\")\n\ngotv_balance <- gotv |>\n  group_by(...) |>\n  summarise(...)\n\nprint(gotv_balance)"},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"results","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.6.5 Results","text":"Finally, let’s replicate final results (Table 2). treatment group, calculate percentage individuals got voted, well total number individuals group! started code . need toPass argument group_by() working treatment arm seperately (!)Pass two arguments summarise() following:\nCreate column titled Percentage_Voting contains percent group voted\nCreate column titled num_of_individuals contains total number people group\nCreate column titled Percentage_Voting contains percent group votedCreate column titled num_of_individuals contains total number people group","code":"\ngotv_results <- gotv |>\n  group_by(...) |>\n  summarise(...)\n\nprint(gotv_results)"},{"path":"discussion-03.-analyzing-an-experiment-in-r-solutions.html","id":"discussion-03.-analyzing-an-experiment-in-r-solutions","chapter":"Discussion 03. Analyzing an Experiment in R SOLUTIONS","heading":"Discussion 03. Analyzing an Experiment in R SOLUTIONS","text":"possible solutions exercises discussion. code may look different , long output matters!can download file .","code":""},{"path":"discussion-03.-analyzing-an-experiment-in-r-solutions.html","id":"necessary-packages-1","chapter":"Discussion 03. Analyzing an Experiment in R SOLUTIONS","heading":"13.7 Necessary packages","text":"Note: errors probably either don’t dplyr haven installed.","code":"\nlibrary(dplyr)\nlibrary(haven)"},{"path":"discussion-03.-analyzing-an-experiment-in-r-solutions.html","id":"import-data-1","chapter":"Discussion 03. Analyzing an Experiment in R SOLUTIONS","heading":"13.7.1 Import data","text":"","code":"\ngotv <- read_dta(\"https://causal3900.github.io/assets/data/social_pressure.dta\")\nglimpse(gotv)## Rows: 344,084\n## Columns: 16\n## $ sex           <dbl+lbl> 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,…\n## $ yob           <dbl> 1941, 1947, 1951, 1950, 1982, 1981, …\n## $ g2000         <dbl+lbl> 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,…\n## $ g2002         <dbl+lbl> 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,…\n## $ g2004         <dbl+lbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n## $ p2000         <dbl+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n## $ p2002         <dbl+lbl> 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,…\n## $ p2004         <dbl+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n## $ treatment     <dbl+lbl> 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0,…\n## $ cluster       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n## $ voted         <dbl+lbl> 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,…\n## $ hh_id         <dbl> 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 5, 6, …\n## $ hh_size       <dbl> 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 1, 2, …\n## $ numberofnames <dbl> 21, 21, 21, 21, 21, 21, 21, 21, 21, …\n## $ p2004_mean    <dbl> 0.09523810, 0.09523810, 0.04761905, …\n## $ g2004_mean    <dbl> 0.8571429, 0.8571429, 0.8571429, 0.8…"},{"path":"discussion-03.-analyzing-an-experiment-in-r-solutions.html","id":"clean-data-1","chapter":"Discussion 03. Analyzing an Experiment in R SOLUTIONS","heading":"13.7.2 Clean data","text":"First, construct age variable describing old (number years) person year 2006. yob variable says year person born . > , use mutate function, can read .Fill appropriate expression age = add column gotv labeled age contains old person 2006.Now, convert treatment variable ’s numeric representation corresponding labels are0: “Control”1: “Hawthorne” (‘researchers viewing records via public data’ treatment arm)2: “Civic Duty” (‘voting civic duty’ treatment arm)3: “Neighbors” (‘voting turnout revealed neighbors’ treatment arm)4: “Self” (‘voting turnout revealed household’ treatment arm)One solution uses function case_when described .done differently. exercise, split task two pieces: create new column age gotv contains ages individual 2006, replace numeric treatment labels column treatment alphabetic/word labels. can actually one block code follows:Another alternative solution\nNow, use glimpse see added age variable treatments word instead number labels.Note, ways arrived ’s totally fine long results !","code":"\ngotv <- gotv |>\n  mutate(age = 2006 - yob)\ngotv <- gotv |>\n  mutate(treatment = case_when(\n      treatment == 0 ~ \"Control\",\n      treatment == 1 ~ \"Hawthorne\",\n      treatment == 2 ~ \"Civic Duty\",\n      treatment == 3 ~ \"Neighbors\",\n      treatment == 4 ~ \"Self\")) gotv <- gotv |>\n  mutate(\n    age = floor(2006 - yob),\n    treatment = case_when(\n      treatment == 0 ~ \"Control\",\n      treatment == 1 ~ \"Hawthorne\",\n      treatment == 2 ~ \"Civic Duty\",\n      treatment == 3 ~ \"Neighbors\",\n      treatment == 4 ~ \"Self\")\n    ) \nglimpse(gotv)## Rows: 344,084\n## Columns: 17\n## $ sex           <dbl+lbl> 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,…\n## $ yob           <dbl> 1941, 1947, 1951, 1950, 1982, 1981, …\n## $ g2000         <dbl+lbl> 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,…\n## $ g2002         <dbl+lbl> 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,…\n## $ g2004         <dbl+lbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n## $ p2000         <dbl+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n## $ p2002         <dbl+lbl> 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,…\n## $ p2004         <dbl+lbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n## $ treatment     <chr> \"Civic Duty\", \"Civic Duty\", \"Hawthor…\n## $ cluster       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n## $ voted         <dbl+lbl> 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,…\n## $ hh_id         <dbl> 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 5, 6, …\n## $ hh_size       <dbl> 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 1, 2, …\n## $ numberofnames <dbl> 21, 21, 21, 21, 21, 21, 21, 21, 21, …\n## $ p2004_mean    <dbl> 0.09523810, 0.09523810, 0.04761905, …\n## $ g2004_mean    <dbl> 0.8571429, 0.8571429, 0.8571429, 0.8…\n## $ age           <dbl> 65, 59, 55, 56, 24, 25, 47, 50, 38, …"},{"path":"discussion-03.-analyzing-an-experiment-in-r-solutions.html","id":"balance-table-1","chapter":"Discussion 03. Analyzing an Experiment in R SOLUTIONS","heading":"13.7.3 Balance table","text":"Next, confirm control treatment groups look pretty much across set covariates, .e. two groups balanced covariates. Specifically means calculate mean value set covariates across treatment/control arms, expect pretty much equal randomization worked. related idea exchangeability.solution uses functions group_by, summarise, across.may done something different. output (similar), fine!alternative solution explicitly use group_by:","code":"\ncovariates <- c(\"sex\", \"age\", \"g2000\", \"g2002\", \"p2000\", \"p2002\", \"p2004\", \"hh_size\")\n\ngotv_balance <- gotv |>\n  group_by(treatment) |>\n  summarise(across(.cols = covariates, .fns = mean))\n\nprint(gotv_balance)## # A tibble: 5 × 9\n##   treatment    sex   age g2000 g2002 p2000 p2002 p2004\n##   <chr>      <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1 Civic Duty 0.500  49.7 0.842 0.811 0.254 0.389 0.399\n## 2 Control    0.499  49.8 0.843 0.811 0.252 0.389 0.400\n## 3 Hawthorne  0.499  49.7 0.844 0.813 0.250 0.394 0.403\n## 4 Neighbors  0.500  49.9 0.842 0.811 0.251 0.387 0.407\n## 5 Self       0.500  49.8 0.840 0.811 0.251 0.392 0.402\n## # ℹ 1 more variable: hh_size <dbl>covariates <- c(\"sex\", \"age\", \"g2000\", \"g2002\", \"p2000\", \"p2002\", \"p2004\", \"hh_size\")\n\ngotv_results <- gotv |>\n  summarise(\n    across(.cols = all_of(covariates), .fns = mean),\n    .by = treatment\n    )\nprint(gotv_results)"},{"path":"discussion-03.-analyzing-an-experiment-in-r-solutions.html","id":"results-1","chapter":"Discussion 03. Analyzing an Experiment in R SOLUTIONS","heading":"13.7.4 Results","text":"Finally, treatment group, calculate percentage individuals got voted, well total number individuals group! solutions use function n counts number observations current group .Alternatively, write without using group_by explicitly:","code":"\ngotv_results <- gotv |>\n  group_by(treatment) |>\n  summarise(Percentage_Voting = mean(voted), num_of_individuals = n())\n\nprint(gotv_results)## # A tibble: 5 × 3\n##   treatment  Percentage_Voting num_of_individuals\n##   <chr>                  <dbl>              <int>\n## 1 Civic Duty             0.315              38218\n## 2 Control                0.297             191243\n## 3 Hawthorne              0.322              38204\n## 4 Neighbors              0.378              38201\n## 5 Self                   0.345              38218gotv |>\n  summarise(Percentage_Voting = mean(voted), num_of_individuals = n(), .by = treatment)"},{"path":"discussion-07.-causal-effects-with-matching.html","id":"discussion-07.-causal-effects-with-matching","chapter":"Discussion 07. Causal Effects with Matching","heading":"Discussion 07. Causal Effects with Matching","text":"Slides\nDownload today’s .Rmd document .packages may need install first:optmatch: install.packages(\"optmatch\")sandwich: install.packages(\"sandwich\")MatchIt: install.packages(\"MatchIt\")marginaleffects: install.packages(\"marginaleffects\")","code":""},{"path":"discussion-07.-causal-effects-with-matching.html","id":"r-markdown","chapter":"Discussion 07. Causal Effects with Matching","heading":"13.8 R Markdown","text":"Learn matching “MatchIt” packageFirst, ’ll use data set last week compare greedy vs optimal matching. ’ll use NLSY data since larger.First compare optimal vs greedy. Optimal matching usually better greedy matching, long data isn’t bigOn full data, using optimal possible, can take bit time. larger data sets, might possible","code":"\nd <- readRDS(\"assets/discussions/d.RDS\")\n\n# matchit function implements matching\n# Formula: Treatment ~ variables to match on\n# method: nearest is a greedy 1:1 matching without replacement\n# distance: euclidean (other possible options are scaled_euclidean, mahalanobis, robust_mahalanobis)\n# read more on distances here: https://rdrr.io/cran/MatchIt/man/distance.html#mat\nm.out0 <- matchit(a == \"college\" ~ log_parent_income + log_parent_wealth\n                              + test_percentile,\n                              method = \"nearest\", distance = \"euclidean\",\n                              data = d)\n## Optimal vs greedy with NYLSR data\n# With n = 1000; .01 sec vs .8 sec\n# With n = 2000; .05 sec vs 8 sec\n# With n = 4000; .1 vs 25\n\nind <- sample(nrow(d), size = 1000)\n\n# Greedy is using nearest\nsystem.time(m.out0 <- matchit(a == \"college\" ~ log_parent_income + log_parent_wealth\n                              + test_percentile,\n                              method = \"nearest\", distance = \"euclidean\",\n                              data = d[ind, ]))##    user  system elapsed \n##   0.016   0.000   0.016\n# method: optimal is optimal matching\nsystem.time(m.out0 <- matchit(a == \"college\" ~ log_parent_income + log_parent_wealth\n                              + test_percentile,\n                              method = \"optimal\", distance = \"euclidean\",\n                              data = d[ind, ]))##    user  system elapsed \n##   1.290   0.077   1.524\n# With full data set greedy matching takes ~ 0.5-1.5 seconds\nsystem.time(m.out0 <- matchit(a == \"college\" ~ log_parent_income + log_parent_wealth\n                              + test_percentile,\n                              method = \"nearest\", distance = \"euclidean\",\n                              data = d))##    user  system elapsed \n##   0.605   0.077   0.684\n# Meanwhile, optimal matching takes 60-130 seconds\nsystem.time(m.out0 <- matchit(a == \"college\" ~ log_parent_income + log_parent_wealth\n                              + test_percentile,\n                              method = \"optimal\", distance = \"euclidean\",\n                              data = d))##    user  system elapsed \n## 117.511   5.250 124.215"},{"path":"discussion-07.-causal-effects-with-matching.html","id":"matching-with-job-training-data-from-evaluating-the-econometric-evaluations-of-training-programs-with-experimental-data-lalonde-1986","chapter":"Discussion 07. Causal Effects with Matching","heading":"13.9 Matching with job training data from “Evaluating the econometric evaluations of training programs with experimental data” (LaLonde 1986)","text":"remainder lab, ’ll use portion data job training program. particular, treatment whether someone participated job training program. outcome interest salary 1978 (re78).expect income 1974 highly correlated income 1975. also much higher variability age.Now let’s try run matching procedure using function.","code":"\n# Load the data\ndata(\"lalonde\")\n\n# See what's in the data\n?lalonde # this opens up the \"Help\" tab with the documentation! \nhead(lalonde)##      treat age educ   race married nodegree re74 re75\n## NSW1     1  37   11  black       1        1    0    0\n## NSW2     1  22    9 hispan       0        1    0    0\n## NSW3     1  30   12  black       0        0    0    0\n## NSW4     1  27   11  black       0        1    0    0\n## NSW5     1  33    8  black       0        1    0    0\n## NSW6     1  22    9  black       0        1    0    0\n##            re78\n## NSW1  9930.0460\n## NSW2  3595.8940\n## NSW3 24909.4500\n## NSW4  7506.1460\n## NSW5   289.7899\n## NSW6  4056.4940\n## Suppose there are 3 individuals\ndat <- matrix(c(50, 5000, 5500,\n                20, 5100, 5900,\n                40, 5200, 6200), ncol = 3, byrow = T)\ncolnames(dat) <- c(\"age\", \"re74\", \"re75\")\n\n# Is individual 2 or 3 more similar to individual 1? \n# To answer this, we should compute the distances between individuals 1 and 2, and 1 and 3. \n\n# One way is to compute the Mahalanobis distance by first computing the covariance matrix of the confounding variables\n# In this case, the confounders are age, re74, and re75\ndataCov <- lalonde %>%\n  select(age, re74, re75) %>%\n  cov\n\n# Then we compute the Mahalanobis distance with the function mahalanobis_dist\nmahalanobis_dist( ~ age + re74 + re75, data = dat, var = dataCov)##          1        2        3\n## 1 0.000000 3.225953 1.098595\n## 2 3.225953 0.000000 2.152528\n## 3 1.098595 2.152528 0.000000\n# We can also compute the Euclidean distance\ndist(dat, method = \"euclidean\")##          1        2\n## 2 413.4005         \n## 3 728.0797 316.8596\n# For now, let's start with Euclidean distance, even if may not be great\n\n# method: nearest (i.e. greedy 1:1)\n# distance: euclidean\n# data: lalonde (the data set we're working with)\n# replace: True (T) or False (F) - whether to sample with or without replacement. \n    # Note, if allowing for replacement, greedy and optimal are the same\n    # So for the function, you only need to specify if using method = \"nearest\"\n# ratio: how many control matches for each treated unit\n# caliper: by default, the caliper width in standard units (i.e., Z-scores)\nm.out0 <- matchit(treat ~ re74 + re75 + age + race,\n                  method = \"nearest\", distance = \"euclidean\",\n                  data = lalonde, replace = F, \n                  ratio = 1, caliper = c(re74  = .2, re75 = .2))"},{"path":"discussion-07.-causal-effects-with-matching.html","id":"assessing-the-matching","chapter":"Discussion 07. Causal Effects with Matching","heading":"13.10 Assessing the matching","text":"can check well balancing doneWe can also visually asses matchesAs first step, simply compare means outcomes groups. Notice first time ’ve looked outcomes!","code":"\n?summary.matchit # Look in the Help tab (on the bottom right) for documentation on summary.matchit\n\n\n# interactions: check interaction terms too? (T or F)\n# un: show statistics for unmatched data as well (T or F)\nsummary(m.out0, interactions = F, un = F)## \n## Call:\n## matchit(formula = treat ~ re74 + re75 + age + race, data = lalonde, \n##     method = \"nearest\", distance = \"euclidean\", replace = F, \n##     caliper = c(re74 = 0.2, re75 = 0.2), ratio = 1)\n## \n## Summary of Balance for Matched Data:\n##            Means Treated Means Control Std. Mean Diff.\n## re74           1643.2931     1666.9106         -0.0048\n## re75           1021.5989     1086.1734         -0.0201\n## age              25.6971       26.1543         -0.0639\n## raceblack         0.8400        0.2800          1.5403\n## racehispan        0.0571        0.1714         -0.4833\n## racewhite         0.1029        0.5486         -1.5040\n##            Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\n## re74           1.0122    0.0108   0.1543          0.0293\n## re75           1.0026    0.0166   0.1543          0.0422\n## age            0.4213    0.0894   0.2000          0.9967\n## raceblack           .    0.5600   0.5600          1.7289\n## racehispan          .    0.1143   0.1143          0.7249\n## racewhite           .    0.4457   0.4457          1.6968\n## \n## Sample Sizes:\n##           Control Treated\n## All           429     185\n## Matched       175     175\n## Unmatched     254      10\n## Discarded       0       0\n# Std. Mean Diff (SMD): difference of means, standardized by sd of treatment group\n# Var. Ratio: ratio of variances in treatment and control group. Compares spread of data\n# Rubin (2001) presents rule of thumb that SMD should be less than .25 and variance ratio should be between .5 and 2\n# Max eCDF: Kolmogorov-Smirnov statistic. Max difference across entire CDF\n## Produces QQ plots of all variables in which.xs\nplot(m.out0, type = \"qq\", which.xs = ~age + re74, interactive = F)\n## Plots the density of all variables in which.xs\nplot(m.out0, type = \"density\", which.xs = ~age + re74 + race, interactive = F)\n## Plots the empirical CDF of all variables in which.xs\nplot(m.out0, type = \"ecdf\", which.xs = ~age + re74, interactive = F)\n# Produces data frame same as input, but has two additional columns\n# weights: the weight of the row in the paired data set. Can be greater than 1\n#         if the data set was matched more than once\n# subclass: the index of the \"pair\"\n#\nm.out0.data <- match.data(m.out0, drop.unmatched = T)\nhead(m.out0.data)##      treat age educ   race married nodegree re74 re75\n## NSW1     1  37   11  black       1        1    0    0\n## NSW2     1  22    9 hispan       0        1    0    0\n## NSW3     1  30   12  black       0        0    0    0\n## NSW4     1  27   11  black       0        1    0    0\n## NSW5     1  33    8  black       0        1    0    0\n## NSW6     1  22    9  black       0        1    0    0\n##            re78 weights subclass\n## NSW1  9930.0460       1        1\n## NSW2  3595.8940       1       88\n## NSW3 24909.4500       1       99\n## NSW4  7506.1460       1      110\n## NSW5   289.7899       1      121\n## NSW6  4056.4940       1      132\nnames(m.out0.data)##  [1] \"treat\"    \"age\"      \"educ\"     \"race\"     \"married\" \n##  [6] \"nodegree\" \"re74\"     \"re75\"     \"re78\"     \"weights\" \n## [11] \"subclass\"\n# Also produces matched data set, though will duplicate rows if matching with replacement\n# and a control is matched more than once\nm.out0.data <- get_matches(m.out0)\n# Take the mean of both groups\n# this will only work if all weights are 1\naggregate(re78~ treat, FUN = mean, data = m.out0.data)##   treat     re78\n## 1     0 5422.184\n## 2     1 6193.594\n# Fitting a linear model on the treatments will work\n# even if all weights are not 1. We just need to feed them in\nfit1 <- lm(re78~ treat, data = m.out0.data, weights = weights)\n\n# vcov: tells R to use robust standard errors\navg_comparisons(fit1, variables = \"treat\",\n                vcov = \"HC3\",\n                newdata = subset(m.out0.data, treat == 1),\n                wts = \"weights\")## Warning: The `treat` variable is treated as a categorical\n##   (factor) variable, but the original data is of class\n##   integer. It is safer and faster to convert such\n##   variables to factor before fitting the model and\n##   calling `slopes` functions.\n##   \n##   This warning appears once per session.## \n##   Term Contrast Estimate Std. Error    z Pr(>|z|)   S 2.5 %\n##  treat    1 - 0      771        753 1.02    0.306 1.7  -705\n##  97.5 %\n##    2248\n## \n## Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n## Type:  response"},{"path":"discussion-07.-causal-effects-with-matching.html","id":"fit-your-own-model","chapter":"Discussion 07. Causal Effects with Matching","heading":"13.11 Fit your own model","text":"Now try . Note, need something similar homework dataset.Think appropriate DAG might choose variables want match \nAsk : know choose variables match ?\nAsk : know choose variables match ?Choose matching procedure\nAsk : know explain matching procedure bias-variance trade ?\nAsk : know explain matching procedure bias-variance trade ?Evaluate balance match. doesn’t look good, try another matching procedure\nAsk : know balanced matching looks like?\nAsk : know balanced matching looks like?Estimate causal effect\nAsk : know just estimated?\nAsk : know just estimated?","code":""},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"discussion-10.-causal-effects-with-regression-discontinuity","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"Discussion 10. Causal Effects with Regression Discontinuity","text":"Slides\nDownload today’s .Rmd document .","code":""},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"choosing-a-bandwidth","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.12 Choosing a bandwidth","text":"One main difficulties regression discontinuity analysis choosing ``good’’ bandwidth. good example bias variance trade-appears time statistics. general, let software choose us, helpful run small example better understand subtleties involved.Suppose \\(Y\\) non-linear function \\(X\\), want extrapolate predict \\(Y\\) \\(X=0\\).blindly fit line observations, predicted value \\(X=0\\) quite bit.\ninstead restrict smaller region around 0, can bit betterIdeally, decrease bandwidth reduce bias, also means working less data. Since data point noisy, get new sample, estimate change quite bit. Try running times see much estimate moves. Compare much estimate moves bandwidth 2.Knowing best bandwidth hard problem, generally speaking good choice bandwidth get smaller sample size increases.measure accuracy terms average squared error, can see \\(n = 100\\), bandwidth 1.2 seems best. However, test see changes larger values \\(n\\)estimate linear regression model, usually pick linear coefficients minimize squared errors\n\\[\\min_b \\sum_i(Y_i - X_i b)^2\\]\nsquared error observation count considered equally. good fit around points important points, can also use weighted least squares \\(w_i\\) weight.\n\\[\\min_b \\sum_i w_i(Y_i - X_i b)^2\\]\n\\(w_i\\) larger, squared error observation \\(\\) cost selected linear coefficient prioritize minimizing errors \\(w_i\\) large. particular setting, want prioritize fitting data well around \\(0\\), can use weights ``triangular’’ prioritize points near \\(0\\).","code":"\nn <- 100\nX <- runif(n, -5, 0)\n### Y is a very non-linear function of X + some noise\nY <- 5 - 3*X + .6 * X^2 + .3 * X^3 + 15 * sin(X) + rnorm(n, sd = 4)\nplot(X, Y, ylim = c(-15, 30), xlim = c(-5, 1))\nabline(v = 0, col = \"gray\", lwd = 2)\n\n### Avg Y when X = 0 is 5\npoints(0, 5, pch = 18, col = \"orange\", cex = 3)\n## Only consider points h away from 0\nh <- 3\n\nfit.mod.bandwidth <- lm(Y~X, subset = X > -h)\nplotColor <- ifelse(abs(X) > h, \"gray\", \"red\")\n\nplot(X, Y, ylim = c(-15, 30), xlim = c(-5, 1), col = plotColor)\nabline(v = 0, col = \"gray\", lwd = 2)\nsegments(-h, predict(fit.mod.bandwidth, newdata = data.frame(X = -h)),\n         0, predict(fit.mod.bandwidth, newdata = data.frame(X = 0)),col = \"blue\",\n         lwd = 2)\npoints(0, 5, pch = 18, col = \"orange\", cex = 3)\nn <- 10000\nX <- runif(n, -5, 0)\nY <- 5 - 3*X + .6 * X^2 + .3 * X^3 + 15 * sin(X) + rnorm(n, sd = 4)\nh <- 3\nfit.mod.bandwidth <- lm(Y~X, subset = X > -h)\nplotColor <- ifelse(abs(X) > h, \"gray\", \"red\")\n\nplot(X, Y, ylim = c(-15, 30), xlim = c(-5, 1), col = plotColor)\nabline(v = 0, col = \"gray\", lwd = 2)\nsegments(-h, predict(fit.mod.bandwidth, newdata = data.frame(X = -h)),\n         0, predict(fit.mod.bandwidth, newdata = data.frame(X = 0)),col = \"blue\",\n         lwd = 2)\n\npoints(0, 5, pch = 18, col = \"orange\", cex = 3)\nh <- .3 # bandwidth\n# h <- 2 \nn <- 100\nX <- runif(n, -5, 0)\nY <- 5 - 3*X + .6 * X^2 + .3 * X^3 + 15 * sin(X) + rnorm(n, sd = 4)\n\nfit.mod.bandwidth <- lm(Y~X, subset = X > -h)\nplotColor <- ifelse(abs(X) > h, \"gray\", \"red\")\n\nplot(X, Y, ylim = c(-15, 30), xlim = c(-5, 1), col = plotColor)\nabline(v = 0, col = \"gray\", lwd = 2)\nsegments(-h, predict(fit.mod.bandwidth, newdata = data.frame(X = -h)),\n         0, predict(fit.mod.bandwidth, newdata = data.frame(X = 0)),col = \"blue\",\n         lwd = 2)\n\npoints(0, 5, pch = 18, col = \"orange\", cex = 3)\npoints(0, predict(fit.mod.bandwidth, newdata = data.frame(X = 0)), col = \"blue\", pch = 19)\nn <- 100\nh <- c(.5, .8, 1.2, 2)\nsim.size <- 500\nrec <- matrix(0, sim.size, 8)\n\nfor(i in 1:sim.size){\n  X <- runif(n, -5, 0)\n  Y <- 5 - 3*X + .6 * X^2 + .3 * X^3 + 15 * sin(X) + rnorm(n, sd = 4)\n\n  fit.mod.bandwidth1 <- lm(Y~X, subset = X > -h[1])\n  fit.mod.bandwidth2 <- lm(Y~X, subset = X > -h[2])\n  fit.mod.bandwidth3 <- lm(Y~X, subset = X > -h[3])\n  fit.mod.bandwidth4 <- lm(Y~X, subset = X > -h[4])\n\n  rec[i, 1] <- (predict(fit.mod.bandwidth1, newdata = data.frame(X = 0)) - 5)^2\n  rec[i, 2] <- (predict(fit.mod.bandwidth2, newdata = data.frame(X = 0)) - 5)^2\n  rec[i, 3] <- (predict(fit.mod.bandwidth3, newdata = data.frame(X = 0)) - 5)^2\n  rec[i, 4] <- (predict(fit.mod.bandwidth4, newdata = data.frame(X = 0)) - 5)^2\n  \n  rec[i, 5] <- predict(fit.mod.bandwidth1, newdata = data.frame(X = 0))\n  rec[i, 6] <- predict(fit.mod.bandwidth2, newdata = data.frame(X = 0))\n  rec[i, 7] <- predict(fit.mod.bandwidth3, newdata = data.frame(X = 0))\n  rec[i, 8] <- predict(fit.mod.bandwidth4, newdata = data.frame(X = 0))\n\n}\n\ncolMeans(rec)## [1] 12.576240  5.638429  3.978909 12.279501  5.031613\n## [6]  4.777614  4.205076  1.844670\nboxplot(as.list(data.frame(rec[, 5:8])), ylim = c(0, 20))\nabline(h = 5, col = \"red\")\nn <- 200\nX <- runif(n, -5, 0)\nY <- 5 - 3*X + .6 * X^2 + .3 * X^3 + 15 * sin(X) + rnorm(n, sd = 4)\nweight <- 1 - abs(X) / 5 \nplot(X, weight, type = \"p\", main = \"Triangular Weights\")"},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"question","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.12.0.1 Question","text":"use bandwidth \\(h\\) (.e., include observations within \\(h\\) cut-), mean weights ?using triangular weights, can see better using points.","code":"\nn <- 200\nh <- 3\nX <- runif(n, -5, 0)\nY <- 5 - 3*X + .6 * X^2 + .3 * X^3 + 15 * sin(X) + rnorm(n, sd = 4)\n\nweight <- 1 - abs(X) / 5 \n# We don't include a bandwidth, but instead use triangular weights\nfit.mod <- lm(Y~X)\nfit.mod.weights <- lm(Y~X, weights = weight)\nfit.mod.bandwidth <- lm(Y~X, subset = abs(X) < h)\n\nplotColor <- ifelse(abs(X) > h, \"gray\", \"red\")\n\nplot(X, Y, ylim = c(-15, 30), xlim = c(-5, 1), col = plotColor)\nabline(v = 0, col = \"gray\", lwd = 2)\nsegments(-h, predict(fit.mod.bandwidth, newdata = data.frame(X = -h)),\n         0, predict(fit.mod.bandwidth, newdata = data.frame(X = 0)),col = \"blue\",\n         lwd = 2)\n\n# Fit from weighted least squares\nsegments(-5, predict(fit.mod.weights, newdata = data.frame(X = -h)),\n         0, predict(fit.mod.weights, newdata = data.frame(X = 0)),col = \"green\",\n         lwd = 2)\n\n# Fit from ordinary least squares\nsegments(-5, predict(fit.mod, newdata = data.frame(X = -h)),\n         0, predict(fit.mod, newdata = data.frame(X = 0)),col = \"purple\",\n         lwd = 2)\n\npoints(0, 5, pch = 18, col = \"orange\", cex = 3)\npoints(0, predict(fit.mod.bandwidth, newdata = data.frame(X = 0)), col = \"blue\", pch = 19)\npoints(0, predict(fit.mod.weights, newdata = data.frame(X = 0)), col = \"green\", pch = 19)\npoints(0, predict(fit.mod, newdata = data.frame(X = 0)), col = \"purple\", pch = 19)\n# bandwidth\nh <- 3\nn <- 200\nX <- runif(n, -5, 0)\nY <- 5 - 3*X + .6 * X^2 + .3 * X^3 + 15 * sin(X) + rnorm(n, sd = 4)\n\n# weight is triangular within the bandwidth\nweight <- ifelse(abs(X) < h, 1 - abs(X) / h, 0) \n\nplot(X, weight, type = \"p\", main = \"Triangular Weights\")"},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"rdd-analysis","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.13 RDD Analysis","text":"Now let’s apply regression discontinuity analysis data. example follows analysis ``Randomization Inference Regression Discontinuity Design: Application Party Advantages U.S. Senate’’ Cattaneo, Frandsen, Titiunik (2015) replication file.Political scientists interested effect incumbent (currently elected politician) share votes election. current public official means increased name recognition, fundraising opportunities, etc. hand, incumbent means can get blamed bad things happened term.data analyze considers US senators. state US two senators 6 year terms, election two senators alternates every 3 years. instance, two senate seats: B. 2000 senate seat undergoes election, 2003 senate seat B undergoes election senate seat continues, 2006 senate seat undergoes election senate seat B continues , etc.look causal effect incumbent senate races two different ways.First, incumbent effect vote share next time run office? Second, sitting senator party, effect election share?","code":""},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"question-1","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.13.0.1 Question","text":"Consider first question draw causal diagram treatment incumbent outcome interest vote share election.might use regression discontinuity provide answer question?","code":""},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"data","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.13.1 Data","text":"’ll examine data US Senate races 1914 2010 try answer questions.first analysis, consider whether incumbent effects vote share next election. Thus, outcome interest next time seat goes election 2 cycles future.Let’s zoom bit closer races use non-linear regression. appears still slight discontinuity.","code":"\ninstall <- function(package) {\n  if (!require(package, quietly = TRUE, character.only = TRUE)) {\n    install.packages(package, repos = \"http://cran.us.r-project.org\", type = \"binary\")\n    library(package, character.only = TRUE)\n  }\n}\n\ninstall(\"ggplot2\")\ninstall(\"lpdensity\")\ninstall(\"rddensity\")\ninstall(\"rdrobust\")\ninstall(\"rdlocrand\")\n\ndata <- read.csv(\"https://raw.githubusercontent.com/rdpackages-replication/CIT_2020_CUP/master/CIT_2020_CUP_senate.csv\")\n\nhead(data)##         state year dopen population presdemvoteshlag1\n## 1 Connecticut 1914     0    1233000          39.15937\n## 2 Connecticut 1916     0    1294000          39.15937\n## 3 Connecticut 1922     0    1431000          33.02737\n## 4 Connecticut 1926     0    1531000          27.52570\n## 5 Connecticut 1928     1    1577000          27.52570\n## 6 Connecticut 1932     0    1637000          45.57480\n##         demmv demvoteshlag1 demvoteshlag2 demvoteshfor1\n## 1  -7.6885610            NA            NA      46.23941\n## 2  -3.9237082      42.07694            NA      36.09757\n## 3  -6.8686604      36.09757      46.23941      35.64121\n## 4 -27.6680560      45.46875      36.09757      45.59821\n## 5  -8.2569685      35.64121      45.46875      48.47606\n## 6   0.7324815      45.59821      35.64121      51.74687\n##   demvoteshfor2 demwinprv1 demwinprv2 dmidterm dpresdem\n## 1      36.09757         NA         NA        1        1\n## 2      45.46875          0         NA        0        1\n## 3      45.59821          0          0        1        0\n## 4      48.47606          0          0        1        0\n## 5      51.74687          0          0        0        0\n## 6      39.80264          0          0        0        0\n# presdemvoteshlag1 is democratic vote share in the previous presidential election\n# demmv is the democratic margin of victory in the current senate election (i.e., democratic percentage - next closest percentage)\n#   so a value just above 0 indicates a very close victory, a value just below 0 indicates a very close loss\n# demovoteshlag1 and 2 indicates the vote share 1 and 2 election cycles ago\n# demovoteshfor1 and 2 indicates the vote share 1 and 2 elections cycles in the future\ndem_vote_t2 <- data$demvoteshfor2\ndem_margin_t0 <- data$demmv\n\n# plot the data\n# Set p = 0 for a straight line (i.e., regression with X^p)\nrdplot(y = dem_vote_t2, x =  dem_margin_t0, nbins = c(1000, 1000), p = 0, col.lines = \"red\", col.dots = \"lightgray\", title = \"Incumbency Advantage\", y.lim = c(0,100), x.label = \"Dem Margin of Victory\", y.label = \"Dem Vote Share in next election\")\nrdplot(dem_vote_t2[abs(dem_margin_t0) <= 25], dem_margin_t0[abs(dem_margin_t0) <= 25], nbins = c(2500, 500), p = 4, col.lines = \"red\", col.dots = \"lightgray\", title = \"\",  y.lim = c(0,100))"},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"estimating-the-causal-effect","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.13.2 Estimating the causal effect","text":"example, ’ll manually set bandwidth 10, estimate linear regression sides cut-. estimated intercept prediction \\(0\\), get estimate, just take difference.can try triangular weights.","code":"\n# Set bandwidth to 10\nh <- 10\n# Fit regression to left and right of cut-off\nlm_left <- lm(dem_vote_t2 ~ dem_margin_t0, subset = dem_margin_t0 < 0 & abs(dem_margin_t0) <= h)\nlm_right <- lm(dem_vote_t2 ~ dem_margin_t0, subset = dem_margin_t0 > 0 & abs(dem_margin_t0) <= h)\n\n# Estimate is difference in interecepts\nlm_right$coefficients[1] - lm_left$coefficients[1] ## (Intercept) \n##    6.898794\nh <- 10\nweight <- ifelse(abs(dem_margin_t0) < h, 1 - abs(dem_margin_t0) / h, 0) \n\n## Note we don't need subs\nlm_left <- lm(dem_vote_t2 ~ dem_margin_t0, subset = dem_margin_t0 < 0 & abs(dem_margin_t0) <= h, weights = weight)\nlm_right <- lm(dem_vote_t2 ~ dem_margin_t0, subset = dem_margin_t0 > 0 & abs(dem_margin_t0) <= h, weights = weight)\n\n\n# Estimate is difference in interecepts\nlm_right$coefficients[1] - lm_left$coefficients[1] ## (Intercept) \n##    7.984687"},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"question-2","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.13.2.1 Question","text":"Describe mathematical notation, plain language causal effect estimated.","code":""},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"using-rdrobust","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.14 Using rdrobust","text":"can see, getting estimates aren’t difficult ’ve selected bandwidth. selecting good bandwidth can tricky getting standard errors estimate also difficult. can use R package select bandwidth, estimate causal effect quantities, give standard errors.don’t specify bandwidth directly, software choose us","code":"\n# uniform kernel with bandwidth 10\nout <- rdrobust(dem_vote_t2, dem_margin_t0, kernel = 'uniform',  p = 1, h = 10)\nsummary(out)## Sharp RD estimates using local polynomial regression.\n## \n## Number of Obs.                 1297\n## BW type                      Manual\n## Kernel                      Uniform\n## VCE method                       NN\n## \n## Number of Obs.                  595          702\n## Eff. Number of Obs.             245          206\n## Order est. (p)                    1            1\n## Order bias  (q)                   2            2\n## BW est. (h)                  10.000       10.000\n## BW bias (b)                  10.000       10.000\n## rho (h/b)                     1.000        1.000\n## Unique Obs.                     595          702\n## \n## =============================================================================\n##         Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n## =============================================================================\n##   Conventional     6.899     1.722     4.007     0.000     [3.525 , 10.273]    \n##         Robust         -         -     3.891     0.000     [5.156 , 15.624]    \n## =============================================================================\n# triangular kernel with bandwidth 10\nout <- rdrobust(dem_vote_t2, dem_margin_t0,  kernel = 'triangular',  p = 1, h = 10)\nsummary(out)## Sharp RD estimates using local polynomial regression.\n## \n## Number of Obs.                 1297\n## BW type                      Manual\n## Kernel                   Triangular\n## VCE method                       NN\n## \n## Number of Obs.                  595          702\n## Eff. Number of Obs.             245          206\n## Order est. (p)                    1            1\n## Order bias  (q)                   2            2\n## BW est. (h)                  10.000       10.000\n## BW bias (b)                  10.000       10.000\n## rho (h/b)                     1.000        1.000\n## Unique Obs.                     595          702\n## \n## =============================================================================\n##         Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n## =============================================================================\n##   Conventional     7.985     1.838     4.344     0.000     [4.382 , 11.587]    \n##         Robust         -         -     4.387     0.000     [6.595 , 17.249]    \n## =============================================================================\n# uniform kernel with software selected bandwidth\nout <- rdrobust(dem_vote_t2, dem_margin_t0, kernel = 'triangular',  p = 1)\nsummary(out)## Sharp RD estimates using local polynomial regression.\n## \n## Number of Obs.                 1297\n## BW type                       mserd\n## Kernel                   Triangular\n## VCE method                       NN\n## \n## Number of Obs.                  595          702\n## Eff. Number of Obs.             360          323\n## Order est. (p)                    1            1\n## Order bias  (q)                   2            2\n## BW est. (h)                  17.754       17.754\n## BW bias (b)                  28.028       28.028\n## rho (h/b)                     0.633        0.633\n## Unique Obs.                     595          665\n## \n## =============================================================================\n##         Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n## =============================================================================\n##   Conventional     7.414     1.459     5.083     0.000     [4.555 , 10.273]    \n##         Robust         -         -     4.311     0.000     [4.094 , 10.919]    \n## ============================================================================="},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"try-on-your-own","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.14.1 Try on your own","text":"Now try estimate causal effect senator election democrat democratic vote share senator election. case, outcome interest demvoteshfor1 since interested immediately following election.","code":"\ndem_vote_t1 <- data$demvoteshfor1\ndem_margin_t0 <- data$demmv\n\n# plot the data\n# Set p = 0 for a straight line (i.e., regression with X^p)\nrdplot(y = dem_vote_t1, x =  dem_margin_t0, nbins = c(1000, 1000), p = 0, col.lines = \"red\", col.dots = \"lightgray\", title = \"Incumbency Advantage\", y.lim = c(0,100), x.label = \"Dem Margin of Victory\", y.label = \"Dem Vote Share in next election\")"},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","text":"demonstrate synthetic control method using data Abadie \nGardeazabal (2003), studied economic effects conflict, using \nterrorist conflict Basque Country case study. paper used \ncombination Spanish regions construct synthetic Basque Country\nresembling many relevant economic characteristics Basque Country \nonset political terrorism 1970s.","code":""},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"load-the-data","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"13.15 Load the Data","text":"Let’s go ahead load packages data. notice quite\nlot missing data, don’t worry, ’s meant !","code":"\ninstall <- function(package) {\n  if (!require(package, quietly = TRUE, character.only = TRUE)) {\n    install.packages(package, repos = \"http://cran.us.r-project.org\", type = \"binary\")\n    library(package, character.only = TRUE)\n  }\n}\n\n# Load packages\ninstall(\"dplyr\")\ninstall(\"Synth\")\n\n# Load data\ndata(\"basque\")\nglimpse(basque)## Rows: 774\n## Columns: 17\n## $ regionno              <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n## $ regionname            <chr> \"Spain (Espana)\", \"Spain (Es…\n## $ year                  <dbl> 1955, 1956, 1957, 1958, 1959…\n## $ gdpcap                <dbl> 2.354542, 2.480149, 2.603613…\n## $ sec.agriculture       <dbl> NA, NA, NA, NA, NA, NA, 19.5…\n## $ sec.energy            <dbl> NA, NA, NA, NA, NA, NA, 4.71…\n## $ sec.industry          <dbl> NA, NA, NA, NA, NA, NA, 26.4…\n## $ sec.construction      <dbl> NA, NA, NA, NA, NA, NA, 6.27…\n## $ sec.services.venta    <dbl> NA, NA, NA, NA, NA, NA, 36.6…\n## $ sec.services.nonventa <dbl> NA, NA, NA, NA, NA, NA, 6.44…\n## $ school.illit          <dbl> NA, NA, NA, NA, NA, NA, NA, …\n## $ school.prim           <dbl> NA, NA, NA, NA, NA, NA, NA, …\n## $ school.med            <dbl> NA, NA, NA, NA, NA, NA, NA, …\n## $ school.high           <dbl> NA, NA, NA, NA, NA, NA, NA, …\n## $ school.post.high      <dbl> NA, NA, NA, NA, NA, NA, NA, …\n## $ popdens               <dbl> NA, NA, NA, NA, NA, NA, NA, …\n## $ invest                <dbl> NA, NA, NA, NA, NA, NA, NA, …"},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"data-definitions","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"13.16 Data definitions","text":"following definitions variables see :regionno regionname: Numeric identifiers Spanish region,\nnames (character) region.regionno regionname: Numeric identifiers Spanish region,\nnames (character) region.year: year corresponding row data. Spans \n1955-1997.year: year corresponding row data. Spans \n1955-1997.gdpcap: 1960–1969 averages real GDP per-capita measured thousands \n1986 USD.gdpcap: 1960–1969 averages real GDP per-capita measured thousands \n1986 USD.Variables sec. prefix: 1961–1969 average percentage total GDP \nsix different industries. example, first non-missing value \nsec.agriculture = 19.54. means 1961, Agriculture industry\nmade 20% total GDP Spain (note regionname\nvariable = Spain (Espana).Variables sec. prefix: 1961–1969 average percentage total GDP \nsix different industries. example, first non-missing value \nsec.agriculture = 19.54. means 1961, Agriculture industry\nmade 20% total GDP Spain (note regionname\nvariable = Spain (Espana).Variables school. prefix: 1964–1969 averages share \nworking-age population illiterate (school.illit), share \nprimary school education (school.prim), share high school\n(school.med), share high school (school.high), share \nhigh school (school.post.high).Variables school. prefix: 1964–1969 averages share \nworking-age population illiterate (school.illit), share \nprimary school education (school.prim), share high school\n(school.med), share high school (school.high), share \nhigh school (school.post.high).popdens: 1969 population density measured people per square kilometer.popdens: 1969 population density measured people per square kilometer.invest: 1964–1969 averages gross total investment divided total GDP.invest: 1964–1969 averages gross total investment divided total GDP.Running following line code show helpful information “Help” box bottom right R Studio Screen. call documentation. tells outcome variable predictor variables , plus descriptions variable dataset. Please ask us additional questions variable means.","code":"\n?basque"},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"questions-for-you","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"13.16.1 Questions For You","text":"running code reading documentation dataset, answer following questions:years contained dataset?Answer.many Spanish regions contained dataset?Answer.outcome variable called dataset?Answer.possible predictor variables?Answer.","code":""},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"prepare-the-data-for-analysis-using-dataprep","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"13.17 Prepare the data for analysis using dataprep","text":"first step reorganize dataset appropriate format \nsuitable main estimator function synth(). , use \ndataprep() function. see examples details data extraction, run\n?dataprep console. pop helpful page “Help” tab bottom right R Studio screen.code , need tell Synth following:predictor variables (split two groups)predictors: variables non-missing years included analysis\npredictors: variables non-missing years included analysisspecial.predictors: predictor variables missing values require little extra handling\nspecial.predictors: predictor variables missing values require little extra handlingHow want predictor variables aggregated (case, average)time period considering (case, 1964 1969)outcome variable (case, gdpcap)variable(s) identify different regions (regionname) /\nnumbers (regionno).variable denotes time period (year)region treated unit (region 17 AKA Basque Country)regions control units (c(2:16,18))time period want train model (pre-treatment period 1960:1969)time-period outcome data plotted\n(usually treatment, e.g., 1955 1997)Okay, now let’s prepare ’ve just described code.\n’ve added comments code explain exactly ’s happening line.\ncreating “prepared” dataset dataprep.running “unprepared” data dataprep function.\nSynth package requires data specific format synthetic control.Notice code use arguments predictors,\npredictors.op, time.predictors.prior, rest information\npredictor variables specified special.predictors\nlist. functionality designed allow easy handling \nseveral predictors operator (e.g. taking average) \npre-treatment period (case, school investment variables)\nwell additional custom (“special”) predictors varying operators\ntime-periods. example, variables sector production\nshares (e.g. sec.agriculture) available biennial basis\n(1961,1963,…,1969) extracted use code seq(1961,1969,2).\ndetails examples use special.predictors\ncan seen running ?dataprep console.","code":"\ndataprep.out <- dataprep(\n  foo = basque,          # Our analysis data that needs to be prepared\n  predictors = c(        # 1(a). list the variables that we want to use as predictors\n    \"school.illit\",     \n    \"school.prim\",       \n    \"school.med\",\n    \"school.high\",\n    \"school.post.high\",\n    \"invest\"\n  ),\n  predictors.op = \"mean\",               # 2. Tell Synth to take the average of all variables in `predictors` above.\n  time.predictors.prior = 1964:1969,    # 3. Take the average of variables in `predictors` from 1964 to 1969.\n  special.predictors = list(            # 1(b). Additional variables to include as predictors in our model:\n    list(\"gdpcap\", 1960:1969 , \"mean\"), # -    Take the average of `gdcap` from 1960 to 1969\n                                        # -    Take the average of all others for every OTHER year from 1961 to 1969\n    list(\"sec.agriculture\", seq(1961, 1969, 2), \"mean\"),\n    list(\"sec.energy\", seq(1961, 1969, 2), \"mean\"),\n    list(\"sec.industry\", seq(1961, 1969, 2), \"mean\"),\n    list(\"sec.construction\", seq(1961, 1969, 2), \"mean\"),\n    list(\"sec.services.venta\", seq(1961, 1969, 2), \"mean\"),\n    list(\"sec.services.nonventa\", seq(1961, 1969, 2), \"mean\"),\n    list(\"popdens\", 1969, \"mean\")       # -    Take the average of `popdens` only in 1969\n  ),\n  dependent = \"gdpcap\",                 # 4. Specify our outcome variable\n  unit.variable = \"regionno\",           # 5(a). Specify the numeric identifier of each region\n  unit.names.variable = \"regionname\",   # 5(b). Specify the name of each region\n  time.variable = \"year\",               # 6. Specify what variable is our time variable\n  treatment.identifier = 17,            # 7. Specify which region in `regionno` is our treated region\n  controls.identifier = c(2:16, 18),    # 8. Specify which regions in `regionno` should be in our donor pool\n  time.optimize.ssr = 1960:1969,        # 9. Specify what years should make up our pre-treatment time period\n  time.plot = 1955:1997                 # 10. Specify what years to plot our outcome variable for\n)"},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"construct-our-synthetic-control","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"13.18 Construct our Synthetic Control","text":"Now, ’re ready use synth() function create synthetic control\nGDP Basque Country region Spain. described discussion,\nmeans synth() create weights regions\nweighted average regions’ GDP closely match \ntrue GDP Basque Country region.’ll explore model output .","code":"\nsynth.out <- synth(data.prep.obj = dataprep.out, method = \"BFGS\")## \n## X1, X0, Z1, Z0 all come directly from dataprep object.\n## \n## \n## **************** \n##  searching for synthetic control unit  \n##  \n## \n## **************** \n## **************** \n## **************** \n## \n## MSPE (LOSS V): 0.008864605 \n## \n## solution.v:\n##  0.03881798 0.001220442 4.26792e-05 0.0001235262 1.6599e-06 1.76355e-05 0.04072702 0.2396775 0.02234054 0.248494 0.005974697 0.01098894 0.04858995 0.3429834 \n## \n## solution.w:\n##  1.67e-08 4.27e-08 7.43e-08 2.78e-08 2.97e-08 5.545e-07 3.66e-08 4.28e-08 0.8508029 9.23e-08 2.75e-08 4.94e-08 0.1491958 4.13e-08 9.75e-08 1.167e-07"},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"summarizing-our-synthetic-control-with-tables","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"13.19 Summarizing our Synthetic Control with Tables","text":"First, can begin creating summary tables Synthetic Control\nmodel.synth.tables variable now contains four tables help us evaluate\nsynthetic control. first table looks pre-treatment period \ncompares predictor values Basque Country (denoted Treated \ntable) Synthetic Control. Note want values \nTreated Synthetic columns really close together.can see values Treated Synthetic columns pretty\ndifferent variables indicate perhaps synthetic\ncontrol isn’t similar like.Next, can look weights got assigned non-treatment\nregions. can drop regions weight 0 since regions don’t\ncontribute synthetic control !w.weights column can see two regions contribute \nsynthetic control. Cataluna region makes approximately 85% \nsynthetic control Madrid region makes additional 15%. means\n16 regions donor pool, 2 picked create\nsynthetic control!Finally can look weights got assigned predictor\nvariables. can interpreted relative importance \npredictor variables.can see school.med, school.high, school.post.high, \ninvest variables weight 0, means least\nimportant impact creation synthetic control. \nhand, population density 1969 (special.popdens.1969) \nimportant variable.","code":"\nsynth.tables <- synth.tab(dataprep.res = dataprep.out, synth.res = synth.out)\nsynth.tables$tab.pred##                                          Treated Synthetic\n## school.illit                              39.888   256.335\n## school.prim                             1031.742  2730.092\n## school.med                                90.359   223.341\n## school.high                               25.728    63.437\n## school.post.high                          13.480    36.154\n## invest                                    24.647    21.583\n## special.gdpcap.1960.1969                   5.285     5.271\n## special.sec.agriculture.1961.1969          6.844     6.179\n## special.sec.energy.1961.1969               4.106     2.760\n## special.sec.industry.1961.1969            45.082    37.636\n## special.sec.construction.1961.1969         6.150     6.952\n## special.sec.services.venta.1961.1969      33.754    41.104\n## special.sec.services.nonventa.1961.1969    4.072     5.371\n## special.popdens.1969                     246.890   196.287\n##                                         Sample Mean\n## school.illit                                170.786\n## school.prim                                1127.186\n## school.med                                   76.260\n## school.high                                  24.235\n## school.post.high                             13.478\n## invest                                       21.424\n## special.gdpcap.1960.1969                      3.581\n## special.sec.agriculture.1961.1969            21.353\n## special.sec.energy.1961.1969                  5.310\n## special.sec.industry.1961.1969               22.425\n## special.sec.construction.1961.1969            7.276\n## special.sec.services.venta.1961.1969         36.528\n## special.sec.services.nonventa.1961.1969       7.111\n## special.popdens.1969                         99.414\nsynth.tables$tab.w[synth.tables$tab.w$w.weights != 0, ]##    w.weights            unit.names unit.numbers\n## 10     0.851              Cataluna           10\n## 14     0.149 Madrid (Comunidad De)           14\nsynth.tables$tab.v##                                         v.weights\n## school.illit                            0.039    \n## school.prim                             0.001    \n## school.med                              0        \n## school.high                             0        \n## school.post.high                        0        \n## invest                                  0        \n## special.gdpcap.1960.1969                0.041    \n## special.sec.agriculture.1961.1969       0.24     \n## special.sec.energy.1961.1969            0.022    \n## special.sec.industry.1961.1969          0.248    \n## special.sec.construction.1961.1969      0.006    \n## special.sec.services.venta.1961.1969    0.011    \n## special.sec.services.nonventa.1961.1969 0.049    \n## special.popdens.1969                    0.343"},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"summarizing-our-synthetic-control-with-plots","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"13.20 Summarizing our Synthetic Control with Plots","text":"Finally, can plot economic output Basque Contry region \ncompare economic output Synthetic Control. make \nconvincing case large treatment effect, like see two\ntrajectories outcome variable Basque Country Synthetic\nControl unit quite similar prior violent conflict diverge\nsharply violent conflict occurs.Let’s create plot see indicates significant treatment effect.can see economic output units looks super similar \n1975 violent conflict began earnest. point ,\neconomic output Basque Country drops significantly. \nindicate violent conflict fairly large negative impact \neconomic output Basque Country region.Another way can visualize creating plot, instead\nshowing two lines outcome Basque Country region \noutcome Synthetic Control Unit, plot single line \ndifference two lines time period.plot conveys information . , economic output\nBasque Country region drops well economic output \nSynthetic Control unit violent conflict begins 1975. words\nviolent conflict lowers economic output.","code":"\npath.plot(\n  synth.res = synth.out,\n  dataprep.res = dataprep.out,\n  Ylab = \"real per-capita GDP (1986 USD, thousand)\",\n  Xlab = \"year\",\n  Ylim = c(0, 12),\n  Legend = c(\"Basque country\", \"synthetic Basque country\"),\n  Legend.position = \"bottomright\"\n)\nabline(a = NULL, b = NULL, h = NULL, v = 1975, col = \"red\")\ngaps.plot(\n  synth.res = synth.out,\n  dataprep.res = dataprep.out,\n  Ylab = \"gap in real per-capita GDP (1986 USD, thousand)\",\n  Xlab = \"year\",\n  Ylim = c(-1.5, 1.5),\n  Main = NA\n)\nabline(a = NULL, b = NULL, h = NULL, v = 1975, col = \"red\")"},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"summary","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"13.21 Summary","text":"tutorial, ’ve walked prepare data Synthetic\nControl method following steps:Prepare data using dataprep() function.Prepare data using dataprep() function.Create Synthetic Control unit using synth function.Create Synthetic Control unit using synth function.Evaluate model tables using synth.tab function.Evaluate model tables using synth.tab function.Plot outcomes treated unit synthetic control unit \npath.plot gaps.plot functions.Plot outcomes treated unit synthetic control unit \npath.plot gaps.plot functions.","code":""},{"path":"due-dates.html","id":"due-dates","chapter":"Due dates","heading":"Due dates","text":"Th 31 Aug, 5pm. Submit HW 1.Th 7 Sep, 5pm. Peer reviews HW 1.Th 14 Sep, 5pm. Submit HW 2.Th 21 Sep, 5pm. Peer reviews HW 2.Th 28 Sep, 5pm. Submit HW 3.Th 5 Oct, 5pm. Peer reviews HW 3.Th 19 Oct, 5pm. Submit HW 4.Th 26 Oct, 5pm. Peer reviews HW 4.Sun 5 Nov, 5pm. Submit HW 5.Th 9 Nov, 5pm. Peer reviews HW 5.Th 16 Nov, 5pm. Submit HW 6.note: peer reviews homeworkT 21 Nov, 5pm. Submit project writeup.W 29 Nov. Present project lab.","code":""},{"path":"final-project.html","id":"final-project","chapter":"Final Project","heading":"Final Project","text":"final project opportunity engage published research paper asks causal question. choose one paper set prepared. paper, carry two tasks.","code":""},{"path":"final-project.html","id":"summarize-what-the-authors-have-done","chapter":"Final Project","heading":"1. Summarize what the authors have done","text":"Choose one causal estimand paper. estimand,Define estimand using potential outcomesPresent identification assumptions link quantity observable dataDiscuss estimator authors use estimate target quantityIn many papers, three steps stated English may involve ambiguity. may certain quantity authors wanted estimate, exactly assumed. settings, task choose one interpretation authors done make interpretation precise.","code":""},{"path":"final-project.html","id":"propose-a-new-quantity-to-estimate","chapter":"Final Project","heading":"2. Propose a new quantity to estimate","text":"second part, propose new causal estimand. collect new data conduct new analysis estimate new quantity, quantity ? part assignment, can write though unlimited resources collect amount kind data want.Define new causal estimandPresent identification assumptions link quantity observable dataDiscuss estimator use estimate target quantityThere one restriction: part 2, proposed analysis involve randomized treatment. experiments terrific ways conducting research, identification estimation assumptions randomized experiments can trivial. order practice skills learned class, want focus non-randomized setting.","code":""},{"path":"final-project.html","id":"format-of-the-final-project","chapter":"Final Project","heading":"Format of the final project","text":"","code":""},{"path":"final-project.html","id":"working-in-groups","chapter":"Final Project","heading":"Working in groups","text":"anticipate students carry final project groups 4–5 students discussion section. Near middle semester, circulate form tell us people ’d like work , ’d like us randomize group can meet new people. prefer work alone, just come talk us, can discuss work.","code":""},{"path":"final-project.html","id":"writeup","chapter":"Final Project","heading":"Writeup","text":"Writeup due Nov 21 5pmYour group submit writeup 1,500 2,000 words, typeset using RMarkdown.","code":""},{"path":"final-project.html","id":"presentation","chapter":"Final Project","heading":"Presentation","text":"Presentations Nov 29 discussionEach group make 10-minute presentation, using slides. presentation involve parts (1) (2).","code":""},{"path":"final-project.html","id":"grading","chapter":"Final Project","heading":"Grading","text":"post rubrics Canvas assign final project groups.","code":""},{"path":"final-project.html","id":"published-papers-for-the-project","chapter":"Final Project","heading":"Published papers for the project","text":"group study one following papers:Stuart, E. ., & Green, K. M. (2008). Using full matching estimate causal effects nonexperimental studies: Examining relationship adolescent marijuana use adult outcomes. Developmental Psychology, 44(2), 395.\npaper example matching\npaper example matchingHalloran, M. E., & Hudgens, M. G. (2018). Estimating population effects vaccination using large, routinely collected data. Statistics Medicine, 37(2), 294-301.\npaper example addressing interference\npaper example addressing interferenceEggers, . C., & Hainmueller, J. (2009). MPs sale? Returns office postwar British politics. American Political Science Review, 103(4), 513-533.\npaper example regression discontinuity\npaper example regression discontinuityAcharya, ., Blackwell, M., & Sen, M. (2016). political legacy American slavery. Journal Politics, 78(3), 621-641.\npaper example instrumental variables\npaper example instrumental variables","code":""},{"path":"problem-set-1.-definitions.html","id":"problem-set-1.-definitions","chapter":"Problem Set 1. Definitions","heading":"Problem Set 1. Definitions","text":"Relevant material covered Aug 24. Problem set due Aug 31.Welcome problem set! homework practice conceptual notation ideas descriptive causal inference.complete problem set, Download .Rmd complete homework. Omit name can anonymous peer feedback. Compile PDF submit PDF Canvas.","code":""},{"path":"problem-set-1.-definitions.html","id":"practice-with-potential-outcomes","chapter":"Problem Set 1. Definitions","heading":"1. Practice with potential outcomes","text":"Jose says coming Cornell caused discover statistics, became major! says gone NYU, stuck biology.","code":""},{"path":"problem-set-1.-definitions.html","id":"points","chapter":"Problem Set 1. Definitions","heading":"1.1 (7 points)","text":"Jose’s claim, treatment?Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-1","chapter":"Problem Set 1. Definitions","heading":"1.2 (7 points)","text":"Using mathematical notation discussed class, define two potential outcomes Jose referringAnswer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-2","chapter":"Problem Set 1. Definitions","heading":"1.3 (7 points)","text":"sentence two, say Fundamental Problem Causal Inference applies Jose’s claim.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-3","chapter":"Problem Set 1. Definitions","heading":"1.4 (7 points)","text":"Using conditional expectations probabilities, write following math: probability majoring statistics higher among students attend Cornell among students attend NYU.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-4","chapter":"Problem Set 1. Definitions","heading":"1.5 (7 points)","text":"Give one reason average causal effect attending Cornell versus NYU (quantity 1.2, averaged students) might different average descriptive difference (1.4) rates majoring statistics.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"a-sailing-class","chapter":"Problem Set 1. Definitions","heading":"2. A sailing class","text":"looking sailing class Cornell Wellness! claim , tell us whether claim causal descriptive.","code":""},{"path":"problem-set-1.-definitions.html","id":"points-5","chapter":"Problem Set 1. Definitions","heading":"2.1 (5 points)","text":"Last year, survey students take class. proportion reporting felt prepared sail Cayuga Lake higher among took class.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-6","chapter":"Problem Set 1. Definitions","heading":"2.2 (5 points)","text":"Last year, survey students class. proportion reporting felt prepared sail Cayuga Lake higher survey taken class.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-7","chapter":"Problem Set 1. Definitions","heading":"2.3 (5 points)","text":"average, students class emerged prepared sail without class.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"session-info","chapter":"Problem Set 1. Definitions","heading":"Session info","text":"chunk record information R session, useful debugging issues homework assignments contain code.","code":"\nsessionInfo()## R version 4.3.1 (2023-06-16)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Ventura 13.6.2\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \n## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n## \n## locale:\n## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n## \n## time zone: America/New_York\n## tzcode source: internal\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets \n## [6] methods   base     \n## \n## loaded via a namespace (and not attached):\n##  [1] digest_0.6.33     R6_2.5.1          bookdown_0.36    \n##  [4] fastmap_1.1.1     xfun_0.40         cachem_1.0.8     \n##  [7] knitr_1.44        memoise_2.0.1     htmltools_0.5.6.1\n## [10] rmarkdown_2.25    xml2_1.3.5        cli_3.6.1        \n## [13] downlit_0.4.3     sass_0.4.7        withr_2.5.1      \n## [16] jquerylib_0.1.4   compiler_4.3.1    rstudioapi_0.15.0\n## [19] tools_4.3.1       evaluate_0.22     bslib_0.5.1      \n## [22] yaml_2.3.7        fs_1.6.3          jsonlite_1.8.7   \n## [25] rlang_1.1.1"},{"path":"problem-set-2.-experiments.html","id":"problem-set-2.-experiments","chapter":"Problem Set 2. Experiments","heading":"Problem Set 2. Experiments","text":"Relevant material covered Sep 7. Problem set due Sep 14.complete problem set, Download .Rmd complete homework. Omit name can anonymous peer feedback. Compile PDF submit PDF Canvas.problem set based :Bertrand, M & Mullainathan, S. 2004. “Emily Greg Employable Lakisha Jamal? Field Experiment Labor Market Discrimination.” American Economic Review 94(4):991–1013.’s heads-hard problem setfor , reading social science paper hardfor , mathematical statistics hardfor , R coding hardFor almost one three easy.want support succeed! Text format help .","code":""},{"path":"problem-set-2.-experiments.html","id":"conceptual-questions-about-the-study-design","chapter":"Problem Set 2. Experiments","heading":"1. Conceptual questions about the study design","text":"Read first 10 pages paper (end section 2). paper,unit analysis resume submitted job openingthe treatment name top resumethe outcome whether employer called emailed back interview","code":""},{"path":"problem-set-2.-experiments.html","id":"points-fundamental-problem","chapter":"Problem Set 2. Experiments","heading":"1.1. (5 points) Fundamental Problem","text":"One submitted resume name “Emily Baker.” yielded callback. resume name “Lakisha Washington.” Explain Fundamental Problem Causal Inference applies case (1–2 sentences).","code":""},{"path":"problem-set-2.-experiments.html","id":"points-exchangeability","chapter":"Problem Set 2. Experiments","heading":"1.2. (5 points) Exchangeability","text":"sentence, state exchangeability means study. concreteness, question may suppose names study “Emily Baker” “Lakisha Washington.” sure explicitly state treatment potential outcomes.","code":""},{"path":"problem-set-2.-experiments.html","id":"points-something-you-liked","chapter":"Problem Set 2. Experiments","heading":"1.3. (10 points) Something you liked","text":"State something concrete appreciate study design, randomization.","code":""},{"path":"problem-set-2.-experiments.html","id":"analyzing-the-experimental-data","chapter":"Problem Set 2. Experiments","heading":"2. Analyzing the experimental data","text":"Load packages code use.Download study’s data OpenICPSR: https://www.openicpsr.org/openicpsr/project/116023/version/V1/view. require creating account agreeing terms using data ethically. Put data folder computer .Rmd located. Read data R using read_dta.error, might need set working directory first. tells R look data files. top RStudio, click Session -> Set Working Directory -> Source File Location.now see d Global Environment top right RStudio.use four variables:2.1–2.4, think race treatment. 2.5–2.6, think firstname treatment.Restrict variables using select().new R, just happened:created new object d_selectedused assignment operator <- put something objectwe started data object dwe used pipe operator %>% hand d new actionthe action select() selected variables interestWe often analyze data starting data object handing series actions connected pipe %>%","code":"\nlibrary(tidyverse)\nlibrary(haven)\nd <- read_dta(\"lakisha_aer.dta\")\nd_selected <- d %>%\n  select(call, firstname, race, sex)"},{"path":"problem-set-2.-experiments.html","id":"points-point-estimates-of-expected-potential-outcomes","chapter":"Problem Set 2. Experiments","heading":"2.1. (5 points) Point estimates of expected potential outcomes","text":"top Table 1 reports callback rates: 9.65% white names 6.45% Black names. Reproduce numbers. , take code add group_by() action d_selected summarize.’s reference introduces group_by summarize.","code":"\nd_summarized <- d_selected %>%\n  summarize(callback_rate = mean(call),\n            number_cases = n()) %>%\n  print()## # A tibble: 1 × 2\n##   callback_rate number_cases\n##           <dbl>        <int>\n## 1        0.0805         4870"},{"path":"problem-set-2.-experiments.html","id":"points-inference-for-expected-potential-outcomes","chapter":"Problem Set 2. Experiments","heading":"2.2. (5 points) Inference for expected potential outcomes","text":"Use mutate() (see reference page) create new columns containing standard error estimate well lower upper limits 95% confidence intervals.make easier, quick math review R functions can use.Standard error math. Let \\(Y^\\) Bernoulli random variable, taking value 1 random resume name \\(\\) yields callback 0 otherwise. Let \\(\\pi^= P(Y^=1)\\) probability callback. statistics, know variance \\(V(Y^) = \\pi^(1-\\pi^)\\). estimated average: \\(\\hat\\pi^= \\frac{1}{n_a}\\sum_{:A_i=} Y_i^\\). many times many hypothetical samples, always get estimate. fact, estimate sampling variance \\(V(\\hat\\pi^) = \\frac{\\pi^(1-\\pi^)}{n_a}\\). know \\(\\hat\\pi^\\) mean \\(n_a\\) independent identically distributed random variables \\(Y^\\). standard error square root sampling variance: \\(SE(\\hat\\pi^) = \\sqrt\\frac{\\pi^(1-\\pi^)}{n_a}\\). can estimate standard error plugging estimate \\(\\hat\\pi^\\) true unknown \\(\\pi^\\) wherever appears.Standard error code. translated standard error formula code . function accepts estimated probability p sample size n returns estimated standard error. can use se_binary() function code within mutate() just like mean() used within summarize() start problem set.Sampling distribution math. \\(\\hat\\pi^\\) sample mean, know something sampling distribution: limit sample size grows infinity, across hypothetical repeated samples distribution \\(\\hat\\pi^\\) estimates becomes Normal. Central Limit Theorem! Across repeated samples, middle 95% estimates fall within known range: \\(\\pi^\\pm \\Phi^{-1}(.975) \\times SE(\\hat\\pi^)\\), \\(\\Phi^{-1}()\\) inverse cumulative distribution function standard Normal distribution. might previously learned \\(\\Phi^{-1}(.975) \\approx 1.96\\), might familiar number 1.96.Sampling distribution graph.Confidence interval math. get 95% confidence interval plugging estimates \\(\\hat\\pi^\\) \\(\\widehat{SE}(\\hat\\pi^)\\) limits . interval centered estimate \\(\\hat\\pi^\\) nice property: repeatedly made confidence interval procedure using hypothetical samples population, interval contain unknown true parameter \\(\\pi^\\) 95% time.Confidence interval code. translated confidence interval formula code . functions accept estimate standard error return lower upper bounds (respectively) 95% confidence interval assumes Normal sampling distribution. can use functions code within mutate() just like mean() used within summarize() start problem set.","code":"\nse_binary <- function(p, n) {\n  se <- sqrt( p * (1 - p) / n )\n  return(se)\n}\nci_lower <- function(estimate, standard_error) {\n  estimate - qnorm(.975) * standard_error\n}\nci_upper <- function(estimate, standard_error) {\n  estimate + qnorm(.975) * standard_error\n}"},{"path":"problem-set-2.-experiments.html","id":"points-interpret-your-confidence-interval","chapter":"Problem Set 2. Experiments","heading":"2.3. (5 points) Interpret your confidence interval","text":"words, interpret confidence intervals. sure discuss property hypothetical repeated samples, sure frame answer using numbers variables actual experiment analyzing.","code":""},{"path":"problem-set-2.-experiments.html","id":"points-visualize-expected-potential-outcomes","chapter":"Problem Set 2. Experiments","heading":"2.4. (5 points) Visualize expected potential outcomes","text":"Using ggplot(), visualize estimated callback rate race. Use geom_point() point estimates geom_errorbar() confidence intervals, race x axis estimates y axis. Label axes using full words.never used ggplot, see Ch 3 R Data Science Hadley Wickham.","code":""},{"path":"problem-set-2.-experiments.html","id":"points-estimate-and-visualize-by-firstname","chapter":"Problem Set 2. Experiments","heading":"2.5. (5 points) Estimate and visualize by firstname","text":"distinct first names yield distinct effects? Repeat 2.2–2.4, now create estimates grouped race, sex, firstname. Visualize point estimates confidence intervals.One way visualize placing first names \\(x\\)-axis using facet_wrap() layer facet race sex.strategy visualize fine, long shows estimates firstname indicates race sex signaled firstname","code":"\nyour_ggplot +\n  facet_wrap(~ race + sex,\n             scales = \"free_x\", \n             nrow = 1)"},{"path":"problem-set-2.-experiments.html","id":"points-interpret","chapter":"Problem Set 2. Experiments","heading":"2.6. (5 points) Interpret","text":"Within race sex, first names effect. Suppose true differences (due sampling variability). tell importance researcher decisions names use treatments?","code":""},{"path":"problem-set-3.-dags..html","id":"problem-set-3.-dags.","chapter":"Problem Set 3. DAGs.","heading":"Problem Set 3. DAGs.","text":"Relevant material covered Sep 21. Problem set due Sep 28.complete problem set, copy code .Rmd complete homework. Omit name can anonymous peer feedback. Compile PDF submit PDF Canvas.Note: problem set , alternatively may complete homework hand. welcome draw DAGs hand instead producing code. , scan take picture document.","code":""},{"path":"problem-set-3.-dags..html","id":"true-or-false","chapter":"Problem Set 3. DAGs.","heading":"1. True or False","text":"1.1–1.5, answer True False: \\(X\\) sufficient adjustment set identify causal effect \\(\\) \\(Y\\). Explain one sentence. False, state backdoor path unblocked conditional \\(X\\). path linear series nodes connected arrows; see examples 1.6 1.7.","code":""},{"path":"problem-set-3.-dags..html","id":"points-8","chapter":"Problem Set 3. DAGs.","heading":"1.6 (3 points)","text":"True False? Conditioning \\(X\\) blocks path: \\(\\leftarrow B \\leftarrow X \\rightarrow C \\rightarrow Y\\)","code":""},{"path":"problem-set-3.-dags..html","id":"points-9","chapter":"Problem Set 3. DAGs.","heading":"1.7 (3 points)","text":"True False? Conditioning \\(X\\) blocks path: \\(\\leftarrow B \\rightarrow X \\leftarrow C \\rightarrow Y\\)","code":""},{"path":"problem-set-3.-dags..html","id":"draw-a-dag-10-points","chapter":"Problem Set 3. DAGs.","heading":"2. Draw a DAG (10 points)","text":"researcher comes proposal: identify causal effect \\(\\) \\(Y\\) adjusting variable \\(X\\) predicts \\(\\) also predicts \\(Y\\). propose machine learning can thus solve causal identification us.researcher wrong. Show . Draw DAG whichthe effect \\(\\) \\(Y\\) unconfoundeda variable \\(X\\) statistically associated \\(\\)variable \\(X\\) statistically associated \\(Y\\)one need adjust \\(X\\) identify causal effect","code":""},{"path":"problem-set-3.-dags..html","id":"using-dags-in-a-new-context","chapter":"Problem Set 3. DAGs.","heading":"3. Using DAGs in a new context","text":"DAGs just useful causal inference: can useful whenever need know whether one variable statistically independent another. true, example, drawing inference population sample.researcher uses opt-online web survey draw inference support President Biden. ask respondents: ``approve President Biden’s performance office?’’ answer choices Yes/. researcher also gathers data two demographic characteristics: whether respondent completed college current employment. write:sample representative. Suppose every person population, \\(S\\) denotes whether included sample. \\(S\\) related approval President Biden (\\(Y\\)).However, believe sample representative look set people take value along college completion employment, finished college currently employed. variables \\(X_1,X_2\\), believe independence statement: \\(S\\) independent \\(Y\\) given \\(X_1,X_2\\). therefore get population estimates procedure several steps: use sample estimate mean outcome \\(E(Y\\mid \\vec{X} = \\vec{x})\\) stratum, use Census data estimate size stratum \\(P(\\vec{X} = \\vec{x})\\) population, estimate \\(E(Y) = \\sum_{\\vec{x}}E(Y\\mid \\vec{X} = \\vec{x})P(\\vec{X} = \\vec{x})\\).researcher’s reasoning common strategy known post-stratification. question formalizing set conditions researcher right wrong.begin, want emphasize one aspect researcher’s assumption different exchangeability assumption causal inference.causal claims, assume conditional exchangeability: \\(\\) independent \\(Y^\\) given \\(\\vec{X}\\)\ninvolves potential outcome \\(Y^\\)\nholds unblocked paths \\(\\) \\(Y\\) causal paths\ninvolves potential outcome \\(Y^\\)holds unblocked paths \\(\\) \\(Y\\) causal pathsfor sample--population inference, assume conditionally independent sampling \\(S\\) independent \\(Y\\) given \\(\\vec{X}\\)\ninvolves factual outcome \\(Y\\); intervention \nholds unblocked paths \\(S\\) \\(Y\\)\ninvolves factual outcome \\(Y\\); intervention hereholds unblocked paths \\(S\\) \\(Y\\)Although assumption different, principles DAGs still relevant.","code":""},{"path":"problem-set-3.-dags..html","id":"points-10","chapter":"Problem Set 3. DAGs.","heading":"3.1. (5 points)","text":"Draw DAG researcher’s claim valid. Use \\(S,Y,X_1,X_2\\).","code":""},{"path":"problem-set-3.-dags..html","id":"points-11","chapter":"Problem Set 3. DAGs.","heading":"3.2. (2 points)","text":"sentence two, explain DAG 3.1 researcher. Tell us words meant edge DAG.","code":""},{"path":"problem-set-3.-dags..html","id":"points-12","chapter":"Problem Set 3. DAGs.","heading":"3.3. (5 points)","text":"Draw DAG showing counterexample researcher’s claim invalid.","code":""},{"path":"problem-set-3.-dags..html","id":"points-13","chapter":"Problem Set 3. DAGs.","heading":"3.4 (2 points)","text":"sentence two, explain DAG 3.3 researcher. Tell us particularly path creates statistical dependence \\(S\\) \\(Y\\).","code":""},{"path":"problem-set-4.-statistical-modeling.html","id":"problem-set-4.-statistical-modeling","chapter":"Problem Set 4. Statistical modeling","heading":"Problem Set 4. Statistical modeling","text":"Relevant material covered Oct 5. Problem set due Oct 19.complete problem set, Download .Rmd complete homework. Omit name can anonymous peer feedback. Compile PDF submit PDF Canvas.learning goals completing problem set areexplain role statistical modeling\nrespect causal claims\nrespect data sparsity\nrespect causal claimswith respect data sparsityestimate average treatment effects \nexact matching (setting confounders)\nlearning outcome model\nlearning treatment model\nmatching method choosing\nexact matching (setting confounders)learning outcome modellearning treatment modela matching method choosingThe reason practicing many statistical modeling estimators can see ideas class apply estimators—future estimators encounter part class!problem set uses data following paper:Dehejia, R. H. Wahba, S. 1999. Causal Effects Nonexperimental Studies: Reevaluating Evaluation Training Programs. Journal American Statistical Association 94(448):1053–1062.paper compares methods observational causal inference recover average causal effect already known randomized experiment. need read paper; just use study’s data illustration.following lines load data R.learn data, type ?lalonde R console.","code":"\nlibrary(tidyverse)\nlibrary(MatchIt)\ndata(\"lalonde\")"},{"path":"problem-set-4.-statistical-modeling.html","id":"conceptual-questions","chapter":"Problem Set 4. Statistical modeling","heading":"1. Conceptual questions","text":"","code":""},{"path":"problem-set-4.-statistical-modeling.html","id":"points-statistical-modeling-and-causal-claims","chapter":"Problem Set 4. Statistical modeling","heading":"1.1. (5 points) Statistical modeling and causal claims","text":"Imagine someone taken class tells don’t need DAGs causal assumptions know really good matching method. 3 sentences, explain causal assumptions necessary matching yield causal conclusions.","code":""},{"path":"problem-set-4.-statistical-modeling.html","id":"nonparametric-estimation","chapter":"Problem Set 4. Statistical modeling","heading":"2. Nonparametric estimation","text":"goal estimate effect job training treat future earnings re78 (real earnings 1978), among received job training (average treatment effect treated, ATT).","code":""},{"path":"problem-set-4.-statistical-modeling.html","id":"points-exact-matching-with-low-dimensional-confounding","chapter":"Problem Set 4. Statistical modeling","heading":"2.1. (4 points) Exact matching with low-dimensional confounding","text":"part, assume three variables comprise sufficient adjustment set: race, married, nodegree. Use matchit argument method = \"exact\" conduct exact matching, matches two units identical along race, married, nodegree.Note: calling exact matching. thing previously called nonparametric estimation: make subgroups units identical along confounders, estimate treatment effect within subgroups, aggregate sample. using language matching parallel comes Question 4.many control units matched? many treated units?","code":""},{"path":"problem-set-4.-statistical-modeling.html","id":"points-effect-estimate","chapter":"Problem Set 4. Statistical modeling","heading":"2.2. (4 points) Effect estimate","text":"Estimate linear regression model using match data 2.1. Include treatment confounders 2.1 linear, additive specification. Weight weights matching.estimated effect job training earnings?","code":""},{"path":"problem-set-4.-statistical-modeling.html","id":"points-exact-matching-with-high-dimensional-confounding","chapter":"Problem Set 4. Statistical modeling","heading":"2.3. (4 points) Exact matching with high-dimensional confounding","text":"Now suppose adjustment set needs also include 1974 earnings, re74. adjustment set part race, married, nodegree, re74. Repeat exact matching .many control units matched? many treated units?","code":""},{"path":"problem-set-4.-statistical-modeling.html","id":"points-examining-matched-units","chapter":"Problem Set 4. Statistical modeling","heading":"2.4. (4 points) Examining matched units","text":"Look re74 values full data among matched units (need print output).\nExplain happened: different 1974 earnings matched vs unmatched cases?one way :Using function summary, look descriptive statistics re74 values full data.Using function summary, look descriptive statistics re74 values matched data. can get matched data using match.data function.can learn use summary function look descriptive statistics R data .notice? different values re74 full data versus matched data? Explain happened happened.","code":""},{"path":"problem-set-4.-statistical-modeling.html","id":"points-drawbacks-of-exact-matching","chapter":"Problem Set 4. Statistical modeling","heading":"2.5. (4 points) Drawbacks of exact matching","text":"Briefly interpret result 2.4: drawback using exact matching setting?","code":""},{"path":"problem-set-4.-statistical-modeling.html","id":"parametric-estimation","chapter":"Problem Set 4. Statistical modeling","heading":"3. Parametric estimation","text":"","code":""},{"path":"problem-set-4.-statistical-modeling.html","id":"points-outcome-modeling","chapter":"Problem Set 4. Statistical modeling","heading":"3.1. (5 points) Outcome modeling","text":"code , use lm() estimate Ordinary Least Squares regression future earnings re78 treatment treat, interacted confounders: race, married, nodegree, re74.Use model estimate average treatment effect among treated., shouldCreate two data frames\nfirst contain treated individuals (factual treatment 1)\nsecond contain treated individuals, treat set value 0\nfirst contain treated individuals (factual treatment 1)second contain treated individuals, treat set value 0Using model , predict expected outcomes two data frames created step 1.Report average treatment effect among treated.","code":"\noutcome_model <- lm(re78 ~ treat * (race + married + nodegree + re74),\n                    data = lalonde)"},{"path":"problem-set-4.-statistical-modeling.html","id":"points-treatment-modeling-creating-weights","chapter":"Problem Set 4. Statistical modeling","heading":"3.2. (5 points) Treatment modeling: Creating weights","text":"Note: part much help us. read provided understand, small part end. maximize learning-value--workload ratio problem.Using glm() , estimate probability treatment given confounders., using code , wepredict probability treat = 1generate propensity score unitcreate weight estimating Average Treatment Effect Treated, formula\\[w_i = \\frac{P(= 1\\mid \\vec{L} = \\vec\\ell_i)}{P(= a_i\\mid \\vec{L} = \\vec\\ell_i)}\\]Note: treated units, weight 1. untreated units, value varies.many treated units -heavily-weighted untreated unit represent? answer , want determine maximum weight amongst untreated individuals with_weight.","code":"\ntreatment_model <- glm(treat ~ race + married + nodegree + re74,\n                       data = lalonde,\n                       family = binomial)\nwith_weight <- lalonde %>%\n  # Create the propensity score\n  mutate(p_a_1 = predict(treatment_model, type = \"response\"),\n         pscore = case_when(treat == 1 ~ p_a_1,\n                            treat == 0 ~ 1 - p_a_1),\n         weight = p_a_1 / pscore)"},{"path":"problem-set-4.-statistical-modeling.html","id":"points-treatment-modeling-estimating-outcomes","chapter":"Problem Set 4. Statistical modeling","heading":"3.3. (5 points) Treatment modeling: Estimating outcomes","text":"Using with_weight object, take weighted means observed outcomes re78 weighted weight estimate average outcome treated units, weighted average outcome control units (weighted comparable treated units).Hint: want take weighted mean, grouped treatment status.","code":""},{"path":"problem-set-4.-statistical-modeling.html","id":"matching-without-requiring-exact-matches","chapter":"Problem Set 4. Statistical modeling","heading":"4. Matching without requiring exact matches","text":"hope class prepared learn new causal estimators, apply R, explain done. question chance practice! class discussed many matching approaches. question, choose approach. many correct answers, evaluated clarity code explanations.Task: Using matchit, conduct matching estimate ATT treat treatment sufficient adjustment set race, married, nodegree, re74.Use matchit, setting method, distance, arguments values choosing. requirements \nformula = treat ~ race + married + nodegree + re74\nestimand = \"ATT\"\nformula = treat ~ race + married + nodegree + re74estimand = \"ATT\"Create matched dataset using match.data()Estimate linear regression model using lm() formula re78 ~ treat + race + married + nodegree + re74 using matched data, weighted weights produced match.data().","code":""},{"path":"problem-set-4.-statistical-modeling.html","id":"points-conduct-the-matching","chapter":"Problem Set 4. Statistical modeling","heading":"4.1. (4 points) Conduct the matching","text":"space conduct matching. expect part R code chunk.","code":""},{"path":"problem-set-4.-statistical-modeling.html","id":"points-explain-your-choices","chapter":"Problem Set 4. Statistical modeling","heading":"4.2. (2 points) Explain your choices","text":"sentences, tell us matching approach chosen.","code":""},{"path":"problem-set-4.-statistical-modeling.html","id":"points-how-many-units-did-you-keep","chapter":"Problem Set 4. Statistical modeling","heading":"4.3. (2 points) How many units did you keep?","text":"Report number treated control units original data, many kept matching procedure.","code":""},{"path":"problem-set-4.-statistical-modeling.html","id":"points-report-your-causal-estimate","chapter":"Problem Set 4. Statistical modeling","heading":"4.4. (2 points) Report your causal estimate","text":"estimate average treatment effect treated? coefficient treat linear regression fit matched data.","code":""},{"path":"problem-set-5.-iv-rd.html","id":"problem-set-5.-iv-rd","chapter":"Problem Set 5. IV + RD","heading":"Problem Set 5. IV + RD","text":"Relevant material covered Oct 26. Problem set due Nov 2.complete problem set, feel free Download .Rmd. Omit name can anonymous peer feedback. Submit PDF Canvas.learning goals completing problem set engage conceptual assumptions instrumental variables regression discontinuity.","code":""},{"path":"problem-set-5.-iv-rd.html","id":"points-instrumental-variables-in-experiments","chapter":"Problem Set 5. IV + RD","heading":"1. (20 points) Instrumental variables in experiments","text":"Suppose elementary school principal. randomize students new program receive extra tutoring -site location evenings. randomize students -tutoring condition.many cases, students’ treatment assignments \\(Z\\) determines actual treatments \\(\\) (\\(Z = 1\\) \\(= 1\\), \\(Z = 0\\) \\(= 0\\)). difficulties:parents students work evenings can’t drive children tutoring (\\(U\\)). matter value \\(Z\\), children receive tutoring (\\(= 0\\)).parents students make huge fuss (\\(U\\)) regardless value \\(Z\\), parents ensure children receive tutoring (\\(= 1\\)).Answer following sentence .(3 points) intent treat effect?(3 points) always-takers?(3 points) never-takers?(3 points) compliers?(3 points) Although discussed , describe someone defier.(5 points) assumption made credible randomization \\(Z\\)?","code":""},{"path":"problem-set-5.-iv-rd.html","id":"points-iv-in-observational-studies","chapter":"Problem Set 5. IV + RD","heading":"2. (10 points) IV in observational studies","text":"Much water supply state California comes snowmelt Sierra Nevada Mountains. Two economists excited notice years much larger snowpacks others—instrument!Economist 1Economist 2The first economist argues random differences Sierra snowpack create random fluctuations agricultural productivity, thereby providing instrumental variable effect agricultural productivity state’s GDP.second economist argues random difference Sierra snowpack create random fluctuations quality skiing Mammoth Mountain Sierra resorts, thereby providing instrumental variable effect ski resort productivity state’s GDP.economists argue instruments valid snowpack randomly assigned. Can economists right instrument valid? ?","code":""},{"path":"problem-set-5.-iv-rd.html","id":"points-regression-discontinuity","chapter":"Problem Set 5. IV + RD","heading":"3. (20 points) Regression discontinuity","text":"","code":""},{"path":"problem-set-5.-iv-rd.html","id":"points-a-local-estimand","chapter":"Problem Set 5. IV + RD","heading":"3.1 (5 points) A local estimand","text":"colleague tells ’ve read regression discontinuity designs proven winning one election (greater 50% vote) causes political party better chances next election. district, winner won 70% vote. isn’t regression discontinuity evidence informative districts like ?","code":""},{"path":"problem-set-5.-iv-rd.html","id":"points-examples-of-discontinuity","chapter":"Problem Set 5. IV + RD","heading":"3.2 (5 points) Examples of Discontinuity","text":"Describe example encountered regression discontinuity analysis might used estimate causal effect. Draw causal diagram example.","code":""},{"path":"problem-set-5.-iv-rd.html","id":"points-effect-of-incumbency","chapter":"Problem Set 5. IV + RD","heading":"3.3 (10 points) Effect of incumbency","text":"discussion section, considered two causal effects. First, estimated causal effect incumbency senator re-election. Next, considered causal effect senator election democrat democratic vote share senator election.Using package, give estimate second causal effect. Also, explain results clearly stating quantity estimating plain language also explaining whether conclude causal effect non-zero .","code":"\n### Code from Discussion section to get you started\n\nlibrary(ggplot2)\nlibrary(rddensity)\nlibrary(rdrobust)\nlibrary(rdlocrand)\n\ndata <- read.csv(\"https://raw.githubusercontent.com/rdpackages-replication/CIT_2020_CUP/master/CIT_2020_CUP_senate.csv\")\n\ndem_vote_t1 <- data$demvoteshfor1\ndem_margin_t0 <- data$demmv\n\n# plotting the data\n# Shows average to the left and to the right of the cut-off\nrdplot(y = dem_vote_t1, \n       x =  dem_margin_t0, nbins = c(1000, 1000), \n       p = 0, col.lines = \"red\", \n       col.dots = \"lightgray\", \n       title = \"Incumbency Advantage\", \n       y.lim = c(0,100), \n       x.label = \"Dem Margin of Victory\", \n       y.label = \"Dem Vote Share in next election\")"},{"path":"problem-set-6.-difference-in-difference-synthetic-control.html","id":"problem-set-6.-difference-in-difference-synthetic-control","chapter":"Problem Set 6. Difference in Difference + Synthetic Control","heading":"Problem Set 6. Difference in Difference + Synthetic Control","text":"Relevant material covered Nov 9. Problem set due Nov 16.complete problem set, feel free Download .Rmd. Omit name can anonymous peer feedback. Submit PDF Canvas.learning goals completing problem set engage conceptual assumptions difference difference synthetic control.","code":"\nlibrary(tidyverse)"},{"path":"problem-set-6.-difference-in-difference-synthetic-control.html","id":"points-difference-in-difference","chapter":"Problem Set 6. Difference in Difference + Synthetic Control","heading":"1. (25 points) Difference in Difference","text":"figures , treated group becomes treated time 1 time 2. control group never becomes treated. Figures hypothetical scenarios depict true potential outcomes even outcomes observed actual study.","code":""},{"path":"problem-set-6.-difference-in-difference-synthetic-control.html","id":"points-14","chapter":"Problem Set 6. Difference in Difference + Synthetic Control","heading":"1.1 (5 points)","text":"setting parallel trends assumption hold: , B, neither, ?","code":""},{"path":"problem-set-6.-difference-in-difference-synthetic-control.html","id":"points-15","chapter":"Problem Set 6. Difference in Difference + Synthetic Control","heading":"1.2 (5 points)","text":"actual data analysis, can ever know certain whether Setting Setting B? answer , tell us outcome observed.","code":""},{"path":"problem-set-6.-difference-in-difference-synthetic-control.html","id":"points-16","chapter":"Problem Set 6. Difference in Difference + Synthetic Control","heading":"1.3 (5 points)","text":"researcher comes data , depict observed outcomes. researcher wants run difference difference analysis. , depicted counterfactual outcome researcher know .parallel trends assumption doubtful setting?","code":""},{"path":"problem-set-6.-difference-in-difference-synthetic-control.html","id":"points-17","chapter":"Problem Set 6. Difference in Difference + Synthetic Control","heading":"1.4 (5 points)","text":"researcher interested causal effect minimum wage increase employment. plan study U.S., interested time minimum wage rose simultaneously every place U.S. won’t difference difference design work researcher’s question?","code":""},{"path":"problem-set-6.-difference-in-difference-synthetic-control.html","id":"points-18","chapter":"Problem Set 6. Difference in Difference + Synthetic Control","heading":"1.5 (5 points)","text":"Propose another design researcher use answer question (1.4), may involve data outside U.S. Answer question 3 sentences. Many answers possible.","code":""},{"path":"problem-set-6.-difference-in-difference-synthetic-control.html","id":"points-synthetic-control","chapter":"Problem Set 6. Difference in Difference + Synthetic Control","heading":"2. (25 points) Synthetic Control","text":"discussion, considered paper Abadie Gardeazabal (2003) estimates effect terrorist conflict Basque Country GDP per capita. Using synthetic control, construct synthetic version Basque Country. show selected weights plot gap observed synthetic Basque Country .","code":"##    w.weights                   unit.names unit.numbers\n## 2      0.000                    Andalucia            2\n## 3      0.000                       Aragon            3\n## 4      0.000       Principado De Asturias            4\n## 5      0.000             Baleares (Islas)            5\n## 6      0.000                     Canarias            6\n## 7      0.000                    Cantabria            7\n## 8      0.000              Castilla Y Leon            8\n## 9      0.000           Castilla-La Mancha            9\n## 10     0.851                     Cataluna           10\n## 11     0.000         Comunidad Valenciana           11\n## 12     0.000                  Extremadura           12\n## 13     0.000                      Galicia           13\n## 14     0.149        Madrid (Comunidad De)           14\n## 15     0.000           Murcia (Region de)           15\n## 16     0.000 Navarra (Comunidad Foral De)           16\n## 18     0.000                   Rioja (La)           18"},{"path":"problem-set-6.-difference-in-difference-synthetic-control.html","id":"points-motivation","chapter":"Problem Set 6. Difference in Difference + Synthetic Control","heading":"2.1 (10 points) Motivation","text":"Instead selecting weights using synthetic control, instead estimated potential outcome Basque Country using regression approach. Specifically, consider data prior 1970 simply regress GDP per capita Basque region onto GDP per capita regions find coefficients \\(\\hat \\beta\\) \n\\[\\widehat{Y^{0}}_{t, Basque} = \\sum_j \\hat \\beta_{j} Y^{0}_{t, j}.\\]\nuse estimated \\(\\hat \\beta\\) predict \\(\\widehat{Y^{0}}_{t, Basque}\\) treatment occurs. weights gap plot shown . haven’t included regions code , don’t need worry .Looking estimated weights sand gap plots, might prefer synthetic control estimate regression based estimate? might prefer regression estimate synthetic control estimate?","code":"##   weights                         name id\n## 1  -0.632                       Aragon  3\n## 2   1.256       Principado De Asturias  4\n## 3  -0.586             Baleares (Islas)  5\n## 4   0.438                    Cantabria  7\n## 5   0.594                     Cataluna 10\n## 6  -0.788         Comunidad Valenciana 11\n## 7   0.155        Madrid (Comunidad De) 14\n## 8  -0.245 Navarra (Comunidad Foral De) 16\n## 9   1.136                   Rioja (La) 18"},{"path":"problem-set-6.-difference-in-difference-synthetic-control.html","id":"points-assessing-fit","chapter":"Problem Set 6. Difference in Difference + Synthetic Control","heading":"2.2 (7.5 points) Assessing fit","text":"Using dataset, suppose wanted estimate causal effect policy implemented Extramadura (another region Spain) 1970. Running synthetic control gives estimate almost -1000 dollars 1990. Looking plots , might skeptical resulting estimate?","code":""},{"path":"problem-set-6.-difference-in-difference-synthetic-control.html","id":"points-hypothesis-testing","chapter":"Problem Set 6. Difference in Difference + Synthetic Control","heading":"2.3 (7.5 points) Hypothesis testing","text":"Suppose used synthetic control regions Spain placebo test. , show hypothetical plot gap observed synthetic values region. difference observed synthetic Basque country shown dark black line others shown gray. Note made data. However, real plot saw, confident non-zero causal effect Basque Country? Explain .","code":""},{"path":"who-we-are.html","id":"who-we-are","chapter":"Who we are","heading":"Who we are","text":"","code":""},{"path":"who-we-are.html","id":"faculty","chapter":"Who we are","heading":"Faculty","text":"enjoy thinking problems goal discover interpretable structure underlies data generating process. includes problems areas causal discovery, graphical models, mixed membership models. many cases, methods tailored high-dimensional setting number variables considered may large compared number observed samples. applied interests vary generally social science related.study causal questions related inequality: people money others, disparities exist across social groups, intervene promote equality. Beyond causal inference, joys mine include hiking, surfing, oatmeal blueberries.","code":""},{"path":"who-we-are.html","id":"teaching-assistants","chapter":"Who we are","heading":"Teaching assistants","text":"’m currently working problems causal inference network interference. think causal inference really cool applications across many different fields. ’m generally interested applications public health, social welfare, social good. free time, enjoy singing, dancing, cooking, watching movies, traveling various theme parks.’m fascinated causal relationships enjoy exploring shape world. interests broadly centered area Computational Social Science, specifically focused can apply computational Machine Learning methods uncover estimate causal relationships. free time enjoy pretty much anything ’s active outdoors particularly enjoy playing tennis, kayaking, fishing (pro tips, let know).","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
