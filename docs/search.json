[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"Cornell STSCI / INFO / ILRST 3900. Causal Inference. Fall 2024.Welcome! Together, learn make causal claims combining data arguments.Taught Y. Samuel Wang, Mayleen Cortez-Rodriguez, Filippo Fiocchi, Shira Mingelgrin. Read us !","code":""},{"path":"index.html","id":"learning-objectives","chapter":"Welcome","heading":"Learning objectives","text":"result participating course, students able todefine counterfactuals outcomes hypothetical interventionsidentify counterfactuals causal assumptions presented graphsestimate counterfactual outcomes pairing assumptions statistical evidence","code":""},{"path":"index.html","id":"is-this-course-for-me","chapter":"Welcome","heading":"Is this course for me?","text":"course designed upper-division undergraduate students. assume familiarity introductory statistics course level STSCI 2110, PAM 2100, PSYCH 2500, SOC 3010, ECON 3110, similar courses.Cornell student? welcome follow along site.","code":""},{"path":"index.html","id":"readings","chapter":"Welcome","heading":"Readings","text":"Especially beginning, course draws heavily onHernán, M.., J.M. Robins. 2020. Causal Inference: ? Boca Raton: Chapman & Hall / CRC.grateful authors excellent text.","code":""},{"path":"index.html","id":"organization-of-the-site","chapter":"Welcome","heading":"Organization of the site","text":"course module left panel span several lectures. Within module, right panel help navigate. build site course semester, uploading lecture slides go. tells bit teaching team.","code":""},{"path":"index.html","id":"previous-iterations-of-the-course","chapter":"Welcome","heading":"Previous iterations of the course","text":"access course website Fall 2023 click .","code":""},{"path":"index.html","id":"land-acknowledgment","chapter":"Welcome","heading":"Land acknowledgment","text":"recognize university land acknowledgment, well additional emphasis Cornell American Indian Indigenous Studies Program.Cornell University located traditional homelands Gayogo̱hó:nǫɁ (Cayuga Nation). Gayogo̱hó:nǫɁ members Haudenosaunee Confederacy, alliance six sovereign Nations historic contemporary presence land. Confederacy precedes establishment Cornell University, New York state, United States America. acknowledge painful history Gayogo̱hó:nǫɁ dispossession, honor ongoing connection Gayogo̱hó:nǫɁ people, past present, lands waters.land acknowledgment reviewed approved traditional Gayogo̱hó:nǫɁ leadership.addition Gayogo̱hó:nǫɁ land acknowledgment separate , AIISP faculty like emphasize: Cornell’s founding enabled course national genocide sale almost one million acres stolen Indian land Morrill Act 1862. date university neither officially acknowledged complicity theft offered form restitution hundreds Native communities impacted. additional information, see Cornell University Indigenous Dispossession website.","code":""},{"path":"defining-counterfactuals.html","id":"defining-counterfactuals","chapter":"1 Defining counterfactuals","heading":"1 Defining counterfactuals","text":"","code":""},{"path":"defining-counterfactuals.html","id":"observing-versus-intervening","chapter":"1 Defining counterfactuals","heading":"1.1 Observing versus intervening","text":"Aug 27. Slides\nclass, install R Rstudio computer (see slide 14 today’s lecture).Statistical inference observing: observe sample population, can infer population? Causal inference intervening: take sample population intervene change exposure, average outcome result?Today discuss observing, intervening, difference important.","code":""},{"path":"defining-counterfactuals.html","id":"lab-designing-a-study","chapter":"1 Defining counterfactuals","heading":"1.2 Lab: Designing a study","text":"Aug 28In lab, start getting know one another. , discuss hypothetical scenario.researcher (may disagree) says :\n> Coming office hours frequently causes student success classroom.groups 3 students, discuss following.Imagine start semester. design randomized experiment assess claim?\nImagine can assign students treatment condition comply\nConsider details: enroll, define frequently, assess success, etc.\nImagine can assign students treatment condition complyConsider details: enroll, define frequently, assess success, etc.Imagine end semester. randomized study run. want conduct observational study using administrative records (get IRB approval ). design observational study assess claim?expectation clear answers now. course semester learn formalize types questions solutions.","code":""},{"path":"defining-counterfactuals.html","id":"defining-causal-effects","chapter":"1 Defining counterfactuals","heading":"1.3 Defining causal effects","text":"Aug 29. Slides.\nclass, read Chapter 1 Hernán Robins 2020.Today define average causal effects potential outcomes framework.end class, able todefine potential outcomesexplain Fundamental Problem Causal Inference1recall statistical concepts: random variables, expectation, conditional expectation","code":""},{"path":"exchangeability-and-experiments.html","id":"exchangeability-and-experiments","chapter":"2 Exchangeability and experiments","heading":"2 Exchangeability and experiments","text":"","code":""},{"path":"exchangeability-and-experiments.html","id":"randomized-experiments","chapter":"2 Exchangeability and experiments","heading":"2.1 Randomized experiments","text":"Sep 3. Slides. class, read Hernán Robins 2020 Chapter 2 end 2.1.Much course address observational studies non-randomized treatments. set stage, today first discuss randomized experiments powerful possible.","code":""},{"path":"exchangeability-and-experiments.html","id":"lab-statistics-review-with-math-and-simulations","chapter":"2 Exchangeability and experiments","heading":"2.2 Lab: Statistics review with math and simulations","text":"Sep 4. Slides.course use several ideas previous coursework statistics, including random variables, expected values, independence. lab review concepts math using simulations R.","code":""},{"path":"exchangeability-and-experiments.html","id":"exchangeability-and-conditional-randomization","chapter":"2 Exchangeability and experiments","heading":"2.3 Exchangeability and conditional randomization","text":"Sep 5. Slides. class, read Hernán Robins 2020 Chapter 2.2.talk experiments good: help us ask precise causal questions, setting key assumption (exchangeability) holds design. discuss exchangeability simple randomized experiments experiments conditionally randomized treatment assignment probabilities functions pre-existing characteristics.","code":""},{"path":"exchangeability-and-experiments.html","id":"standardization-and-effect-measures","chapter":"2 Exchangeability and experiments","heading":"2.4 Standardization and effect measures","text":"Sep 10. Slides. class, read Hernán Robins 2020 Chapter 1.3 2.3.Standardization important statistical procedure two steps:estimate causal effect population subgroupaverage population distribution subgroupsIn conditionally randomized experiments, standardization essential yield unbiased estimates population average causal effect. strategy also essential observational studies discuss soon.end class, able todescribe different ways quantitatively measure causal effectestimate average causal effect using data conditionally randomized experiment","code":""},{"path":"exchangeability-and-experiments.html","id":"lab-analyze-a-randomized-experiment","chapter":"2 Exchangeability and experiments","heading":"2.5 Lab: Analyze a randomized experiment","text":"Sep 11. Slides.lab use R analyze data randomized experiment households randomized receive mailers encouraging vote, researchers examined effects voter turnout (Gerber, Green, & Larimer 2008).","code":""},{"path":"exchangeability-and-experiments.html","id":"inverse-probability-weighting","chapter":"2 Exchangeability and experiments","heading":"2.6 Inverse probability weighting","text":"Sep 12. Slides. class, read Hernán Robins 2020 Chapter 2.4, 3.1, 3.2.class introduce inverse probability weighting approach estimate average causal effects conditional exchangeability holds.","code":""},{"path":"consistency-and-positivity.html","id":"consistency-and-positivity","chapter":"3 Consistency and positivity","heading":"3 Consistency and positivity","text":"see lecture slides Fall 2023, click . Updated slides linked soon available.","code":""},{"path":"consistency-and-positivity.html","id":"asking-good-causal-questions","chapter":"3 Consistency and positivity","heading":"3.1 Asking good causal questions","text":"Sep 17. Slides. class, read Hernán Robins 2020 Chapter 3. Optionally, read Hernán 2016.Good causal questions structured credibility strong two key assumptions: positivity consistency.Positivity. Every population subgroup receives every treatment value non-zero probabilityConsistency. Potential outcomes \\(Y^\\) well-defined linked observable dataAfter class, ready discussion lab related common violation consistency assumption one unit’s treatment affects another unit’s outcome.","code":""},{"path":"consistency-and-positivity.html","id":"lab-interference","chapter":"3 Consistency and positivity","heading":"3.2 Lab: Interference","text":"Sep 18 Slides.defining causal effects, often discuss outcome \\(Y^\\) person realize exposed treatment value \\(\\). definitions become harder exists interference: outcome unit \\(\\) depends treatment assigned unit \\(j\\). discussion focus understanding interference need update potential outcomes notation interference present.","code":""},{"path":"directed-acyclic-graphs.html","id":"directed-acyclic-graphs","chapter":"4 Directed Acyclic Graphs","heading":"4 Directed Acyclic Graphs","text":"see lecture slides Fall 2023, click . Updated slides linked soon available.","code":""},{"path":"directed-acyclic-graphs.html","id":"marginal-independence","chapter":"4 Directed Acyclic Graphs","heading":"4.1 Marginal independence","text":"Sep 19. Slides. class, read Hernán Robins 2020 Chapter 6.1 6.2. historical reference, optionally see Greenland, Pearl, Robins 1999.class introduce key ideas DAGs.Directed Acyclic Graph. series nodes representing variables, connected directed edges representing direct causal effects. node edge least two nodes must drawn graph.Path. path sequence edges connecting two nodesCollider along path. node \\(B\\) directed edges collide: \\(\\rightarrow B \\leftarrow C\\). collider blocks path.DAGs help us know variables \\(\\) \\(B\\) statistically related\\(\\) \\(B\\) marginally dependent exists unblocked path connecting \\(\\) \\(B\\) marginally independent paths connecting blocked","code":""},{"path":"directed-acyclic-graphs.html","id":"conditional-independence","chapter":"4 Directed Acyclic Graphs","heading":"4.2 Conditional independence","text":"Sep 24. Slides. class, read Hernán Robins 2020 Chapter 6.3 6.4, especially Fine Point 6.1 page abbreviation.Often, want condition set variables \\(\\vec{L}\\) conditional exchangeability holds.path blocked node path blocked. every node path open, entire path openA non-collider blocked conditioned , otherwise openA collider open descendants conditioned . Otherwise blocked","code":""},{"path":"directed-acyclic-graphs.html","id":"lab-dags-review-and-causal-discovery","chapter":"4 Directed Acyclic Graphs","heading":"4.3 Lab: DAGs Review and Causal discovery","text":"Sep 25. Slides. class, read Hernán Robins 2020 Fine Point 6.3.lab, ’re reviewing DAG basics identifying paths determining whether path open closed. ’ll also talk bit causal discovery practice creating DAGs data.","code":""},{"path":"directed-acyclic-graphs.html","id":"sufficient-adjustment-sets","chapter":"4 Directed Acyclic Graphs","heading":"4.4 Sufficient adjustment sets","text":"Sep 26. Slides. class, read Hernán Robins 2020 7.1–7.4.marginal exchangeability hold, may able condition set variables \\(\\vec{L}\\) conditional exchangeability holds. can accomplish blocking non-causal paths \\(\\) \\(Y\\). set called sufficient adjustment set. find sufficient adjustment set, use backdoor criterion:set \\(L\\) blocks backdoor pathsThe set \\(L\\) contain descendants \\(\\)","code":""},{"path":"statistical-modeling.html","id":"statistical-modeling","chapter":"5 Statistical modeling","heading":"5 Statistical modeling","text":"","code":""},{"path":"statistical-modeling.html","id":"why-model","chapter":"5 Statistical modeling","heading":"5.1 Why model?","text":"Sep 26. Slides. class, read Hernán Robins 2020 Chapter 11.point, used statistical models. Instead, havetaken means within subgroupsthen aggregated subgroupsToday discuss strategy breaks many confounding variables, thus many subgroups.","code":""},{"path":"statistical-modeling.html","id":"lab-parametric-g-formula","chapter":"5 Statistical modeling","heading":"5.2 Lab: Parametric g-formula","text":"Sep 27. Download corresponding R Markdown file .discussion, make sure download data ’ll using. See Ed Discussion post detail.class, read Hernán Robins 2020 Chapter 13 15.1.Solutions lab exercise slides","code":""},{"path":"statistical-modeling.html","id":"inverse-probability-of-treatment-weighting","chapter":"5 Statistical modeling","heading":"5.3 Inverse probability of treatment weighting","text":"Sep 28. Slides. Reading: class, read Hernán Robins 2020 Chapter 12.1–12.5.Today introduce estimate causal effects modeling probability treatment, also known propensity score.","code":""},{"path":"statistical-modeling.html","id":"matching","chapter":"5 Statistical modeling","heading":"5.4 Matching","text":"Oct 3. Slides. class, read Hernán Robins 2020 Chapter 15.2.Today introduce idea matching allows us estimate average treatment treated.","code":""},{"path":"statistical-modeling.html","id":"lab-matching-in-r","chapter":"5 Statistical modeling","heading":"5.5 Lab: Matching in R","text":"Oct 4. Slides. R Markdown.lab, ’ll go distance metrics matching multiple covariates. ’ll also go examples using R matching estimate causal effects.","code":""},{"path":"statistical-modeling.html","id":"discussion-of-matching","chapter":"5 Statistical modeling","heading":"5.6 Discussion of matching","text":"Oct 5 Slides. R Markdown.’ll wrap discussion matching introducing propensity score matching coarsened exact matching. ’ll also discuss combining regression matching methods estimate causal effects.","code":""},{"path":"statistical-modeling.html","id":"lab-final-project-hw4-qa","chapter":"5 Statistical modeling","heading":"5.7 Lab: Final Project + HW4 Q&A","text":"Oct 10 SlidesWe’ll talk final project!","code":""},{"path":"statistical-modeling.html","id":"worked-example-of-statistical-modeling","chapter":"5 Statistical modeling","heading":"5.8 Worked example of statistical modeling","text":"section presents math code worked example statistical\nmodeling, includingoutcome modelinginverse probability treatment weightingmatchingWe use methods answer causal question:degree completing 4-year college degree age 25\nincrease probability college-educated spouse \nresidential partner age 35?theory motivates question follows. College causes\npeople personally higher earnings. also affects \nprobability someone lives high-earning partner. college\ndegree thus affects household incomes effect \nindividual earnings also effect individuals pool\nhouseholds.","code":""},{"path":"statistical-modeling.html","id":"data-access","chapter":"5 Statistical modeling","heading":"5.8.1 Data access","text":"prepared data study question. need \ndownload data directly data distributor National\nLongitudinal Survey Youth 1997\ncohort. .First, download two supporting files us:nlsy97.NLSY97\ntagset file containing variable namesprepare_nlsy97.R\nR script prepare dataput files directory workNext, download data NLSY97.register surveylog NLS Investigatorchoose NLSY97 studyupload tagset downloaded usdownload data. , change file name default\nnlsy97unzip file. Find nlsy97.dat unzipped folderdrag file folder workIn R console, run line code belowAfter following steps, data working directory! \ncan load data quickly future typingWhy can’t just send data? Two reasons!NLSY97 created procedure register users encourage\nethical use data researchBy registering, help Bureau Labor Statistics know \nmany people using data, helpful demonstrating\nwide use data useful securing funding \nfuture surveys!","code":"\ninstall.packages(\"tidyverse\") # if you do not have it yet\ninstall.packages(\"Amelia\")    # if you do not have it yet\nsource(\"prepare_nlsy97.R\")\nlibrary(tidyverse)\nd <- readRDS(\"d.RDS\")"},{"path":"statistical-modeling.html","id":"worked-example-outcome-modeling","chapter":"5 Statistical modeling","heading":"5.8.2 Worked example: Outcome modeling","text":"Outcome modeling based following identification result, \ntranslates causal quantity statistical estimand \ninvolve counterfactual outcomes.\\[\\begin{aligned}\n&E(Y^) \\\\\n&\\text{law iterated expectation,}\\\\\n&= E(E(Y^\\mid \\vec{L})) \\\\\n&\\text{exchangeability,}\\\\\n&= E(E(Y^\\mid \\vec{L}, = )) \\\\\n&\\text{consistency,}\\\\\n&= E(E(Y\\mid \\vec{L}, = ))\n\\end{aligned}\\]use sample mean estimator outer expectation, \ndiscuss several estimators inner conditional\nexpectation. \\[\\begin{aligned}\n\\hat{E}(Y^) &= \\frac{1}{n}\\sum_{=1}^n \\hat{E}(Y\\mid \\vec{L} = \\vec\\ell_i, = )\n\\end{aligned}\\]Now intuition: estimator tells tofor unit \\(\\) sample, estimate expected outcome among\npeople look like unit (\\(\\vec{L} = \\vec\\ell_i\\)) got\ntreatment value interest \\(= \\).take average estimate unitsA nonparametric strategy step (1) literally estimate \nexpected outcome taking sample average among units \nidentical unit \\(\\) along confounders \\(\\vec{L}\\). \nmany confounding variables units, might zero\ncases! parametric strategy assume model outcome,\n\\[E(Y\\mid \\vec{L} = \\vec\\ell, = ) = \\alpha + \\beta + \\vec\\ell'\\vec\\gamma\\]\nparameters \\(\\{\\alpha,\\beta,\\vec\\gamma\\}\\) estimated \nOrdinary Least Squares regression.Note: model like! example, add\ninteractions use logistic regression instead.model, want predict expected outcome \ntreatment value \\(\\) every unit unit’s observed confounder\nvalues.\\[\\hat{E}(Y\\mid \\vec{L} = \\vec\\ell_i, = ) = \\hat\\alpha + \\hat\\beta + \\vec\\ell'_i\\hat{\\vec\\gamma}\\]\ncode, wouldmodify every unit’s treatment value \\(\\)\nintuition: actually intervened treatment \nworld, value treatment change values \n\\(\\vec{L}\\) remain unchanged\nintuition: actually intervened treatment \nworld, value treatment change values \n\\(\\vec{L}\\) remain unchangedpredict outcome every unitaverage sampleIn code , estimated three causal quantities\\(E(Y^1)\\), probability respondent \ncollege-educated spouse partner intervened assign \ncollege degree \\(E(Y^0)\\), probability respondent \ncollege-educated spouse partner intervened assign \ncollege degree \\(E(Y^1-Y^0)\\), average causal effect college degree \nspouse partner college degree","code":"\noutcome_model <- lm(y ~ a + sex + race + \n                      mom_educ + dad_educ + \n                      log_parent_income +\n                      log_parent_wealth +\n                      test_percentile,\n                    data = d)\n# Make data where all are treated\nd_if_treated <- d %>%\n  mutate(a = \"college\")\n# Make data where all are untreated\nd_if_untreated <- d %>%\n  mutate(a = \"no_college\")\npredicted_outcome <- d %>%\n  mutate(yhat1 = predict(outcome_model,\n                         newdata = d_if_treated),\n         yhat0 = predict(outcome_model,\n                         newdata = d_if_untreated))\npredicted_outcome %>%\n  summarize(average_yhat1 = mean(yhat1),\n            average_yhat0 = mean(yhat0),\n            average_effect = mean(yhat1 - yhat0))## # A tibble: 1 × 3\n##   average_yhat1 average_yhat0 average_effect\n##           <dbl>         <dbl>          <dbl>\n## 1         0.427         0.164          0.263"},{"path":"statistical-modeling.html","id":"worked-example-treatment-modeling","chapter":"5 Statistical modeling","heading":"5.8.3 Worked example: Treatment modeling","text":"Using different identification result, can also proceed \nparametric model treatment instead outcome. \nstrategy, population mean outcome \\(E(Y^)\\) treatment \\(\\)\nequals weighted average units observed treatment,\nweighted weight equalswhen \\(= \\), inverse probability treatmentwhen \\(\\neq \\), zeroBelow identification proof inverse probability treatment\nweighting.math complicated, intuition simpler: \nweighting, create pseudo-population treatment \\(\\) \nindependent confounders \\(\\vec{L}\\). , need know\npropensity score: probability observed treatment value\ngiven confounders.identification result points toward inverse probability \ntreatment weighting estimator known Horvitz-Thompson estimator,\n\\[\\hat{E}(Y^) = \\frac{1}{n}\\sum_{=1}^n \\frac{Y_i\\mathbb{}(A_i=)}{\\hat{P}(= \\mid\\vec{L} = \\vec\\ell_i)}\\]\nrelated estimator often used Hajek estimator, \nnormalizes weights sum 1.\n\\[\\hat{E}(Y^) = \\frac{1}{\\sum_{=1}^n\\frac{\\mathbb{}(A_i=)}{\\hat{P}(= \\mid \\vec{L} = \\vec\\ell_i)}}\\sum_{=1}^n \\frac{Y_i\\mathbb{}(A_i=)}{\\hat{P}(= \\mid\\vec{L} = \\vec\\ell_i)}\\]code, wouldestimate model probability treatment given confounders,\nexample logistic regressionfor every unit, predict probability \\(= \\text{College}\\)estimate probability treatment observedfor went college, equals p_collegefor , equals 1 - p_collegeThis quantity often called propensity score. encapsulates\ninformation contained confounders probability \ntreatment.estimate mean outcomes among treatment, weighted \ninverse propensity score. actually two ways \n.Horvitz-Thompson estimator relies fact true\npropensity scores sum number observationsthe Hajek estimator uses weighted mean, thus normalizing weights\nsum 1While asymptotically valid, finite-sample reasons \nprefer second estimator (Hajek).","code":"\ntreatment_model <- glm(I(a == \"college\") ~ \n                         sex + race + \n                         mom_educ + dad_educ + \n                         log_parent_income +\n                         log_parent_wealth +\n                         test_percentile,\n                       data = d,\n                       family = binomial)\npredicted_p_college <- d %>%\n  mutate(p_college = predict(treatment_model,\n                             # the line below tells R\n                             # to predict a probability\n                             # rather than log odds\n                             type = \"response\"))\npredicted_p_scores <- predicted_p_college %>%\n  mutate(propensity_score = case_when(\n    a == \"college\" ~ p_college,\n    a == \"no_college\" ~ 1 - p_college\n  ))\npredicted_p_scores %>%\n  summarize(y1 = mean(y * I(a == \"college\") / propensity_score),\n            y0 = mean(y * I(a == \"no_college\") / propensity_score))## # A tibble: 1 × 2\n##      y1    y0\n##   <dbl> <dbl>\n## 1 0.374 0.164\npredicted_p_scores %>%\n  group_by(a) %>%\n  summarize(estimate = weighted.mean(y, w = 1 / propensity_score))## # A tibble: 2 × 2\n##   a          estimate\n##   <chr>         <dbl>\n## 1 college       0.401\n## 2 no_college    0.163"},{"path":"statistical-modeling.html","id":"worked-example-matching","chapter":"5 Statistical modeling","heading":"5.8.4 Worked example: Matching","text":"also estimate matching. Matching can interpreted \noutcome modeling strategy conditional mean outcome\n\\(E(Y\\mid\\vec{} = , \\vec{L} = \\vec\\ell_i)\\) estimated mean\noutcome among set units whose confounder values similar \nunit \\(\\) received treatment value \\(\\) interest.One way defining ``similar’’ propensity score matching: find\nunits whose probability treatment given confounders close \nprobability unit \\(\\). example, code ,estimates probability college completion using logistic\nregressionfor person finished college, matches non-college\ngraduate whose probability completing college similarThe variable matched$weights one element person \ndataset. indicates many times person appears matched\nsample. are1,533 college graduates weight 11,533 matched non-graduates weight 14,705 non-matched non-graudates weight 0Within matched data, non-graduates graduates similar\nalong confounding variables. estimatethe probability college educated spouse among college\ngraduates mean among peoplethe probability persisted \nfinished college, mean among matched counterpartsAn even better estimator might use linear regression adjust \ndifferences within matched pairs exist matches \nidentical., predict potential outcoems matched fit report \nexpected outcome among college graduates factual treatment\nunderThese results suggest completing college increases probability\ncollege-educated spouse partner 27 percentage\npoints.","code":"\nlibrary(MatchIt)\nmatched <- matchit(a == \"college\" ~ sex + race + mom_educ + \n                     dad_educ + log_parent_income + \n                     log_parent_wealth + test_percentile,\n                   method = \"nearest\", \n                   distance = \"glm\",\n                   estimand = \"ATT\",\n                   data = d)\ntable(d$a,matched$weights)##             \n##                 0    1\n##   college       0 1533\n##   no_college 4705 1533\nd %>%\n  mutate(weight = matched$weights) %>%\n  group_by(a) %>%\n  summarize(p_spouse_college = weighted.mean(y, w = weight))## # A tibble: 2 × 2\n##   a          p_spouse_college\n##   <chr>                 <dbl>\n## 1 college               0.528\n## 2 no_college            0.232\nmatched_fit <- lm(y ~ a*(sex + race + mom_educ + \n                           dad_educ + log_parent_income + \n                           log_parent_wealth + test_percentile), \n                  data = d, \n                  weights = matched$weights)\n# Create data frames for prediction\ncollege_grads_factual <- d %>%\n  filter(a == \"college\")\ncollege_grads_counterfactual <- college_grads_factual %>%\n  mutate(a = \"no_college\")\n\n# Predict outcomes from the model\ncollege_grads_factual %>%\n  mutate(yhat_college = predict(matched_fit, \n                                newdata = college_grads_factual),\n         yhat_no_college = predict(matched_fit,\n                                   newdata = college_grads_counterfactual)) %>%\n  # Report estimated average outcomes\n  select(starts_with(\"yhat\")) %>%\n  summarize_all(.funs = mean) %>%\n  # Estimate the causal effect\n  mutate(effect = yhat_college - yhat_no_college)## # A tibble: 1 × 3\n##   yhat_college yhat_no_college effect\n##          <dbl>           <dbl>  <dbl>\n## 1        0.528           0.259  0.269"},{"path":"front-door.html","id":"front-door","chapter":"6 Front door","heading":"6 Front door","text":"Oct 12. Slides. class, read Hernán Robins 2020 Technical Point 7.4. Optionally, see Glynn Kashin 2018This lecture engage new methods causal identification beyond backdoor adjustment. learning goals generalengage new causal identification approachtranslate method codecritique identification assumptionsFront door methods causal identification one case use show building blocks already know prepared learn new approaches causal identification.","code":""},{"path":"front-door.html","id":"identification","chapter":"6 Front door","heading":"Identification","text":"focus simplest case front door identification, depicted DAG variables \\(\\), \\(M\\), \\(Y\\) binary.setting, slides show following identification result.\\[P(Y^)=\\sum_m P(M = m\\mid = ) \\sum_{'}P(= ')P(Y\\mid M = m, = ')\\]","code":""},{"path":"front-door.html","id":"code-example","chapter":"6 Front door","heading":"Code example","text":"lecture slides translate method code one simulated example. providing code make easy copy follow along.Examine descriptive relationship \\(\\) \\(Y\\).Estimate probability \\(M\\) given \\(\\). causal assumptions, corresponds expected value \\(M\\) assignment value \\(\\) since \\(M\\rightarrow \\) unconfounded.Within front-door identification formula, need marginal probability treatment value.also need outcome distribution given \\(M\\) \\(\\).Given , can use backdoor adjustment identify outcome intervention \\(M\\) backdoor adjustment \\(\\).Bringing together, front-door identification.","code":"\nlibrary(tidyverse)\nsim_data <- function(n = 100) {\n  data.frame(U = runif(n)) %>%\n    # Generate a binary treatment\n    mutate(A = rbinom(n(), \n                      prob = U, \n                      size = 1)) %>%\n    # Generate a binary mediator\n    mutate(M = rbinom(n(), \n                      prob = .1 + .8*A, \n                      size = 1)) %>%\n    # Generate a binary outcome\n    mutate(Y = rbinom(n(), \n                      prob = plogis(U + .5*M), \n                      size = 1))\n}\ndata <- sim_data(n = 10e3)\ndata %>%\n  group_by(A) %>%\n  summarize(Y = mean(Y))## # A tibble: 2 × 2\n##       A     Y\n##   <int> <dbl>\n## 1     0 0.597\n## 2     1 0.754\np_M_given_A <- data %>%\n  # Count size of each group\n  group_by(A, M) %>%\n  count() %>%\n  # Convert to probability within A\n  group_by(A) %>%\n  mutate(p_M_under_A = n / sum(n)) %>%\n  select(A,M,p_M_under_A) %>%\n  print()## # A tibble: 4 × 3\n## # Groups:   A [2]\n##       A     M p_M_under_A\n##   <int> <int>       <dbl>\n## 1     0     0      0.902 \n## 2     0     1      0.0982\n## 3     1     0      0.103 \n## 4     1     1      0.897\n# Probability of each A\np_A <- data %>%\n  # Count size of each group\n  group_by(A) %>%\n  count() %>%\n  # Convert to probability\n  ungroup() %>%\n  mutate(p_A = n / sum(n)) %>%\n  select(A,p_A) %>%\n  print()## # A tibble: 2 × 2\n##       A   p_A\n##   <int> <dbl>\n## 1     0   0.5\n## 2     1   0.5\n# Probability of Y = 1 given M and A\np_Y_given_M_A <- data %>%\n  group_by(A,M) %>%\n  summarize(P_Y_given_A_M = mean(Y),\n            .groups = \"drop\") %>%\n  print()## # A tibble: 4 × 3\n##       A     M P_Y_given_A_M\n##   <int> <int>         <dbl>\n## 1     0     0         0.587\n## 2     0     1         0.682\n## 3     1     0         0.660\n## 4     1     1         0.765\n# Probability of Y = 1 under intervention on M\np_Y_under_M <- p_Y_given_M_A %>%\n  left_join(p_A, by = \"A\") %>%\n  group_by(M) %>%\n  summarize(p_Y_under_M = sum(P_Y_given_A_M  * p_A)) %>%\n  print()## # A tibble: 2 × 2\n##       M p_Y_under_M\n##   <int>       <dbl>\n## 1     0       0.624\n## 2     1       0.724\n# Probability of Y = 1 under intervention on A\np_Y_under_A <- p_M_given_A %>%\n  left_join(p_Y_under_M,\n            by = \"M\") %>%\n  group_by(A) %>%\n  summarize(estimate = sum(p_M_under_A * p_Y_under_M)) %>%\n  print()## # A tibble: 2 × 2\n##       A estimate\n##   <int>    <dbl>\n## 1     0    0.634\n## 2     1    0.713"},{"path":"instrumental-variables.html","id":"instrumental-variables","chapter":"7 Instrumental variables","heading":"7 Instrumental variables","text":"","code":""},{"path":"instrumental-variables.html","id":"experimental-settings","chapter":"7 Instrumental variables","heading":"7.1 Experimental settings","text":"Oct 17. Slides.instrumental variable (IV) identification strategy applies treatment effect \\(\\) \\(Y\\) confounded unobserved variables (\\(U\\)), instrument \\(Z\\) creates random unconfounded variation \\(\\).clean setting IV randomized experiments non-compliance: experimenter randomizes assigned treatment (\\(Z\\)) actual treatment (\\(\\)) may unequal \\(Z\\) units follow assignment. first class discuss setting.","code":""},{"path":"instrumental-variables.html","id":"lab-instrumental-variable-analysis-in-code","chapter":"7 Instrumental variables","heading":"7.2 Lab: Instrumental Variable analysis in Code","text":"lab implement instrumental variables estimators R.October 18 Slides. Download \nR Markdown file . Update:\nSolutions coding exercise .","code":""},{"path":"instrumental-variables.html","id":"observational-settings","chapter":"7 Instrumental variables","heading":"7.3 Observational settings","text":"Oct 19 Slides. class, read Hernán Robins 2020 Chapter 16.Thursday, move IV analysis observational settings. focus casual assumptions required IV. assumptions often hold design experiments non-compliance. observational settings, can doubtful.","code":""},{"path":"regression-discontinuity.html","id":"regression-discontinuity","chapter":"8 Regression discontinuity","heading":"8 Regression discontinuity","text":"","code":""},{"path":"regression-discontinuity.html","id":"introduction","chapter":"8 Regression discontinuity","heading":"8.1 Introduction","text":"Oct 24. Slides. class, read Huntington Klein 2021 Chapter 20, sections 20.1 20.3.4.settings, treatment assigned based solely value continuous variable. situations, can identify local ATE without many additional assumptions. Today introduce works using regression discontinuity designs.","code":""},{"path":"regression-discontinuity.html","id":"lab-regression-discontinuity---bandwidth-and-examples","chapter":"8 Regression discontinuity","heading":"8.2 Lab: Regression Discontinuity - Bandwidth and Examples","text":"Oct 25 lab, read Huntington Klein 2021 Chapter 20, section 20.2.1.\nSlides Download today’s .Rmd document .’ll discuss bandwidth selection triangular weighting. ’ll also apply regression discontinuity concrete example give chance try .","code":""},{"path":"regression-discontinuity.html","id":"discussion","chapter":"8 Regression discontinuity","heading":"8.3 Discussion","text":"Oct 26. Slides. class, read Huntington Klein 2021 Chapter 20, sections 20.2.2. 20.3.1.’ll continue discussion regression discontinuity designs discussing fuzzy RDD validation checks RDD.","code":""},{"path":"difference-in-difference.html","id":"difference-in-difference","chapter":"9 Difference in difference","heading":"9 Difference in difference","text":"","code":""},{"path":"difference-in-difference.html","id":"introduction-1","chapter":"9 Difference in difference","heading":"9.1 Introduction","text":"Oct 31. Slides. reading required, reference see Card & Krueger 1994Today study effect policy change New Jersey, drawing evidence neighboring state Pennsylvania.Difference difference identification strategy used one units become treated time point others . believe assumption parallel trends, can use change time never-treated units estimate change time experienced units become treated, counterfactual world become treated.","code":""},{"path":"difference-in-difference.html","id":"lab","chapter":"9 Difference in difference","heading":"9.2 Lab","text":"Nov 1 Slides. Download \nR Markdown file .lab, implement difference difference estimator specific setting. example comes Malesky, Nguyen, & Tran 2014 closely follow re-analysis data Egami & Yamauchi 2023.","code":""},{"path":"difference-in-difference.html","id":"extensions-of-did","chapter":"9 Difference in difference","heading":"9.3 Extensions of DID","text":"Nov 2. Slides. reading required, reference see Egami & Yamauchi 2023How know parallel trends assumption holds? hold? class discusses recent extensions framework.","code":""},{"path":"synthetic-control.html","id":"synthetic-control","chapter":"10 Synthetic control","heading":"10 Synthetic control","text":"","code":""},{"path":"synthetic-control.html","id":"introduction-2","chapter":"10 Synthetic control","heading":"10.1 Introduction","text":"Nov 7 Slides.","code":""},{"path":"synthetic-control.html","id":"lab-1","chapter":"10 Synthetic control","heading":"10.2 Lab","text":"Nov 8 SlidesWe review main idea behind synthetic control well compare contrast synthetic control matching difference--differences. also dig deeper select weights synthetic control review worked example assess performance method.","code":""},{"path":"synthetic-control.html","id":"discussion-1","chapter":"10 Synthetic control","heading":"10.3 Discussion","text":"Nov 9 Slides. (Updated explanation interference).\nclass, read Chapter 10 Causal Inference Mixtape Cunningham 2021","code":""},{"path":"data-driven-methods.html","id":"data-driven-methods","chapter":"11 Data-driven methods","heading":"11 Data-driven methods","text":"","code":""},{"path":"data-driven-methods.html","id":"introduction-3","chapter":"11 Data-driven methods","heading":"11.1 Introduction","text":"Nov 14 Slides.given intervention, subgroups people respond others. Ideas machine learning can help us target human attention toward subgroups.Concrete example. responds nudget go walk? Imagine first conduct survey asks people much love fall, (\\(X = 1\\) least) (\\(X = 10\\) ). randomize control condition (\\(= \\texttt{untreated}\\)) treatment condition (\\(= \\texttt{treated}\\)) encourages go walk outside. outcome \\(Y\\) active minutes day, recorded activity tracker.Simulated data. real data, can difficult evaluate causal estimators truth unknown. Today use data simulated known process order study properties estimators. code prepare R environment function simulate_sample() generate data 50 observations.example code simulate data:Causal estimands. example, like estimate \\[\\tau_x = E(\\underbrace{Y^1 - Y^0}_{\\substack{\\text{effect }\\\\\\text{nudge walk}\\\\\\text{active}\\\\\\text{minutes}}}\\mid \\underbrace{X = x}_{\\substack{\\text{among }\\\\\\text{love }\\\\\\text{fall = }x}})\\]\nvalue \\(x = 1,\\dots,10\\). estimands average causal effect nudge walk (\\(\\)) active minutes (\\(Y\\)) within subgroups defined value scale love fall (\\(X\\)).Identification. simulate data, \\(\\) assigned random. backdoor paths \\(\\) \\(Y\\).Estimator. estimator function takes dataset returns estimates. nonparametric estimator setting.can apply estimator follows.Task. Using sample simulated computer, estimate average causal effect \\(\\) \\(Y\\) within subgroups defined \\(X\\). Report two numbers us.value \\(X\\) estimated effect \\(\\) positive?effect estimate?discuss distribution estimates get class.ready early, think might evaluate performance approach many repeated simulations.","code":"\nlibrary(tidyverse)\nsource(\"https://raw.githubusercontent.com/causal3900/causal3900.github.io/main/assets/data/simulate_sample.R\")\nsimulated <- simulate_sample()##   X         A          Y\n## 1 1 untreated  56.927683\n## 2 1   treated   7.110561\n## 3 1 untreated 107.449019\n## 4 1   treated 111.412427\n## 5 1 untreated  56.421883\n## 6 2   treated  26.483190\nestimator <- function(data) {\n  data %>%\n    # Group by treatment A and confounder X\n    group_by(A, X) %>%\n    # Summarize by the average outcome within groups\n    summarize(Y = mean(Y),\n              .groups = \"drop\") %>%\n    # Reshape the data\n    pivot_wider(names_from = \"A\",\n                values_from = \"Y\",\n                names_prefix = \"y_\") %>%\n    # Estimate the effect within groups\n    mutate(effect = y_treated - y_untreated)\n}\nestimate <- estimator(simulated)"},{"path":"data-driven-methods.html","id":"machine-learning-approaches","chapter":"11 Data-driven methods","heading":"11.2 Machine learning approaches","text":"Nov 16 Slides.Today generalize ideas Tuesday. discuss sample splitting makes easier tochoose among many estimandschoose among many estimatorsdevelop new data science approaches","code":""},{"path":"current-research.html","id":"current-research","chapter":"12 Current research","heading":"12 Current research","text":"","code":""},{"path":"current-research.html","id":"research-discussion-sam","chapter":"12 Current research","heading":"12.1 Research discussion: Sam","text":"Nov 21 SlidesToday discussing causal discovery: might learn DAGs data","code":""},{"path":"current-research.html","id":"research-discussion-ian","chapter":"12 Current research","heading":"12.2 Research discussion: Ian","text":"Nov 28 Slides can optionally read project page: ilundberg.github.io/pstratreg.Sometimes treatment causes outcome undefined. problem well-studied randomized controlled trials biostatistics, people die end trial. talk applications idea study inequality require adjustment measured confounding. joint work Soonhong Cho (PhD Candidate, UCLA Political Science).","code":""},{"path":"course-recap.html","id":"course-recap","chapter":"13 Course recap","heading":"13 Course recap","text":"Nov 30 SlidesWe review learned semester!","code":""},{"path":"discussion-2.-stats-review.html","id":"discussion-2.-stats-review","chapter":"Discussion 2. Stats review","heading":"Discussion 2. Stats review","text":"NOTE: Fall 2023. updated reflect current term yet.execute simulations locally, download .Rmd ","code":"\nlibrary(ggplot2)\nlibrary(ggmosaic)\nlibrary(tibble)"},{"path":"discussion-2.-stats-review.html","id":"sample-expectations-converge-to-population","chapter":"Discussion 2. Stats review","heading":"13.1 Sample expectations converge to population","text":"can generate simulations show sample mean variance converge \npopulation values.","code":"\ntrue_mean <- 2\ntrue_var <- 5\n\nsample_mean_seq <- 1:3000\nsample_means <- vapply(\n  sample_mean_seq,\n  \\(x) mean(rnorm(n = x, mean = true_mean, sd = sqrt(true_var))),\n  numeric(1)\n)\nsample_variances <- vapply(\n  sample_mean_seq,\n  \\(x) {\n    data <- rnorm(n = x, mean = true_mean, sd = sqrt(true_var))\n    sample_mean <- mean(data)\n    sum((data - sample_mean)^2)/length(data)\n  },\n  numeric(1)\n)\n\nmeans <- tibble(\"N\" = sample_mean_seq, \"Sample Mean\" = sample_means)\nvars <- tibble(\"N\" = sample_mean_seq, \"Sample Variance\" = sample_variances)\n\ncolors <- c(\"Sample Mean\" = \"lightblue\", \"Population Mean\" = \"red\")\n\nggplot(means, aes(y = `Sample Mean`, x = N)) +\n  geom_line(color = \"lightblue\") +\n  geom_abline(slope = 0, intercept = true_mean, color = \"red\") +\n  theme_bw()\nggplot(vars, aes(y = `Sample Variance`, x = N)) +\n  geom_line(color = \"lightblue\") +\n  geom_abline(slope = 0, intercept = true_var, color = \"red\") +\n  theme_bw()"},{"path":"discussion-2.-stats-review.html","id":"simulate-conditional-expectations","chapter":"Discussion 2. Stats review","heading":"13.2 Simulate conditional expectations","text":"Simulate conditional expectations within groups differ sample\nmean.","code":"\ngroup1_means <- rnorm(100, mean = 20, sd = 5)\ngroup2_means <- rnorm(100, mean = 30, sd = 5)\ngroup_means <- data.frame(\n  \"Group\" = c(rep(\"Group 1\", 100), rep(\"Group 2\", 100)),\n  \"Values\" = c(group1_means, group2_means),\n  \"x\" = rnorm(200, 5, sd = 3)\n)\nggplot(group_means, aes(x = x, y = Values, color = Group)) +\n  geom_point() +\n  geom_abline(\n    slope = 0,\n    intercept = mean(group_means$Values),\n    show.legend = TRUE,\n    color = \"gray30\"\n  ) +\n  geom_abline(\n    slope = 0,\n    intercept = mean(group_means[group_means$Group == \"Group 1\", ]$Values),\n    show.legend = TRUE,\n    color = \"#F8766D\"\n  ) +\n  geom_abline(\n    slope = 0,\n    intercept = mean(group_means[group_means$Group == \"Group 2\", ]$Values),\n    show.legend = TRUE,\n    color = \"#00BFC4\"\n  ) +\n  theme_bw()"},{"path":"discussion-2.-stats-review.html","id":"show-independence-of-variables---example-of-two-dice-rolling","chapter":"Discussion 2. Stats review","heading":"13.3 Show independence of variables - example of two dice rolling","text":"","code":"\ndice_1 <- sample(1:6, 100000, replace = TRUE)\ndice_2 <- sample(1:6, 100000, replace = TRUE)\ndice <- tibble(\n  \"dice\" = c(rep(\"Die 1\", 100000), rep(\"Die 2\", 100000)),\n  \"value\" = c(dice_1, dice_2)\n)\n\nggplot(data = dice) +\n  geom_mosaic(aes(x = product(dice, value), fill = dice)) +   \n  labs(y=\"\", x=\"Value Rolled\", title = \"Independence of dice roll\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"none\")"},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"discussion-03.-analyzing-an-experiment-in-r","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"Discussion 03. Analyzing an Experiment in R","text":"NOTE: Fall 2023. updated reflect current term yet.Slides","code":""},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"download-.rmd-document","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.4 Download .Rmd Document","text":"Download today’s .Rmd document .","code":""},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"get-out-and-vote-experiment","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.5 Get out and Vote Experiment","text":"lab, explore experiment digs mechanisms\nunderlying people vote. exercise based :Gerber, Alan S., Donald P. Green, Christopher W. Larimer. “Social Pressure Voter Turnout: Evidence Large-scale Field Experiment.” American Political Science Review 102.1 (2008): 33-48.long-standing theory many people\nvote driven social norms (e.g. understanding voting\ncivic duty). theory, dominant theoretical\nexplanation, little empirical backing long time. experiment\nexamines theory asking question:\nextent social norms cause voter turnout?","code":""},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"experimental-design","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.5.1 Experimental Design","text":"order answer question, approximately 80,000 Michigan households\nrandomly assigned treatment control groups, treatment\ngroup randomly assigned one four possible treatment arms. \ntreatment arms varied intensity social pressure conveyed,\ndefined follows:first treatment arm mailed letter simply reminded \nvoting civic duty.second treatment arm mailed letter telling researchers\nstudying voting turnout based public records.third treatment arm mailed letter stating voting turnout\nrevealed members household.fourth treatment arm mailed letter stating voting turnout\nrevealed household neighbors.","code":""},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"analyze-experiment","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.6 Analyze Experiment","text":"Download RMarkdown file .","code":""},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"necessary-packages","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.6.1 Necessary packages","text":"Note: errors probably either don’t dplyr haven\ninstalled.","code":"\nlibrary(dplyr)\nlibrary(haven)"},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"import-data","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.6.2 Import data","text":"Alternatively (really want), download data load directly computer. Make sure save data directory RMarkdown file .\ncan can import data :gotv <- read_dta(\"social_pressure.dta\")Run following code get quick peek dataset using function glimpse. returns info number rows/columns, column names, type data column. Notice information year birth yob explicitly age. Also notice treatments labeled numbers 0 4.","code":"\ngotv <- read_dta(\"https://causal3900.github.io/assets/data/social_pressure.dta\")\nglimpse(gotv)"},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"clean-data","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.6.3 Clean data","text":"First, let’s construct age variable describing old (number years)\nperson year 2006. yob variable says year person\nborn . , use mutate function, can read .Given person’s year birth, calculate age year 2006? Note can arithmetic operations information dataset. example, two columns col_1 col_2 wanted create third column called col_3 sum two columns, write:mutate(col_3 = col_1 + col_2)code started . Fill appropriate expression age = add column gotv labeled age contains old person 2006.Now, convert treatment variable ’s numeric representation \ncorresponding labels are0: “Control”1: “Hawthorne” (‘researchers viewing records via public data’ treatment arm)2: “Civic Duty” (‘voting civic duty’ treatment arm)3: “Neighbors” (‘voting turnout revealed neighbors’ treatment arm)4: “Self” (‘voting turnout revealed household’ treatment arm), want use function case_when described .\ngeneral syntax case_when(condition ~ output-value)example, condition treatement == 0 output value \"Control\". search every value treatment column equals 0 replace string \"Control\".started code . Decide argument(s) pass inside parantheses case_when().Now, use glimpse see added age variable treatments word instead number labels.","code":"\ngotv <- gotv |>\n  mutate(age = )\ngotv <- gotv |>\n  mutate(treatment = case_when()) \nglimpse(gotv)"},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"balance-table","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.6.4 Balance table","text":"Next, ’re going confirm control treatment groups look pretty\nmuch across set covariates, .e. two groups balanced covariates. Specifically means ’re going calculate mean value set covariates across treatment/control\narms, expect pretty much equal randomization worked. related idea exchangeability.exercise, going reproduce table similar Table 1 paper. want table shows mean value following covariates five treatment arms: Household size, Nov 2002, Nov 2000, Aug 2004, Aug 2002, Aug 2000, Female, Age (years). create table 5 rows, one treatment arm, 8 columns, one covariate interest.started code . need :Pass argument group_by() calculate seperate means treatment arm.\nLook documentation group_by function.\nLook documentation group_by function.Pass argument summarise() computes mean covariate covariates seperate treatment arm.\nLook documentation summarise function.\nmay find function across useful well. can use function inside summarise()!\nLook documentation summarise function.may find function across useful well. can use function inside summarise()!Note numbers match exactly Table 1. want notice values column similar across rows.","code":"\ncovariates <- c(\"sex\", \"age\", \"g2000\", \"g2002\", \"p2000\", \"p2002\", \"p2004\", \"hh_size\")\n\ngotv_balance <- gotv |>\n  group_by(...) |>\n  summarise(...)\n\nprint(gotv_balance)"},{"path":"discussion-03.-analyzing-an-experiment-in-r.html","id":"results","chapter":"Discussion 03. Analyzing an Experiment in R","heading":"13.6.5 Results","text":"Finally, let’s replicate final results (Table 2). treatment group, calculate percentage individuals got voted, well total number individuals group! started code . need toPass argument group_by() working treatment arm seperately (!)Pass two arguments summarise() following:\nCreate column titled Percentage_Voting contains percent group voted\nCreate column titled num_of_individuals contains total number people group\nCreate column titled Percentage_Voting contains percent group votedCreate column titled num_of_individuals contains total number people group","code":"\ngotv_results <- gotv |>\n  group_by(...) |>\n  summarise(...)\n\nprint(gotv_results)"},{"path":"discussion-07.-causal-effects-with-matching.html","id":"discussion-07.-causal-effects-with-matching","chapter":"Discussion 07. Causal Effects with Matching","heading":"Discussion 07. Causal Effects with Matching","text":"NOTE: Fall 2023. updated reflect current term yet.Slides\nDownload today’s .Rmd document .packages may need install first:optmatch: install.packages(\"optmatch\")sandwich: install.packages(\"sandwich\")MatchIt: install.packages(\"MatchIt\")marginaleffects: install.packages(\"marginaleffects\")","code":""},{"path":"discussion-07.-causal-effects-with-matching.html","id":"r-markdown","chapter":"Discussion 07. Causal Effects with Matching","heading":"13.7 R Markdown","text":"Learn matching “MatchIt” packageFirst, ’ll use data set last week compare greedy vs optimal matching. ’ll use NLSY data since larger.First compare optimal vs greedy. Optimal matching usually better greedy matching, long data isn’t bigOn full data, using optimal possible, can take bit time. larger data sets, might possible","code":"\nd <- readRDS(\"assets/discussions/d.RDS\")\n\n# matchit function implements matching\n# Formula: Treatment ~ variables to match on\n# method: nearest is a greedy 1:1 matching without replacement\n# distance: euclidean (other possible options are scaled_euclidean, mahalanobis, robust_mahalanobis)\n# read more on distances here: https://rdrr.io/cran/MatchIt/man/distance.html#mat\nm.out0 <- matchit(a == \"college\" ~ log_parent_income + log_parent_wealth\n                              + test_percentile,\n                              method = \"nearest\", distance = \"euclidean\",\n                              data = d)\n## Optimal vs greedy with NYLSR data\n# With n = 1000; .01 sec vs .8 sec\n# With n = 2000; .05 sec vs 8 sec\n# With n = 4000; .1 vs 25\n\nind <- sample(nrow(d), size = 1000)\n\n# Greedy is using nearest\nsystem.time(m.out0 <- matchit(a == \"college\" ~ log_parent_income + log_parent_wealth\n                              + test_percentile,\n                              method = \"nearest\", distance = \"euclidean\",\n                              data = d[ind, ]))##    user  system elapsed \n##   0.016   0.000   0.016\n# method: optimal is optimal matching\nsystem.time(m.out0 <- matchit(a == \"college\" ~ log_parent_income + log_parent_wealth\n                              + test_percentile,\n                              method = \"optimal\", distance = \"euclidean\",\n                              data = d[ind, ]))##    user  system elapsed \n##   1.290   0.077   1.524\n# With full data set greedy matching takes ~ 0.5-1.5 seconds\nsystem.time(m.out0 <- matchit(a == \"college\" ~ log_parent_income + log_parent_wealth\n                              + test_percentile,\n                              method = \"nearest\", distance = \"euclidean\",\n                              data = d))##    user  system elapsed \n##   0.605   0.077   0.684\n# Meanwhile, optimal matching takes 60-130 seconds\nsystem.time(m.out0 <- matchit(a == \"college\" ~ log_parent_income + log_parent_wealth\n                              + test_percentile,\n                              method = \"optimal\", distance = \"euclidean\",\n                              data = d))##    user  system elapsed \n## 117.511   5.250 124.215"},{"path":"discussion-07.-causal-effects-with-matching.html","id":"matching-with-job-training-data-from-evaluating-the-econometric-evaluations-of-training-programs-with-experimental-data-lalonde-1986","chapter":"Discussion 07. Causal Effects with Matching","heading":"13.8 Matching with job training data from “Evaluating the econometric evaluations of training programs with experimental data” (LaLonde 1986)","text":"remainder lab, ’ll use portion data job training program. particular, treatment whether someone participated job training program. outcome interest salary 1978 (re78).expect income 1974 highly correlated income 1975. also much higher variability age.Now let’s try run matching procedure using function.","code":"\n# Load the data\ndata(\"lalonde\")\n\n# See what's in the data\n?lalonde # this opens up the \"Help\" tab with the documentation! \nhead(lalonde)##      treat age educ   race married nodegree re74 re75\n## NSW1     1  37   11  black       1        1    0    0\n## NSW2     1  22    9 hispan       0        1    0    0\n## NSW3     1  30   12  black       0        0    0    0\n## NSW4     1  27   11  black       0        1    0    0\n## NSW5     1  33    8  black       0        1    0    0\n## NSW6     1  22    9  black       0        1    0    0\n##            re78\n## NSW1  9930.0460\n## NSW2  3595.8940\n## NSW3 24909.4500\n## NSW4  7506.1460\n## NSW5   289.7899\n## NSW6  4056.4940\n## Suppose there are 3 individuals\ndat <- matrix(c(50, 5000, 5500,\n                20, 5100, 5900,\n                40, 5200, 6200), ncol = 3, byrow = T)\ncolnames(dat) <- c(\"age\", \"re74\", \"re75\")\n\n# Is individual 2 or 3 more similar to individual 1? \n# To answer this, we should compute the distances between individuals 1 and 2, and 1 and 3. \n\n# One way is to compute the Mahalanobis distance by first computing the covariance matrix of the confounding variables\n# In this case, the confounders are age, re74, and re75\ndataCov <- lalonde %>%\n  select(age, re74, re75) %>%\n  cov\n\n# Then we compute the Mahalanobis distance with the function mahalanobis_dist\nmahalanobis_dist( ~ age + re74 + re75, data = dat, var = dataCov)##          1        2        3\n## 1 0.000000 3.225953 1.098595\n## 2 3.225953 0.000000 2.152528\n## 3 1.098595 2.152528 0.000000\n# We can also compute the Euclidean distance\ndist(dat, method = \"euclidean\")##          1        2\n## 2 413.4005         \n## 3 728.0797 316.8596\n# For now, let's start with Euclidean distance, even if may not be great\n\n# method: nearest (i.e. greedy 1:1)\n# distance: euclidean\n# data: lalonde (the data set we're working with)\n# replace: True (T) or False (F) - whether to sample with or without replacement. \n    # Note, if allowing for replacement, greedy and optimal are the same\n    # So for the function, you only need to specify if using method = \"nearest\"\n# ratio: how many control matches for each treated unit\n# caliper: by default, the caliper width in standard units (i.e., Z-scores)\nm.out0 <- matchit(treat ~ re74 + re75 + age + race,\n                  method = \"nearest\", distance = \"euclidean\",\n                  data = lalonde, replace = F, \n                  ratio = 1, caliper = c(re74  = .2, re75 = .2))"},{"path":"discussion-07.-causal-effects-with-matching.html","id":"assessing-the-matching","chapter":"Discussion 07. Causal Effects with Matching","heading":"13.9 Assessing the matching","text":"can check well balancing doneWe can also visually asses matchesAs first step, simply compare means outcomes groups. Notice first time ’ve looked outcomes!","code":"\n?summary.matchit # Look in the Help tab (on the bottom right) for documentation on summary.matchit\n\n\n# interactions: check interaction terms too? (T or F)\n# un: show statistics for unmatched data as well (T or F)\nsummary(m.out0, interactions = F, un = F)## \n## Call:\n## matchit(formula = treat ~ re74 + re75 + age + race, data = lalonde, \n##     method = \"nearest\", distance = \"euclidean\", replace = F, \n##     caliper = c(re74 = 0.2, re75 = 0.2), ratio = 1)\n## \n## Summary of Balance for Matched Data:\n##            Means Treated Means Control Std. Mean Diff.\n## re74           1643.2931     1666.9106         -0.0048\n## re75           1021.5989     1086.1734         -0.0201\n## age              25.6971       26.1543         -0.0639\n## raceblack         0.8400        0.2800          1.5403\n## racehispan        0.0571        0.1714         -0.4833\n## racewhite         0.1029        0.5486         -1.5040\n##            Var. Ratio eCDF Mean eCDF Max Std. Pair Dist.\n## re74           1.0122    0.0108   0.1543          0.0293\n## re75           1.0026    0.0166   0.1543          0.0422\n## age            0.4213    0.0894   0.2000          0.9967\n## raceblack           .    0.5600   0.5600          1.7289\n## racehispan          .    0.1143   0.1143          0.7249\n## racewhite           .    0.4457   0.4457          1.6968\n## \n## Sample Sizes:\n##           Control Treated\n## All           429     185\n## Matched       175     175\n## Unmatched     254      10\n## Discarded       0       0\n# Std. Mean Diff (SMD): difference of means, standardized by sd of treatment group\n# Var. Ratio: ratio of variances in treatment and control group. Compares spread of data\n# Rubin (2001) presents rule of thumb that SMD should be less than .25 and variance ratio should be between .5 and 2\n# Max eCDF: Kolmogorov-Smirnov statistic. Max difference across entire CDF\n## Produces QQ plots of all variables in which.xs\nplot(m.out0, type = \"qq\", which.xs = ~age + re74, interactive = F)\n## Plots the density of all variables in which.xs\nplot(m.out0, type = \"density\", which.xs = ~age + re74 + race, interactive = F)\n## Plots the empirical CDF of all variables in which.xs\nplot(m.out0, type = \"ecdf\", which.xs = ~age + re74, interactive = F)\n# Produces data frame same as input, but has two additional columns\n# weights: the weight of the row in the paired data set. Can be greater than 1\n#         if the data set was matched more than once\n# subclass: the index of the \"pair\"\n#\nm.out0.data <- match.data(m.out0, drop.unmatched = T)\nhead(m.out0.data)##      treat age educ   race married nodegree re74 re75\n## NSW1     1  37   11  black       1        1    0    0\n## NSW2     1  22    9 hispan       0        1    0    0\n## NSW3     1  30   12  black       0        0    0    0\n## NSW4     1  27   11  black       0        1    0    0\n## NSW5     1  33    8  black       0        1    0    0\n## NSW6     1  22    9  black       0        1    0    0\n##            re78 weights subclass\n## NSW1  9930.0460       1        1\n## NSW2  3595.8940       1       88\n## NSW3 24909.4500       1       99\n## NSW4  7506.1460       1      110\n## NSW5   289.7899       1      121\n## NSW6  4056.4940       1      132\nnames(m.out0.data)##  [1] \"treat\"    \"age\"      \"educ\"     \"race\"     \"married\" \n##  [6] \"nodegree\" \"re74\"     \"re75\"     \"re78\"     \"weights\" \n## [11] \"subclass\"\n# Also produces matched data set, though will duplicate rows if matching with replacement\n# and a control is matched more than once\nm.out0.data <- get_matches(m.out0)\n# Take the mean of both groups\n# this will only work if all weights are 1\naggregate(re78~ treat, FUN = mean, data = m.out0.data)##   treat     re78\n## 1     0 5422.184\n## 2     1 6193.594\n# Fitting a linear model on the treatments will work\n# even if all weights are not 1. We just need to feed them in\nfit1 <- lm(re78~ treat, data = m.out0.data, weights = weights)\n\n# vcov: tells R to use robust standard errors\navg_comparisons(fit1, variables = \"treat\",\n                vcov = \"HC3\",\n                newdata = subset(m.out0.data, treat == 1),\n                wts = \"weights\")## Warning: The `treat` variable is treated as a categorical\n##   (factor) variable, but the original data is of class\n##   integer. It is safer and faster to convert such\n##   variables to factor before fitting the model and\n##   calling `slopes` functions.\n##   \n##   This warning appears once per session.## \n##   Term Contrast Estimate Std. Error    z Pr(>|z|)   S 2.5 %\n##  treat    1 - 0      771        753 1.02    0.306 1.7  -705\n##  97.5 %\n##    2248\n## \n## Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n## Type:  response"},{"path":"discussion-07.-causal-effects-with-matching.html","id":"fit-your-own-model","chapter":"Discussion 07. Causal Effects with Matching","heading":"13.10 Fit your own model","text":"Now try . Note, need something similar homework dataset.Think appropriate DAG might choose variables want match \nAsk : know choose variables match ?\nAsk : know choose variables match ?Choose matching procedure\nAsk : know explain matching procedure bias-variance trade ?\nAsk : know explain matching procedure bias-variance trade ?Evaluate balance match. doesn’t look good, try another matching procedure\nAsk : know balanced matching looks like?\nAsk : know balanced matching looks like?Estimate causal effect\nAsk : know just estimated?\nAsk : know just estimated?","code":""},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"discussion-10.-causal-effects-with-regression-discontinuity","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"Discussion 10. Causal Effects with Regression Discontinuity","text":"NOTE: Fall 2023. updated reflect current term yet.Slides\nDownload today’s .Rmd document .","code":""},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"choosing-a-bandwidth","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.11 Choosing a bandwidth","text":"One main difficulties regression discontinuity analysis choosing ``good’’ bandwidth. good example bias variance trade-appears time statistics. general, let software choose us, helpful run small example better understand subtleties involved.Suppose \\(Y\\) non-linear function \\(X\\), want extrapolate predict \\(Y\\) \\(X=0\\).blindly fit line observations, predicted value \\(X=0\\) quite bit.\ninstead restrict smaller region around 0, can bit betterIdeally, decrease bandwidth reduce bias, also means working less data. Since data point noisy, get new sample, estimate change quite bit. Try running times see much estimate moves. Compare much estimate moves bandwidth 2.Knowing best bandwidth hard problem, generally speaking good choice bandwidth get smaller sample size increases.measure accuracy terms average squared error, can see \\(n = 100\\), bandwidth 1.2 seems best. However, test see changes larger values \\(n\\)estimate linear regression model, usually pick linear coefficients minimize squared errors\n\\[\\min_b \\sum_i(Y_i - X_i b)^2\\]\nsquared error observation count considered equally. good fit around points important points, can also use weighted least squares \\(w_i\\) weight.\n\\[\\min_b \\sum_i w_i(Y_i - X_i b)^2\\]\n\\(w_i\\) larger, squared error observation \\(\\) cost selected linear coefficient prioritize minimizing errors \\(w_i\\) large. particular setting, want prioritize fitting data well around \\(0\\), can use weights ``triangular’’ prioritize points near \\(0\\).","code":"\nn <- 100\nX <- runif(n, -5, 0)\n### Y is a very non-linear function of X + some noise\nY <- 5 - 3*X + .6 * X^2 + .3 * X^3 + 15 * sin(X) + rnorm(n, sd = 4)\nplot(X, Y, ylim = c(-15, 30), xlim = c(-5, 1))\nabline(v = 0, col = \"gray\", lwd = 2)\n\n### Avg Y when X = 0 is 5\npoints(0, 5, pch = 18, col = \"orange\", cex = 3)\n## Only consider points h away from 0\nh <- 3\n\nfit.mod.bandwidth <- lm(Y~X, subset = X > -h)\nplotColor <- ifelse(abs(X) > h, \"gray\", \"red\")\n\nplot(X, Y, ylim = c(-15, 30), xlim = c(-5, 1), col = plotColor)\nabline(v = 0, col = \"gray\", lwd = 2)\nsegments(-h, predict(fit.mod.bandwidth, newdata = data.frame(X = -h)),\n         0, predict(fit.mod.bandwidth, newdata = data.frame(X = 0)),col = \"blue\",\n         lwd = 2)\npoints(0, 5, pch = 18, col = \"orange\", cex = 3)\nn <- 10000\nX <- runif(n, -5, 0)\nY <- 5 - 3*X + .6 * X^2 + .3 * X^3 + 15 * sin(X) + rnorm(n, sd = 4)\nh <- 3\nfit.mod.bandwidth <- lm(Y~X, subset = X > -h)\nplotColor <- ifelse(abs(X) > h, \"gray\", \"red\")\n\nplot(X, Y, ylim = c(-15, 30), xlim = c(-5, 1), col = plotColor)\nabline(v = 0, col = \"gray\", lwd = 2)\nsegments(-h, predict(fit.mod.bandwidth, newdata = data.frame(X = -h)),\n         0, predict(fit.mod.bandwidth, newdata = data.frame(X = 0)),col = \"blue\",\n         lwd = 2)\n\npoints(0, 5, pch = 18, col = \"orange\", cex = 3)\nh <- .3 # bandwidth\n# h <- 2 \nn <- 100\nX <- runif(n, -5, 0)\nY <- 5 - 3*X + .6 * X^2 + .3 * X^3 + 15 * sin(X) + rnorm(n, sd = 4)\n\nfit.mod.bandwidth <- lm(Y~X, subset = X > -h)\nplotColor <- ifelse(abs(X) > h, \"gray\", \"red\")\n\nplot(X, Y, ylim = c(-15, 30), xlim = c(-5, 1), col = plotColor)\nabline(v = 0, col = \"gray\", lwd = 2)\nsegments(-h, predict(fit.mod.bandwidth, newdata = data.frame(X = -h)),\n         0, predict(fit.mod.bandwidth, newdata = data.frame(X = 0)),col = \"blue\",\n         lwd = 2)\n\npoints(0, 5, pch = 18, col = \"orange\", cex = 3)\npoints(0, predict(fit.mod.bandwidth, newdata = data.frame(X = 0)), col = \"blue\", pch = 19)\nn <- 100\nh <- c(.5, .8, 1.2, 2)\nsim.size <- 500\nrec <- matrix(0, sim.size, 8)\n\nfor(i in 1:sim.size){\n  X <- runif(n, -5, 0)\n  Y <- 5 - 3*X + .6 * X^2 + .3 * X^3 + 15 * sin(X) + rnorm(n, sd = 4)\n\n  fit.mod.bandwidth1 <- lm(Y~X, subset = X > -h[1])\n  fit.mod.bandwidth2 <- lm(Y~X, subset = X > -h[2])\n  fit.mod.bandwidth3 <- lm(Y~X, subset = X > -h[3])\n  fit.mod.bandwidth4 <- lm(Y~X, subset = X > -h[4])\n\n  rec[i, 1] <- (predict(fit.mod.bandwidth1, newdata = data.frame(X = 0)) - 5)^2\n  rec[i, 2] <- (predict(fit.mod.bandwidth2, newdata = data.frame(X = 0)) - 5)^2\n  rec[i, 3] <- (predict(fit.mod.bandwidth3, newdata = data.frame(X = 0)) - 5)^2\n  rec[i, 4] <- (predict(fit.mod.bandwidth4, newdata = data.frame(X = 0)) - 5)^2\n  \n  rec[i, 5] <- predict(fit.mod.bandwidth1, newdata = data.frame(X = 0))\n  rec[i, 6] <- predict(fit.mod.bandwidth2, newdata = data.frame(X = 0))\n  rec[i, 7] <- predict(fit.mod.bandwidth3, newdata = data.frame(X = 0))\n  rec[i, 8] <- predict(fit.mod.bandwidth4, newdata = data.frame(X = 0))\n\n}\n\ncolMeans(rec)## [1]  7.915190  5.300368  4.132376 13.093566  4.832740\n## [6]  4.633605  4.038871  1.696406\nboxplot(as.list(data.frame(rec[, 5:8])), ylim = c(0, 20))\nabline(h = 5, col = \"red\")\nn <- 200\nX <- runif(n, -5, 0)\nY <- 5 - 3*X + .6 * X^2 + .3 * X^3 + 15 * sin(X) + rnorm(n, sd = 4)\nweight <- 1 - abs(X) / 5 \nplot(X, weight, type = \"p\", main = \"Triangular Weights\")"},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"question","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.11.0.1 Question","text":"use bandwidth \\(h\\) (.e., include observations within \\(h\\) cut-), mean weights ?using triangular weights, can see better using points.","code":"\nn <- 200\nh <- 3\nX <- runif(n, -5, 0)\nY <- 5 - 3*X + .6 * X^2 + .3 * X^3 + 15 * sin(X) + rnorm(n, sd = 4)\n\nweight <- 1 - abs(X) / 5 \n# We don't include a bandwidth, but instead use triangular weights\nfit.mod <- lm(Y~X)\nfit.mod.weights <- lm(Y~X, weights = weight)\nfit.mod.bandwidth <- lm(Y~X, subset = abs(X) < h)\n\nplotColor <- ifelse(abs(X) > h, \"gray\", \"red\")\n\nplot(X, Y, ylim = c(-15, 30), xlim = c(-5, 1), col = plotColor)\nabline(v = 0, col = \"gray\", lwd = 2)\nsegments(-h, predict(fit.mod.bandwidth, newdata = data.frame(X = -h)),\n         0, predict(fit.mod.bandwidth, newdata = data.frame(X = 0)),col = \"blue\",\n         lwd = 2)\n\n# Fit from weighted least squares\nsegments(-5, predict(fit.mod.weights, newdata = data.frame(X = -h)),\n         0, predict(fit.mod.weights, newdata = data.frame(X = 0)),col = \"green\",\n         lwd = 2)\n\n# Fit from ordinary least squares\nsegments(-5, predict(fit.mod, newdata = data.frame(X = -h)),\n         0, predict(fit.mod, newdata = data.frame(X = 0)),col = \"purple\",\n         lwd = 2)\n\npoints(0, 5, pch = 18, col = \"orange\", cex = 3)\npoints(0, predict(fit.mod.bandwidth, newdata = data.frame(X = 0)), col = \"blue\", pch = 19)\npoints(0, predict(fit.mod.weights, newdata = data.frame(X = 0)), col = \"green\", pch = 19)\npoints(0, predict(fit.mod, newdata = data.frame(X = 0)), col = \"purple\", pch = 19)\n# bandwidth\nh <- 3\nn <- 200\nX <- runif(n, -5, 0)\nY <- 5 - 3*X + .6 * X^2 + .3 * X^3 + 15 * sin(X) + rnorm(n, sd = 4)\n\n# weight is triangular within the bandwidth\nweight <- ifelse(abs(X) < h, 1 - abs(X) / h, 0) \n\nplot(X, weight, type = \"p\", main = \"Triangular Weights\")"},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"rdd-analysis","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.12 RDD Analysis","text":"Now let’s apply regression discontinuity analysis data. example follows analysis ``Randomization Inference Regression Discontinuity Design: Application Party Advantages U.S. Senate’’ Cattaneo, Frandsen, Titiunik (2015) replication file.Political scientists interested effect incumbent (currently elected politician) share votes election. current public official means increased name recognition, fundraising opportunities, etc. hand, incumbent means can get blamed bad things happened term.data analyze considers US senators. state US two senators 6 year terms, election two senators alternates every 3 years. instance, two senate seats: B. 2000 senate seat undergoes election, 2003 senate seat B undergoes election senate seat continues, 2006 senate seat undergoes election senate seat B continues , etc.look causal effect incumbent senate races two different ways.First, incumbent effect vote share next time run office? Second, sitting senator party, effect election share?","code":""},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"question-1","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.12.0.1 Question","text":"Consider first question draw causal diagram treatment incumbent outcome interest vote share election.might use regression discontinuity provide answer question?","code":""},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"data","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.12.1 Data","text":"’ll examine data US Senate races 1914 2010 try answer questions.first analysis, consider whether incumbent effects vote share next election. Thus, outcome interest next time seat goes election 2 cycles future.Let’s zoom bit closer races use non-linear regression. appears still slight discontinuity.","code":"\ninstall <- function(package) {\n  if (!require(package, quietly = TRUE, character.only = TRUE)) {\n    install.packages(package, repos = \"http://cran.us.r-project.org\", type = \"binary\")\n    library(package, character.only = TRUE)\n  }\n}\n\ninstall(\"ggplot2\")\ninstall(\"lpdensity\")\ninstall(\"rddensity\")\ninstall(\"rdrobust\")\ninstall(\"rdlocrand\")\n\ndata <- read.csv(\"https://raw.githubusercontent.com/rdpackages-replication/CIT_2020_CUP/master/CIT_2020_CUP_senate.csv\")\n\nhead(data)##         state year dopen population presdemvoteshlag1\n## 1 Connecticut 1914     0    1233000          39.15937\n## 2 Connecticut 1916     0    1294000          39.15937\n## 3 Connecticut 1922     0    1431000          33.02737\n## 4 Connecticut 1926     0    1531000          27.52570\n## 5 Connecticut 1928     1    1577000          27.52570\n## 6 Connecticut 1932     0    1637000          45.57480\n##         demmv demvoteshlag1 demvoteshlag2 demvoteshfor1\n## 1  -7.6885610            NA            NA      46.23941\n## 2  -3.9237082      42.07694            NA      36.09757\n## 3  -6.8686604      36.09757      46.23941      35.64121\n## 4 -27.6680560      45.46875      36.09757      45.59821\n## 5  -8.2569685      35.64121      45.46875      48.47606\n## 6   0.7324815      45.59821      35.64121      51.74687\n##   demvoteshfor2 demwinprv1 demwinprv2 dmidterm dpresdem\n## 1      36.09757         NA         NA        1        1\n## 2      45.46875          0         NA        0        1\n## 3      45.59821          0          0        1        0\n## 4      48.47606          0          0        1        0\n## 5      51.74687          0          0        0        0\n## 6      39.80264          0          0        0        0\n# presdemvoteshlag1 is democratic vote share in the previous presidential election\n# demmv is the democratic margin of victory in the current senate election (i.e., democratic percentage - next closest percentage)\n#   so a value just above 0 indicates a very close victory, a value just below 0 indicates a very close loss\n# demovoteshlag1 and 2 indicates the vote share 1 and 2 election cycles ago\n# demovoteshfor1 and 2 indicates the vote share 1 and 2 elections cycles in the future\ndem_vote_t2 <- data$demvoteshfor2\ndem_margin_t0 <- data$demmv\n\n# plot the data\n# Set p = 0 for a straight line (i.e., regression with X^p)\nrdplot(y = dem_vote_t2, x =  dem_margin_t0, nbins = c(1000, 1000), p = 0, col.lines = \"red\", col.dots = \"lightgray\", title = \"Incumbency Advantage\", y.lim = c(0,100), x.label = \"Dem Margin of Victory\", y.label = \"Dem Vote Share in next election\")\nrdplot(dem_vote_t2[abs(dem_margin_t0) <= 25], dem_margin_t0[abs(dem_margin_t0) <= 25], nbins = c(2500, 500), p = 4, col.lines = \"red\", col.dots = \"lightgray\", title = \"\",  y.lim = c(0,100))"},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"estimating-the-causal-effect","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.12.2 Estimating the causal effect","text":"example, ’ll manually set bandwidth 10, estimate linear regression sides cut-. estimated intercept prediction \\(0\\), get estimate, just take difference.can try triangular weights.","code":"\n# Set bandwidth to 10\nh <- 10\n# Fit regression to left and right of cut-off\nlm_left <- lm(dem_vote_t2 ~ dem_margin_t0, subset = dem_margin_t0 < 0 & abs(dem_margin_t0) <= h)\nlm_right <- lm(dem_vote_t2 ~ dem_margin_t0, subset = dem_margin_t0 > 0 & abs(dem_margin_t0) <= h)\n\n# Estimate is difference in interecepts\nlm_right$coefficients[1] - lm_left$coefficients[1] ## (Intercept) \n##    6.898794\nh <- 10\nweight <- ifelse(abs(dem_margin_t0) < h, 1 - abs(dem_margin_t0) / h, 0) \n\n## Note we don't need subs\nlm_left <- lm(dem_vote_t2 ~ dem_margin_t0, subset = dem_margin_t0 < 0 & abs(dem_margin_t0) <= h, weights = weight)\nlm_right <- lm(dem_vote_t2 ~ dem_margin_t0, subset = dem_margin_t0 > 0 & abs(dem_margin_t0) <= h, weights = weight)\n\n\n# Estimate is difference in interecepts\nlm_right$coefficients[1] - lm_left$coefficients[1] ## (Intercept) \n##    7.984687"},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"question-2","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.12.2.1 Question","text":"Describe mathematical notation, plain language causal effect estimated.","code":""},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"using-rdrobust","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.13 Using rdrobust","text":"can see, getting estimates aren’t difficult ’ve selected bandwidth. selecting good bandwidth can tricky getting standard errors estimate also difficult. can use R package select bandwidth, estimate causal effect quantities, give standard errors.don’t specify bandwidth directly, software choose us","code":"\n# uniform kernel with bandwidth 10\nout <- rdrobust(dem_vote_t2, dem_margin_t0, kernel = 'uniform',  p = 1, h = 10)\nsummary(out)## Sharp RD estimates using local polynomial regression.\n## \n## Number of Obs.                 1297\n## BW type                      Manual\n## Kernel                      Uniform\n## VCE method                       NN\n## \n## Number of Obs.                  595          702\n## Eff. Number of Obs.             245          206\n## Order est. (p)                    1            1\n## Order bias  (q)                   2            2\n## BW est. (h)                  10.000       10.000\n## BW bias (b)                  10.000       10.000\n## rho (h/b)                     1.000        1.000\n## Unique Obs.                     595          702\n## \n## =============================================================================\n##         Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n## =============================================================================\n##   Conventional     6.899     1.722     4.007     0.000     [3.525 , 10.273]    \n##         Robust         -         -     3.891     0.000     [5.156 , 15.624]    \n## =============================================================================\n# triangular kernel with bandwidth 10\nout <- rdrobust(dem_vote_t2, dem_margin_t0,  kernel = 'triangular',  p = 1, h = 10)\nsummary(out)## Sharp RD estimates using local polynomial regression.\n## \n## Number of Obs.                 1297\n## BW type                      Manual\n## Kernel                   Triangular\n## VCE method                       NN\n## \n## Number of Obs.                  595          702\n## Eff. Number of Obs.             245          206\n## Order est. (p)                    1            1\n## Order bias  (q)                   2            2\n## BW est. (h)                  10.000       10.000\n## BW bias (b)                  10.000       10.000\n## rho (h/b)                     1.000        1.000\n## Unique Obs.                     595          702\n## \n## =============================================================================\n##         Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n## =============================================================================\n##   Conventional     7.985     1.838     4.344     0.000     [4.382 , 11.587]    \n##         Robust         -         -     4.387     0.000     [6.595 , 17.249]    \n## =============================================================================\n# uniform kernel with software selected bandwidth\nout <- rdrobust(dem_vote_t2, dem_margin_t0, kernel = 'triangular',  p = 1)\nsummary(out)## Sharp RD estimates using local polynomial regression.\n## \n## Number of Obs.                 1297\n## BW type                       mserd\n## Kernel                   Triangular\n## VCE method                       NN\n## \n## Number of Obs.                  595          702\n## Eff. Number of Obs.             360          323\n## Order est. (p)                    1            1\n## Order bias  (q)                   2            2\n## BW est. (h)                  17.754       17.754\n## BW bias (b)                  28.028       28.028\n## rho (h/b)                     0.633        0.633\n## Unique Obs.                     595          665\n## \n## =============================================================================\n##         Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n## =============================================================================\n##   Conventional     7.414     1.459     5.083     0.000     [4.555 , 10.273]    \n##         Robust         -         -     4.311     0.000     [4.094 , 10.919]    \n## ============================================================================="},{"path":"discussion-10.-causal-effects-with-regression-discontinuity.html","id":"try-on-your-own","chapter":"Discussion 10. Causal Effects with Regression Discontinuity","heading":"13.13.1 Try on your own","text":"Now try estimate causal effect senator election democrat democratic vote share senator election. case, outcome interest demvoteshfor1 since interested immediately following election.","code":"\ndem_vote_t1 <- data$demvoteshfor1\ndem_margin_t0 <- data$demmv\n\n# plot the data\n# Set p = 0 for a straight line (i.e., regression with X^p)\nrdplot(y = dem_vote_t1, x =  dem_margin_t0, nbins = c(1000, 1000), p = 0, col.lines = \"red\", col.dots = \"lightgray\", title = \"Incumbency Advantage\", y.lim = c(0,100), x.label = \"Dem Margin of Victory\", y.label = \"Dem Vote Share in next election\")"},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","text":"NOTE: Fall 2023. updated reflect current term yet.demonstrate synthetic control method using data Abadie \nGardeazabal (2003), studied economic effects conflict, using \nterrorist conflict Basque Country case study. paper used \ncombination Spanish regions construct synthetic Basque Country\nresembling many relevant economic characteristics Basque Country \nonset political terrorism 1970s.","code":""},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"load-the-data","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"13.14 Load the Data","text":"Let’s go ahead load packages data. notice quite\nlot missing data, don’t worry, ’s meant !","code":"\ninstall <- function(package) {\n  if (!require(package, quietly = TRUE, character.only = TRUE)) {\n    install.packages(package, repos = \"http://cran.us.r-project.org\", type = \"binary\")\n    library(package, character.only = TRUE)\n  }\n}\n\n# Load packages\ninstall(\"dplyr\")\ninstall(\"Synth\")\n\n# Load data\ndata(\"basque\")\nglimpse(basque)## Rows: 774\n## Columns: 17\n## $ regionno              <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n## $ regionname            <chr> \"Spain (Espana)\", \"Spain (Es…\n## $ year                  <dbl> 1955, 1956, 1957, 1958, 1959…\n## $ gdpcap                <dbl> 2.354542, 2.480149, 2.603613…\n## $ sec.agriculture       <dbl> NA, NA, NA, NA, NA, NA, 19.5…\n## $ sec.energy            <dbl> NA, NA, NA, NA, NA, NA, 4.71…\n## $ sec.industry          <dbl> NA, NA, NA, NA, NA, NA, 26.4…\n## $ sec.construction      <dbl> NA, NA, NA, NA, NA, NA, 6.27…\n## $ sec.services.venta    <dbl> NA, NA, NA, NA, NA, NA, 36.6…\n## $ sec.services.nonventa <dbl> NA, NA, NA, NA, NA, NA, 6.44…\n## $ school.illit          <dbl> NA, NA, NA, NA, NA, NA, NA, …\n## $ school.prim           <dbl> NA, NA, NA, NA, NA, NA, NA, …\n## $ school.med            <dbl> NA, NA, NA, NA, NA, NA, NA, …\n## $ school.high           <dbl> NA, NA, NA, NA, NA, NA, NA, …\n## $ school.post.high      <dbl> NA, NA, NA, NA, NA, NA, NA, …\n## $ popdens               <dbl> NA, NA, NA, NA, NA, NA, NA, …\n## $ invest                <dbl> NA, NA, NA, NA, NA, NA, NA, …"},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"data-definitions","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"13.15 Data definitions","text":"following definitions variables see :regionno regionname: Numeric identifiers Spanish region,\nnames (character) region.regionno regionname: Numeric identifiers Spanish region,\nnames (character) region.year: year corresponding row data. Spans \n1955-1997.year: year corresponding row data. Spans \n1955-1997.gdpcap: 1960–1969 averages real GDP per-capita measured thousands \n1986 USD.gdpcap: 1960–1969 averages real GDP per-capita measured thousands \n1986 USD.Variables sec. prefix: 1961–1969 average percentage total GDP \nsix different industries. example, first non-missing value \nsec.agriculture = 19.54. means 1961, Agriculture industry\nmade 20% total GDP Spain (note regionname\nvariable = Spain (Espana).Variables sec. prefix: 1961–1969 average percentage total GDP \nsix different industries. example, first non-missing value \nsec.agriculture = 19.54. means 1961, Agriculture industry\nmade 20% total GDP Spain (note regionname\nvariable = Spain (Espana).Variables school. prefix: 1964–1969 averages share \nworking-age population illiterate (school.illit), share \nprimary school education (school.prim), share high school\n(school.med), share high school (school.high), share \nhigh school (school.post.high).Variables school. prefix: 1964–1969 averages share \nworking-age population illiterate (school.illit), share \nprimary school education (school.prim), share high school\n(school.med), share high school (school.high), share \nhigh school (school.post.high).popdens: 1969 population density measured people per square kilometer.popdens: 1969 population density measured people per square kilometer.invest: 1964–1969 averages gross total investment divided total GDP.invest: 1964–1969 averages gross total investment divided total GDP.Running following line code show helpful information “Help” box bottom right R Studio Screen. call documentation. tells outcome variable predictor variables , plus descriptions variable dataset. Please ask us additional questions variable means.","code":"\n?basque"},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"questions-for-you","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"13.15.1 Questions For You","text":"running code reading documentation dataset, answer following questions:years contained dataset?Answer.many Spanish regions contained dataset?Answer.outcome variable called dataset?Answer.possible predictor variables?Answer.","code":""},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"prepare-the-data-for-analysis-using-dataprep","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"13.16 Prepare the data for analysis using dataprep","text":"first step reorganize dataset appropriate format \nsuitable main estimator function synth(). , use \ndataprep() function. see examples details data extraction, run\n?dataprep console. pop helpful page “Help” tab bottom right R Studio screen.code , need tell Synth following:predictor variables (split two groups)predictors: variables non-missing years included analysis\npredictors: variables non-missing years included analysisspecial.predictors: predictor variables missing values require little extra handling\nspecial.predictors: predictor variables missing values require little extra handlingHow want predictor variables aggregated (case, average)time period considering (case, 1964 1969)outcome variable (case, gdpcap)variable(s) identify different regions (regionname) /\nnumbers (regionno).variable denotes time period (year)region treated unit (region 17 AKA Basque Country)regions control units (c(2:16,18))time period want train model (pre-treatment period 1960:1969)time-period outcome data plotted\n(usually treatment, e.g., 1955 1997)Okay, now let’s prepare ’ve just described code.\n’ve added comments code explain exactly ’s happening line.\ncreating “prepared” dataset dataprep.running “unprepared” data dataprep function.\nSynth package requires data specific format synthetic control.Notice code use arguments predictors,\npredictors.op, time.predictors.prior, rest information\npredictor variables specified special.predictors\nlist. functionality designed allow easy handling \nseveral predictors operator (e.g. taking average) \npre-treatment period (case, school investment variables)\nwell additional custom (“special”) predictors varying operators\ntime-periods. example, variables sector production\nshares (e.g. sec.agriculture) available biennial basis\n(1961,1963,…,1969) extracted use code seq(1961,1969,2).\ndetails examples use special.predictors\ncan seen running ?dataprep console.","code":"\ndataprep.out <- dataprep(\n  foo = basque,          # Our analysis data that needs to be prepared\n  predictors = c(        # 1(a). list the variables that we want to use as predictors\n    \"school.illit\",     \n    \"school.prim\",       \n    \"school.med\",\n    \"school.high\",\n    \"school.post.high\",\n    \"invest\"\n  ),\n  predictors.op = \"mean\",               # 2. Tell Synth to take the average of all variables in `predictors` above.\n  time.predictors.prior = 1964:1969,    # 3. Take the average of variables in `predictors` from 1964 to 1969.\n  special.predictors = list(            # 1(b). Additional variables to include as predictors in our model:\n    list(\"gdpcap\", 1960:1969 , \"mean\"), # -    Take the average of `gdcap` from 1960 to 1969\n                                        # -    Take the average of all others for every OTHER year from 1961 to 1969\n    list(\"sec.agriculture\", seq(1961, 1969, 2), \"mean\"),\n    list(\"sec.energy\", seq(1961, 1969, 2), \"mean\"),\n    list(\"sec.industry\", seq(1961, 1969, 2), \"mean\"),\n    list(\"sec.construction\", seq(1961, 1969, 2), \"mean\"),\n    list(\"sec.services.venta\", seq(1961, 1969, 2), \"mean\"),\n    list(\"sec.services.nonventa\", seq(1961, 1969, 2), \"mean\"),\n    list(\"popdens\", 1969, \"mean\")       # -    Take the average of `popdens` only in 1969\n  ),\n  dependent = \"gdpcap\",                 # 4. Specify our outcome variable\n  unit.variable = \"regionno\",           # 5(a). Specify the numeric identifier of each region\n  unit.names.variable = \"regionname\",   # 5(b). Specify the name of each region\n  time.variable = \"year\",               # 6. Specify what variable is our time variable\n  treatment.identifier = 17,            # 7. Specify which region in `regionno` is our treated region\n  controls.identifier = c(2:16, 18),    # 8. Specify which regions in `regionno` should be in our donor pool\n  time.optimize.ssr = 1960:1969,        # 9. Specify what years should make up our pre-treatment time period\n  time.plot = 1955:1997                 # 10. Specify what years to plot our outcome variable for\n)"},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"construct-our-synthetic-control","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"13.17 Construct our Synthetic Control","text":"Now, ’re ready use synth() function create synthetic control\nGDP Basque Country region Spain. described discussion,\nmeans synth() create weights regions\nweighted average regions’ GDP closely match \ntrue GDP Basque Country region.’ll explore model output .","code":"\nsynth.out <- synth(data.prep.obj = dataprep.out, method = \"BFGS\")## \n## X1, X0, Z1, Z0 all come directly from dataprep object.\n## \n## \n## **************** \n##  searching for synthetic control unit  \n##  \n## \n## **************** \n## **************** \n## **************** \n## \n## MSPE (LOSS V): 0.008864605 \n## \n## solution.v:\n##  0.03881798 0.001220442 4.26792e-05 0.0001235262 1.6599e-06 1.76355e-05 0.04072702 0.2396775 0.02234054 0.248494 0.005974697 0.01098894 0.04858995 0.3429834 \n## \n## solution.w:\n##  1.67e-08 4.27e-08 7.43e-08 2.78e-08 2.97e-08 5.545e-07 3.66e-08 4.28e-08 0.8508029 9.23e-08 2.75e-08 4.94e-08 0.1491958 4.13e-08 9.75e-08 1.167e-07"},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"summarizing-our-synthetic-control-with-tables","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"13.18 Summarizing our Synthetic Control with Tables","text":"First, can begin creating summary tables Synthetic Control\nmodel.synth.tables variable now contains four tables help us evaluate\nsynthetic control. first table looks pre-treatment period \ncompares predictor values Basque Country (denoted Treated \ntable) Synthetic Control. Note want values \nTreated Synthetic columns really close together.can see values Treated Synthetic columns pretty\ndifferent variables indicate perhaps synthetic\ncontrol isn’t similar like.Next, can look weights got assigned non-treatment\nregions. can drop regions weight 0 since regions don’t\ncontribute synthetic control !w.weights column can see two regions contribute \nsynthetic control. Cataluna region makes approximately 85% \nsynthetic control Madrid region makes additional 15%. means\n16 regions donor pool, 2 picked create\nsynthetic control!Finally can look weights got assigned predictor\nvariables. can interpreted relative importance \npredictor variables.can see school.med, school.high, school.post.high, \ninvest variables weight 0, means least\nimportant impact creation synthetic control. \nhand, population density 1969 (special.popdens.1969) \nimportant variable.","code":"\nsynth.tables <- synth.tab(dataprep.res = dataprep.out, synth.res = synth.out)\nsynth.tables$tab.pred##                                          Treated Synthetic\n## school.illit                              39.888   256.335\n## school.prim                             1031.742  2730.092\n## school.med                                90.359   223.341\n## school.high                               25.728    63.437\n## school.post.high                          13.480    36.154\n## invest                                    24.647    21.583\n## special.gdpcap.1960.1969                   5.285     5.271\n## special.sec.agriculture.1961.1969          6.844     6.179\n## special.sec.energy.1961.1969               4.106     2.760\n## special.sec.industry.1961.1969            45.082    37.636\n## special.sec.construction.1961.1969         6.150     6.952\n## special.sec.services.venta.1961.1969      33.754    41.104\n## special.sec.services.nonventa.1961.1969    4.072     5.371\n## special.popdens.1969                     246.890   196.287\n##                                         Sample Mean\n## school.illit                                170.786\n## school.prim                                1127.186\n## school.med                                   76.260\n## school.high                                  24.235\n## school.post.high                             13.478\n## invest                                       21.424\n## special.gdpcap.1960.1969                      3.581\n## special.sec.agriculture.1961.1969            21.353\n## special.sec.energy.1961.1969                  5.310\n## special.sec.industry.1961.1969               22.425\n## special.sec.construction.1961.1969            7.276\n## special.sec.services.venta.1961.1969         36.528\n## special.sec.services.nonventa.1961.1969       7.111\n## special.popdens.1969                         99.414\nsynth.tables$tab.w[synth.tables$tab.w$w.weights != 0, ]##    w.weights            unit.names unit.numbers\n## 10     0.851              Cataluna           10\n## 14     0.149 Madrid (Comunidad De)           14\nsynth.tables$tab.v##                                         v.weights\n## school.illit                            0.039    \n## school.prim                             0.001    \n## school.med                              0        \n## school.high                             0        \n## school.post.high                        0        \n## invest                                  0        \n## special.gdpcap.1960.1969                0.041    \n## special.sec.agriculture.1961.1969       0.24     \n## special.sec.energy.1961.1969            0.022    \n## special.sec.industry.1961.1969          0.248    \n## special.sec.construction.1961.1969      0.006    \n## special.sec.services.venta.1961.1969    0.011    \n## special.sec.services.nonventa.1961.1969 0.049    \n## special.popdens.1969                    0.343"},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"summarizing-our-synthetic-control-with-plots","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"13.19 Summarizing our Synthetic Control with Plots","text":"Finally, can plot economic output Basque Contry region \ncompare economic output Synthetic Control. make \nconvincing case large treatment effect, like see two\ntrajectories outcome variable Basque Country Synthetic\nControl unit quite similar prior violent conflict diverge\nsharply violent conflict occurs.Let’s create plot see indicates significant treatment effect.can see economic output units looks super similar \n1975 violent conflict began earnest. point ,\neconomic output Basque Country drops significantly. \nindicate violent conflict fairly large negative impact \neconomic output Basque Country region.Another way can visualize creating plot, instead\nshowing two lines outcome Basque Country region \noutcome Synthetic Control Unit, plot single line \ndifference two lines time period.plot conveys information . , economic output\nBasque Country region drops well economic output \nSynthetic Control unit violent conflict begins 1975. words\nviolent conflict lowers economic output.","code":"\npath.plot(\n  synth.res = synth.out,\n  dataprep.res = dataprep.out,\n  Ylab = \"real per-capita GDP (1986 USD, thousand)\",\n  Xlab = \"year\",\n  Ylim = c(0, 12),\n  Legend = c(\"Basque country\", \"synthetic Basque country\"),\n  Legend.position = \"bottomright\"\n)\nabline(a = NULL, b = NULL, h = NULL, v = 1975, col = \"red\")\ngaps.plot(\n  synth.res = synth.out,\n  dataprep.res = dataprep.out,\n  Ylab = \"gap in real per-capita GDP (1986 USD, thousand)\",\n  Xlab = \"year\",\n  Ylim = c(-1.5, 1.5),\n  Main = NA\n)\nabline(a = NULL, b = NULL, h = NULL, v = 1975, col = \"red\")"},{"path":"discussion-12.-empirical-application-how-the-onset-of-violent-conflict-affects-economic-output.html","id":"summary","chapter":"Discussion 12. Empirical Application: How the Onset of Violent Conflict Affects Economic Output","heading":"13.20 Summary","text":"tutorial, ’ve walked prepare data Synthetic\nControl method following steps:Prepare data using dataprep() function.Prepare data using dataprep() function.Create Synthetic Control unit using synth function.Create Synthetic Control unit using synth function.Evaluate model tables using synth.tab function.Evaluate model tables using synth.tab function.Plot outcomes treated unit synthetic control unit \npath.plot gaps.plot functions.Plot outcomes treated unit synthetic control unit \npath.plot gaps.plot functions.","code":""},{"path":"due-dates.html","id":"due-dates","chapter":"Due dates","heading":"Due dates","text":"’ll post due dates throughout semester.Pset 1: Due Tuesday, September 10th 5pm via Canvas","code":""},{"path":"final-project.html","id":"final-project","chapter":"Final Project","heading":"Final Project","text":"final project opportunity engage course content via real-world example. group project, components throughout semester might submit work . details come.welcome look final project details Fall 2023, note significant changes.","code":""},{"path":"problem-set-1.-definitions.html","id":"problem-set-1.-definitions","chapter":"Problem Set 1. Definitions","heading":"Problem Set 1. Definitions","text":"Relevant material covered Aug 29. Problem set due Sept 10 5pm.Welcome problem set! homework practice conceptual notation ideas descriptive causal inference.complete problem set, Download .Rmd complete homework. Omit name can anonymous peer feedback. Compile PDF submit PDF Canvas.","code":""},{"path":"problem-set-1.-definitions.html","id":"practice-with-potential-outcomes","chapter":"Problem Set 1. Definitions","heading":"1. Practice with potential outcomes","text":"Jose says coming Cornell caused discover statistics, became major! says gone NYU, stuck biology.","code":""},{"path":"problem-set-1.-definitions.html","id":"points","chapter":"Problem Set 1. Definitions","heading":"1.1 (7 points)","text":"Jose’s claim, treatment?Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-1","chapter":"Problem Set 1. Definitions","heading":"1.2 (7 points)","text":"Using mathematical notation discussed class, define two potential outcomes Jose referringAnswer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-2","chapter":"Problem Set 1. Definitions","heading":"1.3 (7 points)","text":"sentence two, say Fundamental Problem Causal Inference applies Jose’s claim.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-3","chapter":"Problem Set 1. Definitions","heading":"1.4 (7 points)","text":"Using conditional expectations probabilities, write following math: probability majoring statistics higher among students attend Cornell among students attend NYU.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-4","chapter":"Problem Set 1. Definitions","heading":"1.5 (7 points)","text":"Give one reason average causal effect attending Cornell versus NYU (quantity 1.2, averaged students) might different average descriptive difference (1.4) rates majoring statistics.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"a-sailing-class","chapter":"Problem Set 1. Definitions","heading":"2. A sailing class","text":"looking sailing class Cornell Wellness! claim , tell us whether claim causal descriptive.","code":""},{"path":"problem-set-1.-definitions.html","id":"points-5","chapter":"Problem Set 1. Definitions","heading":"2.1 (5 points)","text":"Last year, survey students take class. proportion reporting felt prepared sail Cayuga Lake higher among took class.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-6","chapter":"Problem Set 1. Definitions","heading":"2.2 (5 points)","text":"Last year, survey students class. proportion reporting felt prepared sail Cayuga Lake higher survey taken class.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"points-7","chapter":"Problem Set 1. Definitions","heading":"2.3 (5 points)","text":"average, students class emerged prepared sail without class.Answer. answer ","code":""},{"path":"problem-set-1.-definitions.html","id":"session-info","chapter":"Problem Set 1. Definitions","heading":"Session info","text":"chunk record information R session, useful debugging issues homework assignments contain code.","code":"\nsessionInfo()## R version 4.3.1 (2023-06-16)\n## Platform: aarch64-apple-darwin20 (64-bit)\n## Running under: macOS Sonoma 14.5\n## \n## Matrix products: default\n## BLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \n## LAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n## \n## locale:\n## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n## \n## time zone: America/New_York\n## tzcode source: internal\n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets \n## [6] methods   base     \n## \n## loaded via a namespace (and not attached):\n##  [1] digest_0.6.33     R6_2.5.1          bookdown_0.35    \n##  [4] fastmap_1.2.0     xfun_0.47         cachem_1.1.0     \n##  [7] knitr_1.43        memoise_2.0.1     htmltools_0.5.8.1\n## [10] rmarkdown_2.22    lifecycle_1.0.3   xml2_1.3.4       \n## [13] cli_3.6.1         downlit_0.4.4     sass_0.4.9       \n## [16] withr_2.5.0       jquerylib_0.1.4   compiler_4.3.1   \n## [19] rstudioapi_0.14   tools_4.3.1       evaluate_0.21    \n## [22] bslib_0.8.0       yaml_2.3.7        fs_1.6.2         \n## [25] jsonlite_1.8.7    rlang_1.1.1"},{"path":"problem-set-2.-experiments.html","id":"problem-set-2.-experiments","chapter":"Problem Set 2. Experiments","heading":"Problem Set 2. Experiments","text":"posted assignment yet.meantime, may reference Problem Set 2 Fall 2023, please note may significant changes.","code":""},{"path":"problem-set-3.-dags..html","id":"problem-set-3.-dags.","chapter":"Problem Set 3. DAGs.","heading":"Problem Set 3. DAGs.","text":"posted assignment yet.meantime, may reference Problem Set 3 Fall 2023, please note may significant changes.","code":""},{"path":"problem-set-4.-statistical-modeling.html","id":"problem-set-4.-statistical-modeling","chapter":"Problem Set 4. Statistical modeling","heading":"Problem Set 4. Statistical modeling","text":"posted assignment yet.meantime, may reference Problem Set 4 Fall 2023, please note may significant changes.","code":""},{"path":"problem-set-5.-iv-rd.html","id":"problem-set-5.-iv-rd","chapter":"Problem Set 5. IV + RD","heading":"Problem Set 5. IV + RD","text":"posted assignment yet.meantime, may reference Problem Set 5 Fall 2023, please note may significant changes.","code":""},{"path":"problem-set-6.-difference-in-difference-synthetic-control.html","id":"problem-set-6.-difference-in-difference-synthetic-control","chapter":"Problem Set 6. Difference in Difference + Synthetic Control","heading":"Problem Set 6. Difference in Difference + Synthetic Control","text":"posted assignment yet.meantime, may reference Problem Set 6 Fall 2023, please note may significant changes.","code":""},{"path":"who-we-are.html","id":"who-we-are","chapter":"Who we are","heading":"Who we are","text":"","code":""},{"path":"who-we-are.html","id":"faculty","chapter":"Who we are","heading":"Faculty","text":"enjoy thinking problems goal discover interpretable structure underlies data generating process. includes problems areas causal discovery, graphical models, mixed membership models. many cases, methods tailored high-dimensional setting number variables considered may large compared number observed samples. applied interests vary generally social science related.’m currently working problems causal inference network interference. think causal inference really cool applications across many different fields. ’m generally interested applications public health, social welfare, social good. free time, enjoy singing, dancing, cooking, watching movies, traveling various theme parks.","code":""},{"path":"who-we-are.html","id":"teaching-assistants","chapter":"Who we are","heading":"Teaching assistants","text":"currently working problem related Causality, particularly Causal Graph discovery Functional data. interested learn interpretable structures data hidden confounders. free time enjoy playing basketball, running hanging friends.currently working graphical models noisy measurements. interested causal discovery applications social science biology. free time like puzzles, playing pool, baking.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
